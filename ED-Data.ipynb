{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7140752a-4178-42e9-9f41-3d6d4e214bb6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1486dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import gzip\n",
    "import json\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import asyncpg\n",
    "import glob\n",
    "import gzip\n",
    "import pandas as pd\n",
    "def prettyprint(item):\n",
    "    print(json.dumps(item, indent=4, sort_keys=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c20dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pgsql_params = dict(\n",
    "    dsn=os.getenv(\"PGSQL_URL\"),\n",
    "    server_settings={'search_path': \"edsm\"}\n",
    ")\n",
    "pgpool = await asyncpg.create_pool(**pgsql_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a6752-f9c2-4755-834f-6622628c5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await pgpool.fetch(\"SELECT * FROM edsm.systems WHERE name = $1\", \"Ix\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f09e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "074d80c5",
   "metadata": {},
   "source": [
    "# Setting up the big cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ed58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "count = 0\n",
    "\n",
    "for cy in range(-3000, 3000, 200): # n=30\n",
    "    count += 1\n",
    "    qr = await pgpool.copy_records_to_table(\"volumes\", records=[(cx,cy,cz)  for cx in range(-30000, 30000, 200) for cz in range(-24000, 66000,200)])\n",
    "    print(f\"{count}/{30}\\t{round(100*count/30,2)}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd8e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column with distance to Sol\n",
    "await pgpool.execute(\"\"\"\n",
    "    ALTER TABLE edsm.volumes ADD COLUMN distance integer; \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "await pgpool.execute(\"\"\"\n",
    "    UPDATE  edsm.volumes \n",
    "    SET distance = |/((c200x+100)^2 + (c200y+100)^2 + (c200z+100)^2); \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b84c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Only run after recreating The Voids\"\n",
    "qr = await pgpool.fetch(\"\"\"\n",
    "    SELECT v.*\n",
    "    FROM edsm.volumes v\n",
    "    LEFT JOIN edsm.systems s\n",
    "    ON v.c200x = s.c200x AND v.c200y = s.c200y AND v.c200z = s.c200z \n",
    "    WHERE s.name IS NULL\n",
    "    \"\"\")\n",
    "\n",
    "await pgpool.copy_records_to_table(\"voids\", records=qr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a896ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column with distance to Sol\n",
    "await pgpool.execute(\"\"\"\n",
    "    ALTER TABLE edsm.voids ADD COLUMN distance integer; \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "await pgpool.execute(\"\"\"\n",
    "    UPDATE  edsm.voids \n",
    "    SET distance = |/((c200x+100)^2 + (c200y+100)^2 + (c200z+100)^2); \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784b2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bae340d",
   "metadata": {},
   "source": [
    "# scratchpad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import json\n",
    "# systemsWithCoordinates7days\n",
    "def edc_dbfilereader(filename, verbose=False):\n",
    "    \"\"\"\n",
    "        Opens 'filename' as generator for eddb style objects\n",
    "    \"\"\"\n",
    "\n",
    "    filesize=Path(filename).stat().st_size\n",
    "    chunksize = 64 * 1024 * 1024\n",
    "    est_count = int(6.7*filesize/chunksize) + 1\n",
    "    print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "    count = 0\n",
    "    system_count = 0\n",
    "    item = None\n",
    "\n",
    "    start = time.process_time()\n",
    "    #try:\n",
    "\n",
    "    with gzip.open(filename, 'rt') as jsonfile:\n",
    "\n",
    "        while True:\n",
    "            count += 1\n",
    "            chunk = jsonfile.readlines(chunksize)\n",
    "            if chunk:\n",
    "                for line in chunk:\n",
    "                    if len(line) < 5:\n",
    "                        continue\n",
    "\n",
    "                    yield json.loads(line.rstrip(',\\n\\r '))\n",
    "\n",
    "                    system_count += 1\n",
    "\n",
    "                yield {}\n",
    "                sys.stdout.write(f\"\\r{count}/{est_count}\\t{100*count/est_count:3.2f}%, {int(system_count / (time.process_time() - start)):6} /s, {system_count:9} systems, {((est_count - count) * (time.process_time() - start)/count):5.1f} seconds remaining\")\n",
    "\n",
    "            else:\n",
    "                print(f\"\\nEmpty chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "                break\n",
    "\n",
    "    tpl = (time.process_time() - start)/system_count\n",
    "    sys.stdout.write(f\"\\n{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed6d24-5af3-4c5b-92f6-48fbebff4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Adding indexes ...\")\n",
    "await pgpool.execute(f\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS systems_x_idx ON edsm.systems (x);\n",
    "    CREATE INDEX IF NOT EXISTS systems_y_idx ON edsm.systems (y);\n",
    "    CREATE INDEX IF NOT EXISTS systems_z_idx ON edsm.systems (z); \n",
    "    CREATE INDEX IF NOT EXISTS systems_name_idx ON edsm.systems (name);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5be7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "await pgpool.execute(\"\"\"\n",
    "    ALTER TABLE edsm.systems ADD COLUMN N boolean default false; \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0c0db-035f-4bbb-9e1d-c05a3abac991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"data/galaxy_7days.json.gz\"\n",
    "filename=os.path.join('f:', os.sep, 'data', 'eddb', 'galaxy_1month-2023-01-03.json'+'.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e3a695-b589-4e83-bbed-96f890730ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "updated = 0\n",
    "\n",
    "for item in edc_dbfilereader(filename):\n",
    "    if not item:\n",
    "        #qr = await pgpool.copy_records_to_table(\"systems\", records=data)\n",
    "        qr = await pgpool.executemany(\n",
    "            \"\"\"INSERT INTO edsm.systems (name, x, y, z, n) \n",
    "                VALUES ($1, $2, $3, $4, $5) \n",
    "                ON CONFLICT (name) DO UPDATE SET n=$5, x=$2, y=$3, z=$4\n",
    "            \"\"\", data)\n",
    "        updated += len(data)\n",
    "        data = []\n",
    "        time.sleep(4)\n",
    "        continue\n",
    "\n",
    "    bodies = item.get('bodies',[{}])\n",
    "\n",
    "    neutron = [bool(B.get('subType','')=='Neutron Star') for B in bodies if B.get('mainStar')]        \n",
    "    coords = item.get('coords')\n",
    "    #coordinates = [coords[k] for k in ['x','y','z']]\n",
    "    data.append(\n",
    "        [\n",
    "            item.get('name'), \n",
    "            *[coords[k] for k in ['x','y','z']],\n",
    "            bool(bool(neutron) and all(neutron))\n",
    "        ]\n",
    "        #[item.get('name')] + [coords[k] for k in ['x','y','z']] + [len(neutron)>0 and all(neutron)]\n",
    "    )\n",
    "print(f\"\\nUpdated {updated} systems with neutron star primary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aaf0b0-af1f-42d7-becd-1e00ded4ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"data/galaxy_7days.json.gz\"\n",
    "filename=os.path.join('f:', os.sep, 'data', 'eddb', 'galaxy.json'+'.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7f31d-764a-437e-9ecd-1118a581fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "updated = 0\n",
    "\n",
    "for item in edc_dbfilereader(filename):\n",
    "    if not item:\n",
    "        #qr = await pgpool.copy_records_to_table(\"systems\", records=data)\n",
    "        qr = await pgpool.executemany(\n",
    "            \"\"\"INSERT INTO edsm.systems (name, x, y, z, n) \n",
    "                VALUES ($1, $2, $3, $4,TRUE) \n",
    "                ON CONFLICT (name) DO UPDATE SET n = TRUE\n",
    "            \"\"\", data)\n",
    "        updated += len(data)\n",
    "        data = []\n",
    "        time.sleep(2)\n",
    "        continue\n",
    "\n",
    "    bodies = item.get('bodies',[{}])\n",
    "    if bodies:\n",
    "    \n",
    "        #mainstar = [B.get('subType','-')[0] for B in bodies if B.get('mainStar')]\n",
    "        neutron = [bool(B.get('subType','')=='Neutron Star') for B in bodies if B.get('mainStar')]\n",
    "        if bool(neutron) and all(neutron):\n",
    "            coords = item.get('coords')\n",
    "            #coordinates = [coords[k] for k in ['x','y','z']]\n",
    "            data.append(\n",
    "                [\n",
    "                    item.get('name'), \n",
    "                    *[coords[k] for k in ['x','y','z']]\n",
    "                ]\n",
    "                #[item.get('name')] + [coords[k] for k in ['x','y','z']] + [len(neutron)>0 and all(neutron)]\n",
    "            )\n",
    "\n",
    "print(f\"\\nUpdated {updated} systems with neutron star primary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825ccd6-2995-4e83-ba38-4567751891d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "82/1000000 * 70000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ccb5a0",
   "metadata": {},
   "source": [
    "# Reading EDSM json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c15efb",
   "metadata": {},
   "source": [
    "## Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20351323-66e1-4ba5-bf35-1c0d414857e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/systemsWithCoordinates.json.gz\"\n",
    "\n",
    "# 2022-04-17: 552.40625 seconds 70750837 systems, per system 7.81 us, est: 404.86962424380926\n",
    "# 2022-04-10: 882.5625 seconds 70583001 systems, per system 12.5 us, est: 648.3858730978299\n",
    "#  with 900 duplicate systems\n",
    "#assert False, \"Completed, don't do this again unless you've dropped and recreated the table\"\n",
    "\n",
    "# Drop te existing table:\n",
    "await pgpool.execute(f\"DROP TABLE edsm.systems;\")\n",
    "await pgpool.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS edsm.systems (\n",
    "        name text NOT NULL,\n",
    "        x DOUBLE PRECISION,\n",
    "        y DOUBLE PRECISION,\n",
    "        z DOUBLE PRECISION\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(8*filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with gzip.open(filename, 'rt') as jsonfile:\n",
    "\n",
    "    firstline = jsonfile.readline()\n",
    "\n",
    "    while True:\n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                coords = item.get('coords')\n",
    "                coordinates = [coords[k] for k in coords]\n",
    "                data.append(\n",
    "                    [item.get(\"name\")] + coordinates #+\n",
    "                    #[int(20*math.floor(v/20)) for v in coordinates] +\n",
    "                    #[int(200*math.floor(v/200)) for v in coordinates]\n",
    "                )\n",
    "                \n",
    "            qr = await pgpool.copy_records_to_table(\"systems\", records=data)\n",
    "            system_count += len(data)\n",
    "            sys.stdout.write(f\"{count}/{est_count}\\t{100*count/est_count:3.2f}%, {int(system_count / (time.process_time() - start)):6} /s, {system_count:9} systems, {((est_count - count) * (time.process_time() - start)/count):5.1f} seconds remaining\\r\")\n",
    "            #print(f\"{count}/{est_count}\\t{round(100*count/est_count,2)}%,\\t{int(system_count / (time.process_time() - start))} /s,\\t{system_count} systems, \\t{round(((est_count - count) * (time.process_time() - start)/count),1)} seconds remaining\")\n",
    "            \n",
    "        else:    \n",
    "            print(f\"\\nEmpty chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "            break\n",
    "\n",
    "            \n",
    "\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4dca88-f6a6-4e33-ae18-d69cf04670ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488d256-7497-4c4f-884d-ccf25914cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Adding indexes ...\")\n",
    "await pgpool.execute(f\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS systems_x_idx ON edsm.systems (x);\n",
    "    CREATE INDEX IF NOT EXISTS systems_y_idx ON edsm.systems (y);\n",
    "    CREATE INDEX IF NOT EXISTS systems_z_idx ON edsm.systems (z); \n",
    "    CREATE INDEX IF NOT EXISTS systems_name_idx ON edsm.systems (name);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a883a-b692-4f26-99f4-be554494f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removing duplicates by system name\")\n",
    "await pgpool.execute(\"\"\"\n",
    "    DELETE FROM edsm.systems a\n",
    "    WHERE   a.ctid <> (SELECT min(b.ctid)\n",
    "                     FROM   edsm.systems b\n",
    "                     WHERE  a.name = b.name );\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104eab27-82bb-43b9-9bef-097d259393d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Adding unique index on system name ...\")\n",
    "await pgpool.execute(f\"\"\"\n",
    "    DROP INDEX systems_name_idx ;\n",
    "    CREATE UNIQUE INDEX IF NOT EXISTS systems_name_unique ON edsm.systems (name)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4df3b-71a3-4eb7-99ce-d71adabf2c12",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598f530-cf2d-405e-a1d7-80b7c071ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob(os.path.join('data', \"systemsWithCoordinates7days-*.json.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef779c-0aa9-401a-b7a9-404fc303131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.process_time()\n",
    "\n",
    "for filename in sorted(glob.glob(os.path.join('data', \"systemsWithCoordinates7days-*.json.gz\"))):\n",
    "    \n",
    "    filesize=Path(filename).stat().st_size\n",
    "    chunksize = 4 * 1024 * 1024\n",
    "    est_count = int(8*filesize/chunksize) + 1\n",
    "    print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "    count = 0\n",
    "    system_count = 0\n",
    "    columns = slice(2,6)\n",
    "    data=[]\n",
    "\n",
    "    with gzip.open(filename, 'rt') as jsonfile:\n",
    "\n",
    "        firstline = jsonfile.readline()\n",
    "\n",
    "        while True:\n",
    "            count += 1\n",
    "            chunk = jsonfile.readlines(chunksize)\n",
    "            if chunk:\n",
    "                data = []\n",
    "                for line in chunk:\n",
    "                    if len(line) < 5:\n",
    "                        continue\n",
    "                    item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "\n",
    "                    coords = item.get('coords')\n",
    "                    coordinates = [coords[k] for k in coords]\n",
    "                    data.append(\n",
    "                        [item.get(\"name\")] + coordinates #+\n",
    "                        #[int(20*math.floor(v/20)) for v in coordinates] +\n",
    "                        #[int(200*math.floor(v/200)) for v in coordinates]\n",
    "                    )\n",
    "\n",
    "                qr = await pgpool.executemany(\n",
    "                    \"\"\"INSERT INTO edsm.systems (name, x, y, z) \n",
    "                        VALUES ($1, $2, $3, $4) \n",
    "                        ON CONFLICT DO NOTHING\n",
    "                    \"\"\", data)\n",
    "                system_count += len(data)\n",
    "                sys.stdout.write(f\"{count}/{est_count}\\t{100*count/est_count:3.2f}%, {int(system_count / (time.process_time() - start)):6} /s, {system_count:9} systems, {((est_count - count) * (time.process_time() - start)/count):5.1f} seconds remaining\\r\")\n",
    "                #print(f\"{count}/{est_count}\\t{round(100*count/est_count,2)}%,\\t{int(system_count / (time.process_time() - start))} /s,\\t{system_count} systems, \\t{round(((est_count - count) * (time.process_time() - start)/count),1)} seconds remaining\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nEmpty chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fb06",
   "metadata": {},
   "source": [
    "### Populated systems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c900f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/systemsPopulated.json.gz\"\n",
    "\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(8*filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with gzip.open(filename, \"rt\") as jsonfile:\n",
    "    firstline = jsonfile.readline()\n",
    "\n",
    "    while True:\n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                data.append(\n",
    "                    [item.get(\"name\"), float(item.get(\"population\") if item.get(\"population\") else 0.0), item.get('security')]\n",
    "                )\n",
    "\n",
    "            qr = await pgpool.executemany(\n",
    "                \"\"\"INSERT INTO edsm.populated (systemname, population, security) \n",
    "                    VALUES ($1, $2, $3) \n",
    "                    ON CONFLICT (systemname) DO UPDATE SET \n",
    "                        population = $2,\n",
    "                        security = $3\n",
    "                \"\"\", data)\n",
    "            #qr = await pgpool.copy_records_to_table(\"populated\", records=data)\n",
    "            system_count += len(data)\n",
    "            print(f\"{count}/{est_count}\\t{round(100*count/est_count,2)}%,\\t{int(system_count / (time.process_time() - start))} /s,\\t{system_count} systems, \\t{round(((est_count - count) * (time.process_time() - start)/count),1)} seconds remaining\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Empty chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    "            \n",
    "\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32866850",
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3812b81-8468-41c1-80c6-665167179dc0",
   "metadata": {},
   "source": [
    "### Powerplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d565b5-6347-4a98-85d6-ddb33d6ce993",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/powerPlay.json.gz\"\n",
    "\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(8*filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with gzip.open(filename, 'rt') as jsonfile:\n",
    "    firstline = jsonfile.readline()\n",
    "\n",
    "    while True:\n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                data.append(\n",
    "                    [item.get(k) for k in [\"power\", \"name\",\"powerState\",\"allegiance\"]] \n",
    "                )\n",
    "                \n",
    "            qr = await pgpool.executemany(\n",
    "                \"\"\"INSERT INTO edsm.powers (power, systemname, powerstate, allegiance) \n",
    "                    VALUES ($1, $2, $3, $4) \n",
    "                    ON CONFLICT (power, systemname) DO UPDATE SET \n",
    "                        powerstate = $3,\n",
    "                        allegiance = $4\n",
    "                \"\"\", data)\n",
    "            system_count += len(data)\n",
    "            print(f\"{count}/{est_count}\\t{round(100*count/est_count,2)}%,\\t{int(system_count / (time.process_time() - start))} /s,\\t{system_count} systems, \\t{round(((est_count - count) * (time.process_time() - start)/count),1)} seconds remaining\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    "            \n",
    "\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3184e-3a49-4535-8bc3-25ed184d7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01053a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49e9f075",
   "metadata": {},
   "source": [
    "## Get data from logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea59b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath = \"/Users/fenke/Saved Games/Frontier Developments/Elite Dangerous\"\n",
    "logfiles = glob.glob(os.path.join(logpath, \"journal.*\"))\n",
    "\n",
    "last_system = {\n",
    "    \"timestamp\": \"2000-01-01\",\n",
    "    \"StarSystem\": None\n",
    "}\n",
    "data = {}\n",
    "events = {}\n",
    "jumps = {}\n",
    "scans = {}\n",
    "bodies = {}\n",
    "systems = {}\n",
    "\n",
    "scan_events = set([\n",
    "        \"SAAScanComplete\",\n",
    "        \"SAASignalsFound\",\n",
    "        \"Scan\",\n",
    "        \"FSSDiscoveryScan\",\n",
    "        #\"FSSSignalDiscovered\",\n",
    "        \"FSSAllBodiesFound\",\n",
    "        \"FSDJump\"\n",
    "    ])\n",
    "jump_events = set([\n",
    "    \"StartJump\",\n",
    "    \"FSDJump\",\n",
    "    \"FuelScoop\"\n",
    "])\n",
    "excluded_events = set([\n",
    "    \"ReceiveText\",\n",
    "    \"Location\",\n",
    "    \"Commander\",\n",
    "    \"Materials\",\n",
    "    \"Rank\",\n",
    "    \"Progress\",\n",
    "    \"Reputation\",\n",
    "    \"LoadGame\",\n",
    "    \"EngineerProgress\",\n",
    "    \"Music\",\n",
    "    \"Missions\",\n",
    "    \"Loadout\",\n",
    "    \"Music\",\n",
    "    \"Statistics\",\n",
    "    \"Cargo\",\n",
    "    \"SupercruiseEntry\"\n",
    "])\n",
    "\n",
    "async with pgpool.acquire() as pgconnection: \n",
    "    prepped_query = await pgconnection.prepare(\n",
    "        \"\"\"INSERT INTO edsm.systems (name, x, y, z) \n",
    "            VALUES ($1, $2, $3, $4) \n",
    "            ON CONFLICT DO NOTHING\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    columns = slice(2,6)\n",
    "    start = time.process_time()\n",
    "    system_count = 0\n",
    "    for filename in logfiles[-9:]:\n",
    "\n",
    "        filesize=Path(filename).stat().st_size\n",
    "        chunksize = 1 * 1024 * 1024\n",
    "        est_count = int(filesize/chunksize) + 1\n",
    "        #print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "        count = 0\n",
    "\n",
    "        with open(filename, \"rt\") as jsonfile:\n",
    "            firstline = jsonfile.readline()\n",
    "\n",
    "            while True:\n",
    "                count += 1\n",
    "                chunk = jsonfile.readlines(chunksize)\n",
    "                if chunk:\n",
    "                    #data = []\n",
    "                    for line in chunk:\n",
    "                        if len(line) < 5:\n",
    "                            continue\n",
    "                        item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                        if item.get(\"event\") in excluded_events:\n",
    "                            continue\n",
    "                            \n",
    "                        timestamp = item.get(\"timestamp\")\n",
    "                        eventname = item.get(\"event\")\n",
    "                        system_name = item.get(\"SystemName\", item.get(\"StarSystem\"))\n",
    "                        if system_name and system_name not in systems:\n",
    "                            systems[system_name] = dict(bodies=dict())\n",
    "                            system = systems[system_name]\n",
    "                        \n",
    "                        if eventname in jump_events and item.get(\"JumpType\") != \"Supercruise\":\n",
    "                            if timestamp not in jumps:\n",
    "                                jumps[timestamp] = []\n",
    "                            jump = jumps.get(timestamp)\n",
    "                            jump.append(item)\n",
    "                            \n",
    "                        elif eventname in scan_events:\n",
    "                            body_name = item.get(\"BodyName\")\n",
    "                            if body_name not in bodies:\n",
    "                                bodies[body_name] = {}\n",
    "                            body = bodies[body_name]\n",
    "                            \n",
    "                            if system_name and body_name and body_name not in system[\"bodies\"]:\n",
    "                                system[\"bodies\"][body_name] = body\n",
    "                                \n",
    "                            \n",
    "                            for key in [\"StarSystem\", \"DistanceFromArrivalLS\",\"ProbesUsed\", \"WasDiscovered\", \"WasMapped\", \"Landable\"]: # items\n",
    "                                body[key] = item.get(key, body.get(key, None))\n",
    "                            \n",
    "                            #for key in [\"Signals\", \"Materials\"]: # lists\n",
    "                            \n",
    "                            if timestamp not in scans:\n",
    "                                scans[timestamp] = []\n",
    "                            scan = scans.get(timestamp)\n",
    "                            scan.append(item)\n",
    "                            \n",
    "                        else:\n",
    "                            if timestamp not in events:\n",
    "                                events[timestamp] = []\n",
    "                            event = events.get(timestamp)\n",
    "                            event.append(item)\n",
    "                        \n",
    "                        if item.get(\"event\") == \"FSDJump\":\n",
    "                            coordinates = item.get('StarPos')\n",
    "                            data[item.get(\"StarSystem\")] = coordinates + [int(20*math.floor(v/20)) for v in coordinates] +  [int(200*math.floor(v/200)) for v in coordinates]\n",
    "                            if timestamp > last_system.get(\"timestamp\"):\n",
    "                                last_system = {k:item[k] for k in last_system }\n",
    "                            \n",
    "                        #all_events.append(item.get(\"event\"))\n",
    "                        #data.append( [item.get(k) for k in [\"name\",\"systemName\", \"type\",\"distanceToArrival\"]]  )\n",
    "                    continue # -> while\n",
    "\n",
    "\n",
    "                #print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "                break\n",
    "                \n",
    "        #await prepped_query.executemany([[S] + data[S] for S in data])\n",
    "        system_count += len(data)\n",
    "        data = {}\n",
    "        \n",
    "    #print(f\"{count}/{est_count}\\t{system_count}\\tsystems,\\t{int(system_count / (time.process_time() - start))} /s,\\t{round(100*count/est_count,2)}%, {round(((est_count - count) * (time.process_time() - start)/count),1)} remaining\")\n",
    "\n",
    "\n",
    "    if system_count > 0:\n",
    "        tpl = (time.process_time() - start)/system_count\n",
    "        print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b25179",
   "metadata": {},
   "outputs": [],
   "source": [
    "{T:jumps[T] for T in jumps if jumps[T][0]['event']=='FSDJump'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c373b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyprint(systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ab3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyprint(scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyprint(last_system)\n",
    "prettyprint([R.get(\"name\") for R in await find_nearby_systems(last_system.get(\"StarSystem\"), 10) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753a517",
   "metadata": {},
   "source": [
    "## Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242f37c",
   "metadata": {},
   "source": [
    "### One query to remove them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "await pgpool.execute(\"\"\"\n",
    "    DELETE FROM edsm.systems a\n",
    "    WHERE   a.ctid <> (SELECT min(b.ctid)\n",
    "                     FROM   edsm.systems b\n",
    "                     WHERE  a.name = b.name );\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba738e",
   "metadata": {},
   "source": [
    "### Another ONE bites the dust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "await pgpool.execute(\"\"\"\n",
    "    DELETE FROM edsm.systems a USING (\n",
    "          SELECT MIN(ctid) as ctid, name\n",
    "            FROM edsm.systems \n",
    "            GROUP BY name HAVING COUNT(*) > 1\n",
    "          ) b\n",
    "          WHERE a.name = b.name \n",
    "          AND a.ctid <> b.ctid\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr = await pgpool.fetch(\"SELECT name, count(1) FROM edsm.systems GROUP BY name HAVING count(1) > 1\")\n",
    "\n",
    "async with pgpool.acquire() as pgconnection: \n",
    "    prepped_query = await pgconnection.prepare(\n",
    "        \"\"\"\n",
    "            DELETE FROM edsm.systems a\n",
    "            WHERE   a.name = $1 AND\n",
    "                    a.ctid <> (SELECT min(b.ctid)\n",
    "                             FROM   edsm.systems b\n",
    "                             WHERE  a.name = b.name );        \n",
    "        \"\"\"\n",
    "    )\n",
    "    await prepped_query.executemany([[R.get(\"name\")] for R in qr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "[R.get(\"name\") for R in qr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f71477",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c95d8",
   "metadata": {},
   "source": [
    "## Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6877728",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/stations.json\"\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with open(filename, \"rt\") as jsonfile:\n",
    "    firstline = jsonfile.readline()\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                data.append( [item.get(k) for k in [\"name\",\"systemName\", \"type\",\"distanceToArrival\"]]  )\n",
    "                \n",
    "            qr = await pgpool.copy_records_to_table(\"stations\", records=data)\n",
    "            system_count += len(data)\n",
    "            print(f\"{count}/{est_count}\\t{system_count}\\tsystems,\\t{int(system_count / (time.process_time() - start))} /s,\\t{round(100*count/est_count,2)}%, {round(((est_count - count) * (time.process_time() - start)/count),1)} remaining\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    "            \n",
    "if system_count > 0:\n",
    "    tpl = (time.process_time() - start)/system_count\n",
    "    print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64de20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await pgpool.execute(\n",
    "        \"\"\"\n",
    "            DELETE FROM edsm.stations a\n",
    "            WHERE   a.ctid <> (SELECT min(b.ctid)\n",
    "                             FROM   edsm.stations b\n",
    "                             WHERE  a.name = b.name AND a.systemname = b.systemname);        \n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13f8e7",
   "metadata": {},
   "source": [
    "# Reading EDDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"data/galaxy_1day.json.gz\"\n",
    "filename = \"E:/data/eddb/galaxy_1day.json.gz\"\n",
    "\n",
    "filesize=8*Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, estimated {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd46c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with gzip.GzipFile(filename, 'r') as jsonfile:\n",
    "    firstline = jsonfile.readline()\n",
    "    for t in range(0,4):\n",
    "        line = jsonfile.readline().decode('utf-8')\n",
    "        #print(line, \"\\n\")\n",
    "        prettyprint(json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line))\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        break\n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                coords = item.get('coords')\n",
    "                coordinates = [coords[k] for k in coords]\n",
    "                data.append(\n",
    "                    [item.get(\"name\")] + \n",
    "                    coordinates +\n",
    "                    [int(20*math.floor(v/20)) for v in coordinates] +\n",
    "                    [int(200*math.floor(v/200)) for v in coordinates]\n",
    "                )\n",
    "                \n",
    "            qr = await pgpool.copy_records_to_table(\"stations\", records=data)\n",
    "            system_count += len(data)\n",
    "            print(f\"{count}/{est_count}\\t{system_count}\\tsystems,\\t{int(system_count / (time.process_time() - start))} /s,\\t{round(100*count/est_count,2)}%, {round(((est_count - count) * (time.process_time() - start)/count),1)} remaining\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    "            \n",
    "if system_count > 0:\n",
    "    tpl = (time.process_time() - start)/system_count\n",
    "    print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d39ff",
   "metadata": {},
   "source": [
    "# Reading EDDB csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pgsql_params = dict(\n",
    "    dsn=os.getenv(\"PGSQL_URL\"),\n",
    "    server_settings={'search_path': \"eddb\"}\n",
    ")\n",
    "pgpool = await asyncpg.create_pool(**pgsql_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile(r'''\n",
    "    \\s*                # Any whitespace.\n",
    "    (                  # Start capturing here.\n",
    "      [^,\"']*?         # Either a series of non-comma non-quote characters.\n",
    "      |                # OR\n",
    "      \"(?:             # A double-quote followed by a string of characters...\n",
    "          [^\"\\\\]|\\\\.   # That are either non-quotes or escaped...\n",
    "       )*              # ...repeated any number of times.\n",
    "      \"                # Followed by a closing double-quote.\n",
    "      |                # OR\n",
    "      '(?:[^'\\\\]|\\\\.)*'# Same as above, for single quotes.\n",
    "    )                  # Done capturing.\n",
    "    \\s*                # Allow arbitrary space before the comma.\n",
    "    (?:,|$)            # Followed by a comma or the end of a string.\n",
    "    ''', re.VERBOSE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#filename='data/systems_recently-20220406.csv'\n",
    "assert False\n",
    "filename='data/systems.csv'\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 32 * 1024 * 1024\n",
    "est_count = int(filesize/chunksize) + 1\n",
    "est_systems = 51854708\n",
    "print(f\"Reading {filename}, {int(filesize/(1024*1024))} Mb in approx {est_count} chunks\")\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "\n",
    "start = time.process_time()\n",
    "with open(filename, \"rt\") as csvfile:\n",
    "    headers = csvfile.readline()\n",
    "    print(headers)\n",
    "    while True:\n",
    "        count += 1\n",
    "        chunk = csvfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                name, *coordinates = r.findall(line)[columns]\n",
    "                data.append(\n",
    "                    [name.strip('\"')] + \n",
    "                    [float(c) for c in coordinates])\n",
    "            qr = await pgpool.copy_records_to_table(\"systems\", records=data)\n",
    "            system_count += len(data)\n",
    "            print(f\"Sytems: {system_count}\\t{round(100*system_count/est_systems,2)}%,\\t{int(system_count / (time.process_time() - start))} /s, {round(((est_systems - system_count) * (time.process_time() - start)/system_count),1)} remaining\")\n",
    "            continue\n",
    "        print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    "            \n",
    "\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3f534f",
   "metadata": {},
   "source": [
    "## Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c421b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await pgpool.fetchrow(\"SELECT * FROM systems WHERE name = $1\", \"Sol\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680868fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await pgpool.fetchrow(\"SELECT * FROM systems WHERE name = $1\", \"Deciat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await pgpool.fetchrow(\"SELECT min(x), max(x) FROM systems\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await pgpool.fetchrow(\"SELECT min(y), max(y) FROM systems\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a355f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await pgpool.fetchrow(\"SELECT min(z), max(z) FROM systems\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb45b7",
   "metadata": {},
   "source": [
    "# Guardian Archeology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate, permutations, combinations, product\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardiandata_path = os.path.join(os.getcwd(), 'data', 'guardian')\n",
    "guardiandata_files = {n.split(' - ')[2].split('.')[-2]:os.path.join(guardiandata_path, n) for n in os.listdir(guardiandata_path) if 'Canonn - Guardians' in n}\n",
    "guardiandata = {n:pd.read_csv(p) for n,p in guardiandata_files.items()}\n",
    "soi = ['Ruins','Structures','Beacons']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37aff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "soi = ['Ruins','Structures','Beacons']\n",
    "inter_columns = list([i for i in accumulate([set(guardiandata['Guardian '+n].columns) for n in soi], lambda D1, D2: D1 & D2)][-1])\n",
    "union_columns = list([i for i in accumulate([set(guardiandata['Guardian '+n].columns) for n in soi], lambda D1, D2: D1 | D2)][-1])\n",
    "\n",
    "column_order_inter = {c:i for c,i in zip(guardiandata['Guardian Beacons'].columns, range(len(guardiandata['Guardian Beacons'].columns))) }\n",
    "inter_columns = sorted(inter_columns, key=lambda I:column_order_inter.get(I,100))\n",
    "union_columns = sorted(union_columns, key=lambda I:column_order_inter.get(I,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soi_systems = {\n",
    "    r['System Name']:dict(\n",
    "        coord=np.asarray([r[c] for c in ['x', 'y', 'z']]),\n",
    "        info= np.asarray([g] + [r[c] for c in inter_columns])\n",
    "    )\n",
    "    for g in soi for i, r in guardiandata['Guardian '+g].iterrows()\n",
    "}\n",
    "\n",
    "soi_systemnames = np.vstack([v['info'] for k,v in soi_systems.items()])\n",
    "soi_coordinates = np.vstack([v['coord'] for k,v in soi_systems.items()])\n",
    "print(soi_coordinates.shape, soi_systemnames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffe249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soi_systemnames = np.asarray(\n",
    "    [[g]+[r[c] for c in inter_columns] for g in soi for i, r in guardiandata['Guardian '+g].iterrows()] )\n",
    "soi_coordinates = np.asarray([[r[c] for c in ['x', 'y', 'z']] for g in soi for i, r in guardiandata['Guardian '+g].iterrows()])\n",
    "print(soi_coordinates.shape, soi_systemnames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ace193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soi_systems = {r['System Name']:np.asarray([r[c] for c in ['x', 'y', 'z']]) for g in soi for i, r in guardiandata['Guardian '+g].iterrows()}\n",
    "soi_systemnames = np.asarray(\n",
    "    [[g]+[r[c] for c in inter_columns] for g in soi for i, r in guardiandata['Guardian '+g].iterrows()] )\n",
    "soi_coordinates = np.asarray([[r[c] for c in ['x', 'y', 'z']] for g in soi for i, r in guardiandata['Guardian '+g].iterrows()])\n",
    "print(soi_coordinates.shape, soi_systemnames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_clusters = DBSCAN(eps=120, min_samples=3).fit(soi_coordinates)\n",
    "#print(np.unique(coord_clusters.labels_))\n",
    "#print(np.count_nonzero(np.less(coord_clusters.labels_,0)))\n",
    "print([(l, np.count_nonzero(np.equal(coord_clusters.labels_,l))) for l in np.unique(coord_clusters.labels_)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86706df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outliers = np.copy(soi_coordinates[np.less(coord_clusters.labels_,0)])\n",
    "outlier_systemnames = np.copy(soi_systemnames[np.less(coord_clusters.labels_,0)])\n",
    "\n",
    "pd.DataFrame([[-1]+n.tolist()+c.tolist() for n,c in zip(outlier_systemnames, outliers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6871a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"custom/guardian-clusters.json\", 'wt') as of:\n",
    "    json.dump(dict(markers=[\n",
    "            dict(\n",
    "                pin='red',\n",
    "                text='\\n'.join([f\"{cn:20}: {v}\" for cn, v in zip(['type'] + inter_columns,row[0])]),\n",
    "                **{c:v for c,v in zip(['x','y','z'], row[1])}\n",
    "            )\n",
    "            for row in [(n,c) for n,c in zip(outlier_systemnames, outliers)]\n",
    "        ] + [\n",
    "            dict(\n",
    "                pin='cyan',\n",
    "                text=f'Center of cluster {row[0]} with {row[2]} guardian sites',\n",
    "                **{c:v for c,v in zip(['x','y','z'], row[1])}\n",
    "            )\n",
    "            for row in [\n",
    "                (l, np.round(np.mean(soi_coordinates[np.equal(coord_clusters.labels_,l)], axis=0),2).tolist(), np.count_nonzero(np.equal(coord_clusters.labels_,l)) ) \n",
    "                for l in np.unique(coord_clusters.labels_) \n",
    "                if not l < 0]\n",
    "        ]), of, indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91d6f9",
   "metadata": {},
   "source": [
    "## Two line ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a7b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d, LinearNDInterpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = np.asarray([\n",
    "    np.round(np.mean(soi_coordinates[np.equal(coord_clusters.labels_,l)], axis=0),2).tolist() \n",
    "    for l in np.unique(coord_clusters.labels_) if l >= 0])\n",
    "line_clusters = DBSCAN(eps=12000, min_samples=3).fit(cluster_centers)\n",
    "print([(l, np.count_nonzero(np.equal(line_clusters.labels_,l))) for l in np.unique(line_clusters.labels_)])\n",
    "guardian_lines = {l:cluster_centers[np.equal(line_clusters.labels_,l)] for l in np.unique(line_clusters.labels_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f73c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"custom/two-lines.json\", 'wt') as of:\n",
    "    json.dump(dict(markers=[\n",
    "            dict(\n",
    "                pin='yellow', text=f'line 0 {str(row)}',\n",
    "                **{c:v for c,v in zip(['x','y','z'], row)}\n",
    "            )\n",
    "            for row in guardian_lines[0]\n",
    "        ] + [\n",
    "            dict(\n",
    "                pin='green', text=f'line 1 {str(row)}',\n",
    "                **{c:v for c,v in zip(['x','y','z'], row)}\n",
    "            )\n",
    "            for row in guardian_lines[1]\n",
    "        ]), of, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ceb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_0 = interp1d(guardian_lines[0][:,0], guardian_lines[0][:,2], fill_value='extrapolate')\n",
    "line_1 = interp1d(guardian_lines[1][:,0], guardian_lines[1][:,2], fill_value='extrapolate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [np.min(guardian_lines[0][:,0]), np.max(guardian_lines[0][:,0])]\n",
    "z0 = line_0(x0)\n",
    "x0,z0\n",
    "\n",
    "p0 = np.column_stack((x0,z0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25439331",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [np.min(guardian_lines[1][:,0]), np.max(guardian_lines[1][:,0])]\n",
    "z1 = line_1(x1)\n",
    "p1 = np.column_stack((x1,z1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c362af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import empty_like, dot, array\n",
    "def perp( a ) :\n",
    "    b = empty_like(a)\n",
    "    b[0] = -a[1]\n",
    "    b[1] = a[0]\n",
    "    return b\n",
    "\n",
    "# line segment a given by endpoints a1, a2\n",
    "# line segment b given by endpoints b1, b2\n",
    "# return \n",
    "def seg_intersect(a1,a2, b1,b2) :\n",
    "    da = a2-a1\n",
    "    db = b2-b1\n",
    "    dp = a1-b1\n",
    "    dap = perp(da)\n",
    "    denom = dot( dap, db)\n",
    "    num = dot( dap, dp )\n",
    "    return (num / denom.astype(float))*db + b1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0,p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f320a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_home = seg_intersect(p0[0],p0[1], p1[0],p1[1])\n",
    "with open(f\"custom/guardian-intersect.json\", 'wt') as of:\n",
    "    json.dump(dict(markers=[\n",
    "            dict(\n",
    "                pin='white', text=f'intersect {str(guardian_home)}',\n",
    "                y=0,\n",
    "                **{c:round(v) for c,v in zip(['x','z'], guardian_home.tolist())}\n",
    "            )\n",
    "            \n",
    "        ]), of, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_point = np.array([0,0])\n",
    "target_point = np.array([ -22761.6875 , -3380.84375]) # DSSA Maerzenbecher\n",
    "\n",
    "np.linalg.norm(np.cross(guardian_home-p1[0], p1[0]-target_point))/np.linalg.norm(guardian_home-p1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(np.cross(guardian_home-p0[0], p0[0]-target_point))/np.linalg.norm(guardian_home-p0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p3=target_point\n",
    "target_point = np.array([ -22761.6875 , -3380.84375]) # DSSA Maerzenbecher\n",
    "#p1=guardian_home\n",
    "#p2=p1[0]\n",
    "#The line extending the segment is parameterized as p1 + t (p2 - p1).\n",
    "#The projection falls where t = [(p3-p1) . (p2-p1)] / |p2-p1|^2\n",
    "l2 = np.sum((guardian_home-p1[0])**2)\n",
    "#if you need the point to project on line extention connecting p1 and p2\n",
    "t = np.sum((target_point - guardian_home) * (p1[0] - guardian_home)) / l2\n",
    "\n",
    "#if you need to ignore if p3 does not project onto line segment\n",
    "if t > 1 or t < 0:\n",
    "  print('p3 does not project onto p1-p2 line segment')\n",
    "\n",
    "#if you need the point to project on line segment between p1 and p2 or closest point of the line segment\n",
    "t = max(0, min(1, np.sum((target_point - guardian_home) * (p1[0] - guardian_home)) / l2))\n",
    "\n",
    "projection1 = guardian_home + t * (p1[0] - guardian_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p3=target_point\n",
    "target_point = np.array([  -15755.34375 , 107.0625 ])\n",
    "#p1=guardian_home\n",
    "#p2=p1[0]\n",
    "#The line extending the segment is parameterized as p1 + t (p2 - p1).\n",
    "#The projection falls where t = [(p3-p1) . (p2-p1)] / |p2-p1|^2\n",
    "l2 = np.sum((guardian_home-p1[0])**2)\n",
    "#if you need the point to project on line extention connecting p1 and p2\n",
    "t = np.sum((target_point - guardian_home) * (p1[0] - guardian_home)) / l2\n",
    "\n",
    "#if you need to ignore if p3 does not project onto line segment\n",
    "if t > 1 or t < 0:\n",
    "  print('p3 does not project onto p1-p2 line segment')\n",
    "\n",
    "#if you need the point to project on line segment between p1 and p2 or closest point of the line segment\n",
    "t = max(0, min(1, np.sum((target_point - guardian_home) * (p1[0] - guardian_home)) / l2))\n",
    "\n",
    "projection2 = guardian_home + t * (p1[0] - guardian_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p3=target_point SYREADIAE JX-F C0\n",
    "target_point = np.array([ -9529.4375, -7428.4375])\n",
    "#p1=guardian_home\n",
    "#p2=p1[0]\n",
    "#The line extending the segment is parameterized as p1 + t (p2 - p1).\n",
    "#The projection falls where t = [(p3-p1) . (p2-p1)] / |p2-p1|^2\n",
    "l2 = np.sum((guardian_home-p1[0])**2)\n",
    "#if you need the point to project on line extention connecting p1 and p2\n",
    "t = np.sum((target_point - guardian_home) * (p1[0] - guardian_home)) / l2\n",
    "\n",
    "#if you need to ignore if p3 does not project onto line segment\n",
    "if t > 1 or t < 0:\n",
    "  print('p3 does not project onto p1-p2 line segment')\n",
    "\n",
    "#if you need the point to project on line segment between p1 and p2 or closest point of the line segment\n",
    "t = max(0, min(1, np.sum((target_point - guardian_home) * (p1[0] - guardian_home)) / l2))\n",
    "\n",
    "projection3 = guardian_home + t * (p1[0] - guardian_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ccbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"custom/guardian-intersect.json\", 'wt') as of:\n",
    "    json.dump(dict(markers=[\n",
    "            dict(\n",
    "                pin='white', text=f'intersect {str(guardian_home)}',\n",
    "                y=0,\n",
    "                **{c:round(v) for c,v in zip(['x','z'], guardian_home.tolist())}\n",
    "            ),\n",
    "            dict(\n",
    "                pin='red', text=f'project DSSA Maerzenbecher {str(projection1)}',\n",
    "                y=0,\n",
    "                **{c:round(v) for c,v in zip(['x','z'], projection1.tolist())}\n",
    "            ),\n",
    "            dict(\n",
    "                pin='red', text=f'project Hemi Cuda {str(projection2)}',\n",
    "                y=0,\n",
    "                **{c:round(v) for c,v in zip(['x','z'], projection2.tolist())}\n",
    "            ),\n",
    "            dict(**{\n",
    "                    \"pin\": \"red\",\n",
    "                    \"text\": \"Syreadiae JX-F c0 \\n -9529.4375 / -64.5 / -7428.4375\",\n",
    "                    \"y\": -64.5,\n",
    "                    \"x\": -9529.4375,\n",
    "                    \"z\": -7428.4375\n",
    "                }            \n",
    "            )\n",
    "            \n",
    "        ]), of, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc61b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b25e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eaeec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8162167",
   "metadata": {},
   "source": [
    "## Beacons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b45f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"custom/beacons.json\", 'wt') as of:\n",
    "    json.dump(dict(markers=[\n",
    "            dict(\n",
    "                pin='yellow',\n",
    "                text=f\"{row['System Name']:24} Body: {row['Body Name']} {row['Distance To Arrival']}\\n{row['Guardian Structure System']}\",\n",
    "                **{c:row[c] for c in ['x','y','z']}\n",
    "            )\n",
    "            for i, row in guardiandata['Guardian Beacons'].iterrows()\n",
    "        ] ), of, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21441e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(guardiandata['Guardian Beacons'].iterrows())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d8645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39065261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardiandata['Guardian Beacons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2f720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcbe0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e011f73f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([ \n",
    "    [l]+n.tolist()+c.tolist() \n",
    "    for l in np.unique(coord_clusters.labels_) \n",
    "    for n,c in zip(\n",
    "        soi_systemnames[np.equal(coord_clusters.labels_,l)], \n",
    "        soi_coordinates[np.equal(coord_clusters.labels_,l)] )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56111db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([ \n",
    "    [l]+n.tolist()+c.tolist() \n",
    "    for l in np.unique(coord_clusters.labels_) \n",
    "    for n,c in zip(\n",
    "        soi_systemnames[np.equal(coord_clusters.labels_,l)], \n",
    "        soi_coordinates[np.equal(coord_clusters.labels_,l)] )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44a781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4649bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_markers('guardian-clusters', dict(\n",
    "    red=[\n",
    "        dict(\n",
    "            name=str(row[2]), type=str(row[1]), body=f\"{row[3]} - {row[4]}\",\n",
    "            x=row[6], y=row[7], z=row[8], distance=row[5] ) \n",
    "        for index, row in pd.read_csv('data/Guardian - Ruins.csv').iterrows() \n",
    "    ],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fea73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2f642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outlier_clusters = DBSCAN(eps=200, min_samples=2).fit(outliers)\n",
    "#print(np.unique(outlier_clusters.labels_))\n",
    "#print(np.count_nonzero(np.less(outlier_clusters.labels_,0)))\n",
    "pd.DataFrame([(l, np.count_nonzero(np.equal(outlier_clusters.labels_,l))) for l in np.unique(outlier_clusters.labels_)]).set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49293be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "really_lonely = np.copy(outliers[np.less(outlier_clusters.labels_,0)])\n",
    "really_lonely_systemnames = np.copy(outlier_systemnames[np.less(outlier_clusters.labels_,0)])\n",
    "\n",
    "#print(really_lonely, really_lonely_systemnames)\n",
    "pd.DataFrame([n.tolist()+c.tolist() for n,c in zip(really_lonely_systemnames, really_lonely)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for n,c in zip(really_lonely_systemnames, really_lonely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad416a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_markers('guardian-isolated', dict(\n",
    "    red=[\n",
    "        dict(name=str(row['System Name']), type=str(row['Type(s)']), body=str(row['Body'])) \n",
    "        for index, row in pd.read_csv('data/Guardian - Ruins.csv').iterrows() \n",
    "    ],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4956d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dac964",
   "metadata": {},
   "outputs": [],
   "source": [
    "really_lonely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(outlier_systemnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b8c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3256e1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8637bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, df in guardiandata.items():\n",
    "    print(f\"{n} - {len(df)}\\n{set(df.columns)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63898fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "soi = ['Ruins','Structures','Beacons']\n",
    "inter_columns = list([i for i in accumulate([set(guardiandata['Guardian '+n].columns) for n in soi], lambda D1, D2: D1 & D2)][-1])\n",
    "union_columns = list([i for i in accumulate([set(guardiandata['Guardian '+n].columns) for n in soi], lambda D1, D2: D1 | D2)][-1])\n",
    "\n",
    "column_order_inter = {c:i for c,i in zip(guardiandata['Guardian Beacons'].columns, range(len(guardiandata['Guardian Beacons'].columns))) }\n",
    "inter_columns = sorted(inter_columns, key=lambda I:column_order_inter.get(I,100))\n",
    "union_columns = sorted(union_columns, key=lambda I:column_order_inter.get(I,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([guardiandata['Guardian '+n][inter_columns] for n in soi]).sort_values('Body Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15abc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e8e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ef24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [set(guardiandata['Guardian '+n].columns) for n in ['Ruins','Structures','Beacons']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6049b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca24873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54a3435d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a79e503d",
   "metadata": {},
   "source": [
    "# Dividing the Galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae598b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the extend of the explored galaxy\n",
    "qx = await pgpool.fetchrow(\"SELECT min(c20x), max(c20x) from systems\")\n",
    "qy = await pgpool.fetchrow(\"SELECT min(c20y), max(c20y) from systems\")\n",
    "qz = await pgpool.fetchrow(\"SELECT min(c20z), max(c20z) from systems\")\n",
    "print([(R.get(\"min\"), R.get(\"max\"), R.get(\"max\")-R.get(\"min\")) for R in [qx, qy, qz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the extend of the explored galaxy\n",
    "qx = await pgpool.fetchrow(\"SELECT min(x), max(x) from systems\")\n",
    "qy = await pgpool.fetchrow(\"SELECT min(y), max(y) from systems\")\n",
    "qz = await pgpool.fetchrow(\"SELECT min(z), max(z) from systems\")\n",
    "print([(R.get(\"min\"), R.get(\"max\"), R.get(\"max\")-R.get(\"min\")) for R in [qx, qy, qz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbe7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8428d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_extend = [(-42213.8125, 40503.8125, 82717.625), (-29359.8125, 39518.34375, 68878.15625), (-23405.0, 65630.15625, 89035.15625)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [(int(math.floor(v[2]/200))) for v in known_extend]\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[0] * counts[1] * counts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(len(range(-40000, 40000, 200)),3)/pow(100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0992c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr, *YZ = [(int(200*math.floor(R.get(\"min\")/200)) , int(200*math.floor(R.get(\"max\")/200))) for R in [qx, qy, qz]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(*Xr, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490fb1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yr, *Z = YZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d251c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(*Yr, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zr, *T = Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ae333",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(*Zr, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "count = 0\n",
    "start = time.process_time()\n",
    "\n",
    "for cy in range(-3000, 3000, 200): # n=30\n",
    "    count += 1\n",
    "    qr = await pgpool.copy_records_to_table(\"volumes\", records=[(cx,cy,cz)  for cx in range(-30000, 30000, 200) for cz in range(-24000, 66000,200)])\n",
    "    print(f\"{count}/{30}\\t{round(100*count/30,2)}%, {round(((30 - count) * (time.process_time() - start)/count),1)} remaining\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c259829",
   "metadata": {},
   "outputs": [],
   "source": [
    "cy=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b4b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(cx,cy,cz)  for cx in range(-30000, 30000, 2000) for cz in range(-24000, 66000,2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd440758",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(-24000, 66000, 200)) * len(range(-30000, 30000, 200)) * len(range(-3000, 3000, 200)) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0528391",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(-30000, 30000, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae19a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base (/home/fenke/bin/python)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
