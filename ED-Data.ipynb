{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7140752a-4178-42e9-9f41-3d6d4e214bb6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1486dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import asyncpg\n",
    "import glob\n",
    "import gzip\n",
    "import pandas as pd\n",
    "def prettyprint(item):\n",
    "    print(json.dumps(item, indent=4, sort_keys=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c20dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pgsql_params = dict(\n",
    "    dsn=os.getenv(\"PGSQL_URL\"),\n",
    "    server_settings={'search_path': \"edsm\"}\n",
    ")\n",
    "pgpool = await asyncpg.create_pool(**pgsql_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a6752-f9c2-4755-834f-6622628c5203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(await pgpool.fetch(\"SELECT * FROM edsm.systems WHERE name = $1\", \"Ix\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f09e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "074d80c5",
   "metadata": {},
   "source": [
    "# Setting up the big cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ed58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "count = 0\n",
    "\n",
    "for cy in range(-3000, 3000, 200): # n=30\n",
    "    count += 1\n",
    "    qr = await pgpool.copy_records_to_table(\"volumes\", records=[(cx,cy,cz)  for cx in range(-30000, 30000, 200) for cz in range(-24000, 66000,200)])\n",
    "    print(f\"{count}/{30}\\t{round(100*count/30,2)}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd8e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column with distance to Sol\n",
    "await pgpool.execute(\"\"\"\n",
    "    ALTER TABLE edsm.volumes ADD COLUMN distance integer; \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "await pgpool.execute(\"\"\"\n",
    "    UPDATE  edsm.volumes \n",
    "    SET distance = |/((c200x+100)^2 + (c200y+100)^2 + (c200z+100)^2); \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b84c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Only run after recreating The Voids\"\n",
    "qr = await pgpool.fetch(\"\"\"\n",
    "    SELECT v.*\n",
    "    FROM edsm.volumes v\n",
    "    LEFT JOIN edsm.systems s\n",
    "    ON v.c200x = s.c200x AND v.c200y = s.c200y AND v.c200z = s.c200z \n",
    "    WHERE s.name IS NULL\n",
    "    \"\"\")\n",
    "\n",
    "await pgpool.copy_records_to_table(\"voids\", records=qr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a896ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column with distance to Sol\n",
    "await pgpool.execute(\"\"\"\n",
    "    ALTER TABLE edsm.voids ADD COLUMN distance integer; \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "await pgpool.execute(\"\"\"\n",
    "    UPDATE  edsm.voids \n",
    "    SET distance = |/((c200x+100)^2 + (c200y+100)^2 + (c200z+100)^2); \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784b2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bae340d",
   "metadata": {},
   "source": [
    "# scratchpad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import json\n",
    "# systemsWithCoordinates7days\n",
    "def edc_dbfilereader(filename, verbose=False):\n",
    "    \"\"\"\n",
    "        Opens 'filename' as generator for eddb style objects\n",
    "    \"\"\"\n",
    "\n",
    "    filesize=Path(filename).stat().st_size\n",
    "    chunksize = 64 * 1024 * 1024\n",
    "    est_count = int(6.7*filesize/chunksize) + 1\n",
    "    print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "    count = 0\n",
    "    system_count = 0\n",
    "    item = None\n",
    "\n",
    "    start = time.process_time()\n",
    "    #try:\n",
    "\n",
    "    with gzip.open(filename, 'rt') as jsonfile:\n",
    "\n",
    "        while True:\n",
    "            count += 1\n",
    "            chunk = jsonfile.readlines(chunksize)\n",
    "            if chunk:\n",
    "                for line in chunk:\n",
    "                    if len(line) < 5:\n",
    "                        continue\n",
    "\n",
    "                    yield json.loads(line.rstrip(',\\n\\r '))\n",
    "\n",
    "                    system_count += 1\n",
    "\n",
    "                yield {}\n",
    "                sys.stdout.write(f\"\\r{count}/{est_count}\\t{100*count/est_count:3.2f}%, {int(system_count / (time.process_time() - start)):6} /s, {system_count:9} systems, {((est_count - count) * (time.process_time() - start)/count):5.1f} seconds remaining\")\n",
    "\n",
    "            else:\n",
    "                print(f\"\\nEmpty chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "                break\n",
    "\n",
    "    tpl = (time.process_time() - start)/system_count\n",
    "    sys.stdout.write(f\"\\n{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed6d24-5af3-4c5b-92f6-48fbebff4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Adding indexes ...\")\n",
    "await pgpool.execute(f\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS systems_x_idx ON edsm.systems (x);\n",
    "    CREATE INDEX IF NOT EXISTS systems_y_idx ON edsm.systems (y);\n",
    "    CREATE INDEX IF NOT EXISTS systems_z_idx ON edsm.systems (z); \n",
    "    CREATE INDEX IF NOT EXISTS systems_name_idx ON edsm.systems (name);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5be7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "await pgpool.execute(\"\"\"\n",
    "    ALTER TABLE edsm.systems ADD COLUMN N boolean default false; \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0c0db-035f-4bbb-9e1d-c05a3abac991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"data/galaxy_7days.json.gz\"\n",
    "filename=os.path.join('f:', os.sep, 'data', 'eddb', 'galaxy_1month-2023-01-03.json'+'.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e3a695-b589-4e83-bbed-96f890730ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "updated = 0\n",
    "\n",
    "for item in edc_dbfilereader(filename):\n",
    "    if not item:\n",
    "        #qr = await pgpool.copy_records_to_table(\"systems\", records=data)\n",
    "        qr = await pgpool.executemany(\n",
    "            \"\"\"INSERT INTO edsm.systems (name, x, y, z, n) \n",
    "                VALUES ($1, $2, $3, $4, $5) \n",
    "                ON CONFLICT (name) DO UPDATE SET n=$5, x=$2, y=$3, z=$4\n",
    "            \"\"\", data)\n",
    "        updated += len(data)\n",
    "        data = []\n",
    "        time.sleep(4)\n",
    "        continue\n",
    "\n",
    "    bodies = item.get('bodies',[{}])\n",
    "\n",
    "    neutron = [bool(B.get('subType','')=='Neutron Star') for B in bodies if B.get('mainStar')]        \n",
    "    coords = item.get('coords')\n",
    "    #coordinates = [coords[k] for k in ['x','y','z']]\n",
    "    data.append(\n",
    "        [\n",
    "            item.get('name'), \n",
    "            *[coords[k] for k in ['x','y','z']],\n",
    "            bool(bool(neutron) and all(neutron))\n",
    "        ]\n",
    "        #[item.get('name')] + [coords[k] for k in ['x','y','z']] + [len(neutron)>0 and all(neutron)]\n",
    "    )\n",
    "print(f\"\\nUpdated {updated} systems with neutron star primary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aaf0b0-af1f-42d7-becd-1e00ded4ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"data/galaxy_7days.json.gz\"\n",
    "filename=os.path.join('f:', os.sep, 'data', 'eddb', 'galaxy.json'+'.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7f31d-764a-437e-9ecd-1118a581fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "updated = 0\n",
    "\n",
    "for item in edc_dbfilereader(filename):\n",
    "    if not item:\n",
    "        #qr = await pgpool.copy_records_to_table(\"systems\", records=data)\n",
    "        qr = await pgpool.executemany(\n",
    "            \"\"\"INSERT INTO edsm.systems (name, x, y, z, n) \n",
    "                VALUES ($1, $2, $3, $4,TRUE) \n",
    "                ON CONFLICT (name) DO UPDATE SET n = TRUE\n",
    "            \"\"\", data)\n",
    "        updated += len(data)\n",
    "        data = []\n",
    "        time.sleep(2)\n",
    "        continue\n",
    "\n",
    "    bodies = item.get('bodies',[{}])\n",
    "    if bodies:\n",
    "    \n",
    "        #mainstar = [B.get('subType','-')[0] for B in bodies if B.get('mainStar')]\n",
    "        neutron = [bool(B.get('subType','')=='Neutron Star') for B in bodies if B.get('mainStar')]\n",
    "        if bool(neutron) and all(neutron):\n",
    "            coords = item.get('coords')\n",
    "            #coordinates = [coords[k] for k in ['x','y','z']]\n",
    "            data.append(\n",
    "                [\n",
    "                    item.get('name'), \n",
    "                    *[coords[k] for k in ['x','y','z']]\n",
    "                ]\n",
    "                #[item.get('name')] + [coords[k] for k in ['x','y','z']] + [len(neutron)>0 and all(neutron)]\n",
    "            )\n",
    "\n",
    "print(f\"\\nUpdated {updated} systems with neutron star primary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825ccd6-2995-4e83-ba38-4567751891d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "82/1000000 * 70000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ccb5a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading EDSM json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c15efb",
   "metadata": {},
   "source": [
    "## Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20351323-66e1-4ba5-bf35-1c0d414857e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/systemsWithCoordinates.json.gz\"\n",
    "\n",
    "# 2022-04-17: 552.40625 seconds 70750837 systems, per system 7.81 us, est: 404.86962424380926\n",
    "# 2022-04-10: 882.5625 seconds 70583001 systems, per system 12.5 us, est: 648.3858730978299\n",
    "#  with 900 duplicate systems\n",
    "#assert False, \"Completed, don't do this again unless you've dropped and recreated the table\"\n",
    "\n",
    "# Drop te existing table:\n",
    "await pgpool.execute(f\"DROP TABLE edsm.systems;\")\n",
    "await pgpool.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS edsm.systems (\n",
    "        name text NOT NULL,\n",
    "        x DOUBLE PRECISION,\n",
    "        y DOUBLE PRECISION,\n",
    "        z DOUBLE PRECISION\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(8*filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with gzip.open(filename, 'rt') as jsonfile:\n",
    "\n",
    "    firstline = jsonfile.readline()\n",
    "\n",
    "    while True:\n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                coords = item.get('coords')\n",
    "                coordinates = [coords[k] for k in coords]\n",
    "                data.append(\n",
    "                    [item.get(\"name\")] + coordinates #+\n",
    "                    #[int(20*math.floor(v/20)) for v in coordinates] +\n",
    "                    #[int(200*math.floor(v/200)) for v in coordinates]\n",
    "                )\n",
    "                \n",
    "            qr = await pgpool.copy_records_to_table(\"systems\", records=data)\n",
    "            system_count += len(data)\n",
    "            sys.stdout.write(f\"{count}/{est_count}\\t{100*count/est_count:3.2f}%, {int(system_count / (time.process_time() - start)):6} /s, {system_count:9} systems, {((est_count - count) * (time.process_time() - start)/count):5.1f} seconds remaining\\r\")\n",
    "            #print(f\"{count}/{est_count}\\t{round(100*count/est_count,2)}%,\\t{int(system_count / (time.process_time() - start))} /s,\\t{system_count} systems, \\t{round(((est_count - count) * (time.process_time() - start)/count),1)} seconds remaining\")\n",
    "            \n",
    "        else:    \n",
    "            print(f\"\\nEmpty chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "            break\n",
    "\n",
    "            \n",
    "\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4dca88-f6a6-4e33-ae18-d69cf04670ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488d256-7497-4c4f-884d-ccf25914cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Adding indexes ...\")\n",
    "await pgpool.execute(f\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS systems_x_idx ON edsm.systems (x);\n",
    "    CREATE INDEX IF NOT EXISTS systems_y_idx ON edsm.systems (y);\n",
    "    CREATE INDEX IF NOT EXISTS systems_z_idx ON edsm.systems (z); \n",
    "    CREATE INDEX IF NOT EXISTS systems_name_idx ON edsm.systems (name);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a883a-b692-4f26-99f4-be554494f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Removing duplicates by system name\")\n",
    "await pgpool.execute(\"\"\"\n",
    "    DELETE FROM edsm.systems a\n",
    "    WHERE   a.ctid <> (SELECT min(b.ctid)\n",
    "                     FROM   edsm.systems b\n",
    "                     WHERE  a.name = b.name );\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104eab27-82bb-43b9-9bef-097d259393d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Adding unique index on system name ...\")\n",
    "await pgpool.execute(f\"\"\"\n",
    "    DROP INDEX systems_name_idx ;\n",
    "    CREATE UNIQUE INDEX IF NOT EXISTS systems_name_unique ON edsm.systems (name)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4df3b-71a3-4eb7-99ce-d71adabf2c12",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598f530-cf2d-405e-a1d7-80b7c071ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob(os.path.join('data', \"systemsWithCoordinates7days-*.json.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef779c-0aa9-401a-b7a9-404fc303131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.process_time()\n",
    "\n",
    "for filename in sorted(glob.glob(os.path.join('data', \"systemsWithCoordinates7days-*.json.gz\"))):\n",
    "    \n",
    "    filesize=Path(filename).stat().st_size\n",
    "    chunksize = 4 * 1024 * 1024\n",
    "    est_count = int(8*filesize/chunksize) + 1\n",
    "    print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "    count = 0\n",
    "    system_count = 0\n",
    "    columns = slice(2,6)\n",
    "    data=[]\n",
    "\n",
    "    with gzip.open(filename, 'rt') as jsonfile:\n",
    "\n",
    "        firstline = jsonfile.readline()\n",
    "\n",
    "        while True:\n",
    "            count += 1\n",
    "            chunk = jsonfile.readlines(chunksize)\n",
    "            if chunk:\n",
    "                data = []\n",
    "                for line in chunk:\n",
    "                    if len(line) < 5:\n",
    "                        continue\n",
    "                    item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "\n",
    "                    coords = item.get('coords')\n",
    "                    coordinates = [coords[k] for k in coords]\n",
    "                    data.append(\n",
    "                        [item.get(\"name\")] + coordinates #+\n",
    "                        #[int(20*math.floor(v/20)) for v in coordinates] +\n",
    "                        #[int(200*math.floor(v/200)) for v in coordinates]\n",
    "                    )\n",
    "\n",
    "                qr = await pgpool.executemany(\n",
    "                    \"\"\"INSERT INTO edsm.systems (name, x, y, z) \n",
    "                        VALUES ($1, $2, $3, $4) \n",
    "                        ON CONFLICT DO NOTHING\n",
    "                    \"\"\", data)\n",
    "                system_count += len(data)\n",
    "                sys.stdout.write(f\"{count}/{est_count}\\t{100*count/est_count:3.2f}%, {int(system_count / (time.process_time() - start)):6} /s, {system_count:9} systems, {((est_count - count) * (time.process_time() - start)/count):5.1f} seconds remaining\\r\")\n",
    "                #print(f\"{count}/{est_count}\\t{round(100*count/est_count,2)}%,\\t{int(system_count / (time.process_time() - start))} /s,\\t{system_count} systems, \\t{round(((est_count - count) * (time.process_time() - start)/count),1)} seconds remaining\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nEmpty chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fb06",
   "metadata": {},
   "source": [
    "### Populated systems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c900f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"data/systemsPopulated.json.gz\"\n",
    "\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(8*filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with gzip.open(filename, \"rt\") as jsonfile:\n",
    "    firstline = jsonfile.readline()\n",
    "\n",
    "    while True:\n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                data.append(\n",
    "                    [item.get(\"name\"), float(item.get(\"population\") if item.get(\"population\") else 0.0), item.get('security')]\n",
    "                )\n",
    "\n",
    "            qr = await pgpool.executemany(\n",
    "                \"\"\"INSERT INTO edsm.populated (systemname, population, security) \n",
    "                    VALUES ($1, $2, $3) \n",
    "                    ON CONFLICT (systemname) DO UPDATE SET \n",
    "                        population = $2,\n",
    "                        security = $3\n",
    "                \"\"\", data)\n",
    "            #qr = await pgpool.copy_records_to_table(\"populated\", records=data)\n",
    "            system_count += len(data)\n",
    "            print(f\"{count}/{est_count}\\t{round(100*count/est_count,2)}%,\\t{int(system_count / (time.process_time() - start))} /s,\\t{system_count} systems, \\t{round(((est_count - count) * (time.process_time() - start)/count),1)} seconds remaining\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Empty chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    "            \n",
    "\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32866850",
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3812b81-8468-41c1-80c6-665167179dc0",
   "metadata": {},
   "source": [
    "### Powerplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d565b5-6347-4a98-85d6-ddb33d6ce993",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/powerPlay.json.gz\"\n",
    "\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(8*filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with gzip.open(filename, 'rt') as jsonfile:\n",
    "    firstline = jsonfile.readline()\n",
    "\n",
    "    while True:\n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                data.append(\n",
    "                    [item.get(k) for k in [\"power\", \"name\",\"powerState\",\"allegiance\"]] \n",
    "                )\n",
    "                \n",
    "            qr = await pgpool.executemany(\n",
    "                \"\"\"INSERT INTO edsm.powers (power, systemname, powerstate, allegiance) \n",
    "                    VALUES ($1, $2, $3, $4) \n",
    "                    ON CONFLICT (power, systemname) DO UPDATE SET \n",
    "                        powerstate = $3,\n",
    "                        allegiance = $4\n",
    "                \"\"\", data)\n",
    "            system_count += len(data)\n",
    "            print(f\"{count}/{est_count}\\t{round(100*count/est_count,2)}%,\\t{int(system_count / (time.process_time() - start))} /s,\\t{system_count} systems, \\t{round(((est_count - count) * (time.process_time() - start)/count),1)} seconds remaining\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    "            \n",
    "\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3184e-3a49-4535-8bc3-25ed184d7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01053a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49e9f075",
   "metadata": {},
   "source": [
    "## Get data from logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea59b99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logpath = \"/Users/fenke/Saved Games/Frontier Developments/Elite Dangerous\"\n",
    "logfiles = glob.glob(os.path.join(logpath, \"journal.*\"))\n",
    "\n",
    "last_system = {\n",
    "    \"timestamp\": \"2000-01-01\",\n",
    "    \"StarSystem\": None\n",
    "}\n",
    "data = {}\n",
    "events = {}\n",
    "jumps = {}\n",
    "scans = {}\n",
    "bodies = {}\n",
    "systems = {}\n",
    "\n",
    "scan_events = set([\n",
    "        \"SAAScanComplete\",\n",
    "        \"SAASignalsFound\",\n",
    "        \"Scan\",\n",
    "        \"FSSDiscoveryScan\",\n",
    "        #\"FSSSignalDiscovered\",\n",
    "        \"FSSAllBodiesFound\",\n",
    "        \"FSDJump\"\n",
    "    ])\n",
    "jump_events = set([\n",
    "    \"StartJump\",\n",
    "    \"FSDJump\",\n",
    "    \"FuelScoop\"\n",
    "])\n",
    "excluded_events = set([\n",
    "    \"ReceiveText\",\n",
    "    \"Location\",\n",
    "    \"Commander\",\n",
    "    \"Materials\",\n",
    "    \"Rank\",\n",
    "    \"Progress\",\n",
    "    \"Reputation\",\n",
    "    \"LoadGame\",\n",
    "    \"EngineerProgress\",\n",
    "    \"Music\",\n",
    "    \"Missions\",\n",
    "    \"Loadout\",\n",
    "    \"Music\",\n",
    "    \"Statistics\",\n",
    "    \"Cargo\",\n",
    "    \"SupercruiseEntry\"\n",
    "])\n",
    "\n",
    "async with pgpool.acquire() as pgconnection: \n",
    "    prepped_query = await pgconnection.prepare(\n",
    "        \"\"\"INSERT INTO edsm.systems (name, x, y, z) \n",
    "            VALUES ($1, $2, $3, $4) \n",
    "            ON CONFLICT DO NOTHING\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    columns = slice(2,6)\n",
    "    start = time.process_time()\n",
    "    system_count = 0\n",
    "    for filename in logfiles[-9:]:\n",
    "\n",
    "        filesize=Path(filename).stat().st_size\n",
    "        chunksize = 1 * 1024 * 1024\n",
    "        est_count = int(filesize/chunksize) + 1\n",
    "        #print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "        count = 0\n",
    "\n",
    "        with open(filename, \"rt\") as jsonfile:\n",
    "            firstline = jsonfile.readline()\n",
    "\n",
    "            while True:\n",
    "                count += 1\n",
    "                chunk = jsonfile.readlines(chunksize)\n",
    "                if chunk:\n",
    "                    #data = []\n",
    "                    for line in chunk:\n",
    "                        if len(line) < 5:\n",
    "                            continue\n",
    "                        item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                        if item.get(\"event\") in excluded_events:\n",
    "                            continue\n",
    "                            \n",
    "                        timestamp = item.get(\"timestamp\")\n",
    "                        eventname = item.get(\"event\")\n",
    "                        system_name = item.get(\"SystemName\", item.get(\"StarSystem\"))\n",
    "                        if system_name and system_name not in systems:\n",
    "                            systems[system_name] = dict(bodies=dict())\n",
    "                            system = systems[system_name]\n",
    "                        \n",
    "                        if eventname in jump_events and item.get(\"JumpType\") != \"Supercruise\":\n",
    "                            if timestamp not in jumps:\n",
    "                                jumps[timestamp] = []\n",
    "                            jump = jumps.get(timestamp)\n",
    "                            jump.append(item)\n",
    "                            \n",
    "                        elif eventname in scan_events:\n",
    "                            body_name = item.get(\"BodyName\")\n",
    "                            if body_name not in bodies:\n",
    "                                bodies[body_name] = {}\n",
    "                            body = bodies[body_name]\n",
    "                            \n",
    "                            if system_name and body_name and body_name not in system[\"bodies\"]:\n",
    "                                system[\"bodies\"][body_name] = body\n",
    "                                \n",
    "                            \n",
    "                            for key in [\"StarSystem\", \"DistanceFromArrivalLS\",\"ProbesUsed\", \"WasDiscovered\", \"WasMapped\", \"Landable\"]: # items\n",
    "                                body[key] = item.get(key, body.get(key, None))\n",
    "                            \n",
    "                            #for key in [\"Signals\", \"Materials\"]: # lists\n",
    "                            \n",
    "                            if timestamp not in scans:\n",
    "                                scans[timestamp] = []\n",
    "                            scan = scans.get(timestamp)\n",
    "                            scan.append(item)\n",
    "                            \n",
    "                        else:\n",
    "                            if timestamp not in events:\n",
    "                                events[timestamp] = []\n",
    "                            event = events.get(timestamp)\n",
    "                            event.append(item)\n",
    "                        \n",
    "                        if item.get(\"event\") == \"FSDJump\":\n",
    "                            coordinates = item.get('StarPos')\n",
    "                            data[item.get(\"StarSystem\")] = coordinates + [int(20*math.floor(v/20)) for v in coordinates] +  [int(200*math.floor(v/200)) for v in coordinates]\n",
    "                            if timestamp > last_system.get(\"timestamp\"):\n",
    "                                last_system = {k:item[k] for k in last_system }\n",
    "                            \n",
    "                        #all_events.append(item.get(\"event\"))\n",
    "                        #data.append( [item.get(k) for k in [\"name\",\"systemName\", \"type\",\"distanceToArrival\"]]  )\n",
    "                    continue # -> while\n",
    "\n",
    "\n",
    "                #print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "                break\n",
    "                \n",
    "        #await prepped_query.executemany([[S] + data[S] for S in data])\n",
    "        system_count += len(data)\n",
    "        data = {}\n",
    "        \n",
    "    #print(f\"{count}/{est_count}\\t{system_count}\\tsystems,\\t{int(system_count / (time.process_time() - start))} /s,\\t{round(100*count/est_count,2)}%, {round(((est_count - count) * (time.process_time() - start)/count),1)} remaining\")\n",
    "\n",
    "\n",
    "    if system_count > 0:\n",
    "        tpl = (time.process_time() - start)/system_count\n",
    "        print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b25179",
   "metadata": {},
   "outputs": [],
   "source": [
    "{T:jumps[T] for T in jumps if jumps[T][0]['event']=='FSDJump'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c373b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyprint(systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ab3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyprint(scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce911b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prettyprint(last_system)\n",
    "prettyprint([R.get(\"name\") for R in await find_nearby_systems(last_system.get(\"StarSystem\"), 10) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753a517",
   "metadata": {},
   "source": [
    "## Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5242f37c",
   "metadata": {},
   "source": [
    "### One query to remove them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "await pgpool.execute(\"\"\"\n",
    "    DELETE FROM edsm.systems a\n",
    "    WHERE   a.ctid <> (SELECT min(b.ctid)\n",
    "                     FROM   edsm.systems b\n",
    "                     WHERE  a.name = b.name );\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba738e",
   "metadata": {},
   "source": [
    "### Another ONE bites the dust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "await pgpool.execute(\"\"\"\n",
    "    DELETE FROM edsm.systems a USING (\n",
    "          SELECT MIN(ctid) as ctid, name\n",
    "            FROM edsm.systems \n",
    "            GROUP BY name HAVING COUNT(*) > 1\n",
    "          ) b\n",
    "          WHERE a.name = b.name \n",
    "          AND a.ctid <> b.ctid\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr = await pgpool.fetch(\"SELECT name, count(1) FROM edsm.systems GROUP BY name HAVING count(1) > 1\")\n",
    "\n",
    "async with pgpool.acquire() as pgconnection: \n",
    "    prepped_query = await pgconnection.prepare(\n",
    "        \"\"\"\n",
    "            DELETE FROM edsm.systems a\n",
    "            WHERE   a.name = $1 AND\n",
    "                    a.ctid <> (SELECT min(b.ctid)\n",
    "                             FROM   edsm.systems b\n",
    "                             WHERE  a.name = b.name );        \n",
    "        \"\"\"\n",
    "    )\n",
    "    await prepped_query.executemany([[R.get(\"name\")] for R in qr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "[R.get(\"name\") for R in qr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f71477",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c95d8",
   "metadata": {},
   "source": [
    "## Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6877728",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/stations.json\"\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with open(filename, \"rt\") as jsonfile:\n",
    "    firstline = jsonfile.readline()\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                data.append( [item.get(k) for k in [\"name\",\"systemName\", \"type\",\"distanceToArrival\"]]  )\n",
    "                \n",
    "            qr = await pgpool.copy_records_to_table(\"stations\", records=data)\n",
    "            system_count += len(data)\n",
    "            print(f\"{count}/{est_count}\\t{system_count}\\tsystems,\\t{int(system_count / (time.process_time() - start))} /s,\\t{round(100*count/est_count,2)}%, {round(((est_count - count) * (time.process_time() - start)/count),1)} remaining\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    "            \n",
    "if system_count > 0:\n",
    "    tpl = (time.process_time() - start)/system_count\n",
    "    print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64de20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await pgpool.execute(\n",
    "        \"\"\"\n",
    "            DELETE FROM edsm.stations a\n",
    "            WHERE   a.ctid <> (SELECT min(b.ctid)\n",
    "                             FROM   edsm.stations b\n",
    "                             WHERE  a.name = b.name AND a.systemname = b.systemname);        \n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13f8e7",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Reading EDDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"data/galaxy_1day.json.gz\"\n",
    "filename = \"E:/data/eddb/galaxy_1day.json.gz\"\n",
    "\n",
    "filesize=8*Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, estimated {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd46c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with gzip.GzipFile(filename, 'r') as jsonfile:\n",
    "    firstline = jsonfile.readline()\n",
    "    for t in range(0,4):\n",
    "        line = jsonfile.readline().decode('utf-8')\n",
    "        #print(line, \"\\n\")\n",
    "        prettyprint(json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line))\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        break\n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                coords = item.get('coords')\n",
    "                coordinates = [coords[k] for k in coords]\n",
    "                data.append(\n",
    "                    [item.get(\"name\")] + \n",
    "                    coordinates +\n",
    "                    [int(20*math.floor(v/20)) for v in coordinates] +\n",
    "                    [int(200*math.floor(v/200)) for v in coordinates]\n",
    "                )\n",
    "                \n",
    "            qr = await pgpool.copy_records_to_table(\"stations\", records=data)\n",
    "            system_count += len(data)\n",
    "            print(f\"{count}/{est_count}\\t{system_count}\\tsystems,\\t{int(system_count / (time.process_time() - start))} /s,\\t{round(100*count/est_count,2)}%, {round(((est_count - count) * (time.process_time() - start)/count),1)} remaining\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    "            \n",
    "if system_count > 0:\n",
    "    tpl = (time.process_time() - start)/system_count\n",
    "    print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d39ff",
   "metadata": {},
   "source": [
    "# Reading EDDB csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pgsql_params = dict(\n",
    "    dsn=os.getenv(\"PGSQL_URL\"),\n",
    "    server_settings={'search_path': \"eddb\"}\n",
    ")\n",
    "pgpool = await asyncpg.create_pool(**pgsql_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile(r'''\n",
    "    \\s*                # Any whitespace.\n",
    "    (                  # Start capturing here.\n",
    "      [^,\"']*?         # Either a series of non-comma non-quote characters.\n",
    "      |                # OR\n",
    "      \"(?:             # A double-quote followed by a string of characters...\n",
    "          [^\"\\\\]|\\\\.   # That are either non-quotes or escaped...\n",
    "       )*              # ...repeated any number of times.\n",
    "      \"                # Followed by a closing double-quote.\n",
    "      |                # OR\n",
    "      '(?:[^'\\\\]|\\\\.)*'# Same as above, for single quotes.\n",
    "    )                  # Done capturing.\n",
    "    \\s*                # Allow arbitrary space before the comma.\n",
    "    (?:,|$)            # Followed by a comma or the end of a string.\n",
    "    ''', re.VERBOSE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#filename='data/systems_recently-20220406.csv'\n",
    "assert False\n",
    "filename='data/systems.csv'\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 32 * 1024 * 1024\n",
    "est_count = int(filesize/chunksize) + 1\n",
    "est_systems = 51854708\n",
    "print(f\"Reading {filename}, {int(filesize/(1024*1024))} Mb in approx {est_count} chunks\")\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "\n",
    "start = time.process_time()\n",
    "with open(filename, \"rt\") as csvfile:\n",
    "    headers = csvfile.readline()\n",
    "    print(headers)\n",
    "    while True:\n",
    "        count += 1\n",
    "        chunk = csvfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            data = []\n",
    "            for line in chunk:\n",
    "                name, *coordinates = r.findall(line)[columns]\n",
    "                data.append(\n",
    "                    [name.strip('\"')] + \n",
    "                    [float(c) for c in coordinates])\n",
    "            qr = await pgpool.copy_records_to_table(\"systems\", records=data)\n",
    "            system_count += len(data)\n",
    "            print(f\"Sytems: {system_count}\\t{round(100*system_count/est_systems,2)}%,\\t{int(system_count / (time.process_time() - start))} /s, {round(((est_systems - system_count) * (time.process_time() - start)/system_count),1)} remaining\")\n",
    "            continue\n",
    "        print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    "            \n",
    "\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3f534f",
   "metadata": {},
   "source": [
    "## Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c421b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await pgpool.fetchrow(\"SELECT * FROM systems WHERE name = $1\", \"Sol\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680868fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await pgpool.fetchrow(\"SELECT * FROM systems WHERE name = $1\", \"Deciat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await pgpool.fetchrow(\"SELECT min(x), max(x) FROM systems\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await pgpool.fetchrow(\"SELECT min(y), max(y) FROM systems\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a355f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await pgpool.fetchrow(\"SELECT min(z), max(z) FROM systems\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e503d",
   "metadata": {},
   "source": [
    "# Dividing the Galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae598b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the extend of the explored galaxy\n",
    "qx = await pgpool.fetchrow(\"SELECT min(c20x), max(c20x) from systems\")\n",
    "qy = await pgpool.fetchrow(\"SELECT min(c20y), max(c20y) from systems\")\n",
    "qz = await pgpool.fetchrow(\"SELECT min(c20z), max(c20z) from systems\")\n",
    "print([(R.get(\"min\"), R.get(\"max\"), R.get(\"max\")-R.get(\"min\")) for R in [qx, qy, qz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the extend of the explored galaxy\n",
    "qx = await pgpool.fetchrow(\"SELECT min(x), max(x) from systems\")\n",
    "qy = await pgpool.fetchrow(\"SELECT min(y), max(y) from systems\")\n",
    "qz = await pgpool.fetchrow(\"SELECT min(z), max(z) from systems\")\n",
    "print([(R.get(\"min\"), R.get(\"max\"), R.get(\"max\")-R.get(\"min\")) for R in [qx, qy, qz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbe7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8428d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_extend = [(-42213.8125, 40503.8125, 82717.625), (-29359.8125, 39518.34375, 68878.15625), (-23405.0, 65630.15625, 89035.15625)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [(int(math.floor(v[2]/200))) for v in known_extend]\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[0] * counts[1] * counts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(len(range(-40000, 40000, 200)),3)/pow(100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0992c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr, *YZ = [(int(200*math.floor(R.get(\"min\")/200)) , int(200*math.floor(R.get(\"max\")/200))) for R in [qx, qy, qz]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(*Xr, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490fb1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yr, *Z = YZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d251c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(*Yr, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zr, *T = Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ae333",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(*Zr, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "count = 0\n",
    "start = time.process_time()\n",
    "\n",
    "for cy in range(-3000, 3000, 200): # n=30\n",
    "    count += 1\n",
    "    qr = await pgpool.copy_records_to_table(\"volumes\", records=[(cx,cy,cz)  for cx in range(-30000, 30000, 200) for cz in range(-24000, 66000,200)])\n",
    "    print(f\"{count}/{30}\\t{round(100*count/30,2)}%, {round(((30 - count) * (time.process_time() - start)/count),1)} remaining\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c259829",
   "metadata": {},
   "outputs": [],
   "source": [
    "cy=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b4b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(cx,cy,cz)  for cx in range(-30000, 30000, 2000) for cz in range(-24000, 66000,2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd440758",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(-24000, 66000, 200)) * len(range(-30000, 30000, 200)) * len(range(-3000, 3000, 200)) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0528391",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(-30000, 30000, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae19a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('warp-cuda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b90ba2cb4a04d0cb4f0bc77004892e7ba164401927e26eda978cfb27a67bb769"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
