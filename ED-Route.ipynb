{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27432b0-2f63-40b2-a2a8-353554ff278a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2286edc-64ec-45df-aba9-f4a55f0564ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import importlib\n",
    "import datetime\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from numpy.lib.recfunctions import join_by\n",
    "from numpy.core.records import fromarrays\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import pcolormesh\n",
    "import pandas as pd\n",
    "import aiohttp\n",
    "import asyncpg\n",
    "import math\n",
    "import itertools\n",
    "import edcompanion.timetools\n",
    "import edcompanion.edsm_api\n",
    "from edcompanion.timetools import make_datetime, make_naive_utc\n",
    "import follow_log\n",
    "def prettyprint(item):\n",
    "    print(json.dumps(item, indent=4, sort_keys=False))\n",
    "    \n",
    "def record_to_dict(record):\n",
    "    if isinstance(record,asyncpg.Record):\n",
    "        return {k:v for k,v in record.items()}\n",
    "    return {}\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16,12)\n",
    "\n",
    "pd.options.display.max_colwidth = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442f1f5-17ba-496f-ba60-8ff451c03898",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgsql_params = dict(\n",
    "    dsn=os.getenv(\"PGSQL_URL\"),\n",
    "    server_settings={'search_path': \"eddb\"}\n",
    ")\n",
    "pgpool = await asyncpg.create_pool(**pgsql_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b720f3-3507-4dec-8186-ba1c40432513",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgpool = await asyncpg.create_pool(**pgsql_params)\n",
    "print(await pgpool.fetch(\"SELECT * FROM systems WHERE name = $1\", \"Ix\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.edsm.net/api-logs-v1/get-position\n",
    "importlib.reload(edcompanion.edsm_api)\n",
    "from edcompanion.edsm_api import get_commander_position\n",
    "\n",
    "system_name = get_commander_position('immerlicht', os.getenv('EDSM_TOKEN')).get('system')\n",
    "print(system_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b586844-905e-49a1-a05e-a1120376f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(edcompanion.edsm_api)\n",
    "from edcompanion.edsm_api import get_edsm_info\n",
    "\n",
    "async def find_system(system, distance=40):\n",
    "    if isinstance(system, str):\n",
    "        q1 =  await pgpool.fetchrow(\n",
    "            \"\"\"\n",
    "                SELECT s.*, 0 as distance, p.security\n",
    "                FROM systems s\n",
    "                LEFT JOIN populated p\n",
    "                ON s.name = p.systemname\n",
    "                where s.name = $1\n",
    "            \"\"\", system)\n",
    "        if not q1:\n",
    "            return get_edsm_info(system)\n",
    "        return q1\n",
    "    \n",
    "    assert len(system) == 3\n",
    "    coordinates = system\n",
    "    c20_location = [int(20*math.floor(v/20)) for v in coordinates]\n",
    "    side = int(20*math.floor(distance/20))\n",
    "    q1 = await pgpool.fetch(\n",
    "        \"SELECT systems.*, |/((x-$7)^2 + (y-$8)^2 + (z-$9)^2) as distance, populated.security \"+\n",
    "        \"FROM systems \"+\n",
    "        \"LEFT JOIN populated \" +\n",
    "        \"ON systems.name = populated.systemname \"\n",
    "        \"WHERE x>=$1 AND x<$2 AND  y>=$3 AND y<$4  AND  z>=$5 AND z<$6  AND |/((x-$7)^2 + (y-$8)^2 + (z-$9)^2) < $10\"+\n",
    "        \"ORDER BY distance\",\n",
    "        *[d for c in coordinates for d in [c-40, c+40]], *coordinates, distance)\n",
    "    if not q1:\n",
    "        return q1\n",
    "    return await find_system(q1[0].get(\"name\"))\n",
    "\n",
    "async def find_nearby_systems(system, distance, limit=5, include_neutron=True):\n",
    "    if isinstance(system, str):\n",
    "        ql = await find_system(system)\n",
    "        coordinates = [ql.get(k) for k in [\"x\", \"y\",\"z\"]]\n",
    "    else:\n",
    "        coordinates = system\n",
    "        \n",
    "    #c20_location = [int(20*math.floor(v/20)) for v in coordinates]\n",
    "    side = int(20*math.ceil(distance/20))\n",
    "\n",
    "    return await pgpool.fetch(\n",
    "        \"SELECT name, x,y,z, |/((x-$7)^2 + (y-$8)^2 + (z-$9)^2) as distance \"+\n",
    "        \"FROM systems \"+f\"\"\"\n",
    "            WHERE  x>=$1 AND x<$2 \n",
    "              AND  y>=$3 AND y<$4  \n",
    "              AND  z>=$5 AND z<$6 \n",
    "              {'AND NOT n' if not include_neutron else ''}\n",
    "        \"\"\" +\n",
    "        \"  AND |/((x-$7)^2 + (y-$8)^2 + (z-$9)^2) < $10\"+\n",
    "        \"ORDER BY distance LIMIT \" + str(limit),\n",
    "        *[d for c in coordinates for d in [c-side, c+side]], \n",
    "        *coordinates, distance)\n",
    "\n",
    "\n",
    "edsm_info_cache={}\n",
    "\n",
    "# async def get_edsm_info(edsm_session, sname):\n",
    "#     if sname not in edsm_info_cache:\n",
    "#         async with session.get('https://www.edsm.net/api-system-v1/bodies', params=dict(systemName=sname)) as req:\n",
    "#             edsm_info_cache[sname] = await req.json()\n",
    "#             if not edsm_info_cache.get(sname):\n",
    "#                 edsm_info_cache[sname] = {}\n",
    "#     return edsm_info_cache.get(sname,{})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e40fcc-bb15-4794-bf21-fd8c0366a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_map(set_to_map, missing=set()):\n",
    "    templist = sorted(set_to_map)\n",
    "    return {s:i for i,s in zip(range(len(templist)), templist)}\n",
    "\n",
    "def recreate_map(mapping):\n",
    "    subval = min(mapping.values())\n",
    "    if subval > 0:\n",
    "        mapping = {k:v-subval for k,v in mapping.items()}\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def create_automapper(start_dict):\n",
    "    return lambda k: start_dict.get(\n",
    "        k, \n",
    "        None if start_dict.update(\n",
    "            dict(k=1+max(start_dict.values()))\n",
    "        ) else start_dict.get(k))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d753c-2367-437f-b345-ef70a376ca82",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece710e-54c1-48a0-adb3-5e5ceb0ea548",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_materials = {\n",
    "    1:set(['carbon', 'vanadium', 'niobium', 'yttrium']),\n",
    "    2:set(['chromium', 'phosphorus', 'molybdenum', 'technetium']),\n",
    "    3:set(['sulphur', 'manganese' 'cadmium', 'ruthenium' ]),\n",
    "    4:set(['zinc', 'selenium', 'iron', 'tin']),\n",
    "    5:set(['germanium', 'nickel', 'tungsten', 'tellurium']),\n",
    "    6:set(['polonium', 'arsenic', 'mercury', 'rhenium']),\n",
    "    7:set(['antimony', 'zirconium', 'boron', 'lead'])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc868a-fda0-49fb-bc06-133c33304723",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_types = sorted([T for T in {'Barycentre', 'Planet', 'Star'}])\n",
    "body_types = {i:s for i,s in zip(range(len(body_types)), body_types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f74f386-da93-4fc4-9ddc-eae1496dbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79fc97-49b4-4f5f-828f-68f86bdbec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "await find_system('Dalam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486cd072-c5dc-45b0-a08f-5f88245b7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f9b5e1-933d-4221-8ddc-f7fea2df991e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f9661-a4c4-4cbc-a7da-e9102eb854fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5121e896-ed9a-428d-b1b9-9007a49b29ac",
   "metadata": {},
   "source": [
    "# Import logfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4a356-6fe7-4521-b341-874b02f4935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    True : {}\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "faae3e26-c24f-48fa-806d-3f45ce2c0972",
   "metadata": {},
   "source": [
    "travel_events = {\n",
    "    'StartJump':  lambda item: , # carries the current target for FSDJump and the starclass\n",
    "    'FSDTarget':  , # carries the next target on the route\n",
    "    'FSDJump':    , # we just entered this system\n",
    "    'FuelScoop':  , # fuelsccop finished\n",
    "    'Scan':,\n",
    "    'FSSDiscoveryScan':,\n",
    "    'FSSAllBodiesFound':,\n",
    "}\n",
    "# typical event order: \n",
    "# StartJump / FSDTarget / FSDJump / Scan.AutoScan (id:0) / FSSAllBodiesFound / FSSDiscoveryScan (progress) / FuelScoop \n",
    "# StartJump: SystemName, StarClass\n",
    "# FSDTarget: SystemName, StarClass\n",
    "# FSDJump: SystemName, StarPos, BodyID\n",
    "# Scan.AutoScan: BodyName, BodyID, (StarType|PlanetClass)\n",
    "# Scan.AutoScan: BodyName, BodyID, StarType:\"StarType\",\"Subclass\", \"StellarMass\", \"Luminosity\":\"VI\"\n",
    "\n",
    "\n",
    "scan_types= {\n",
    "    'AutoScan':,\n",
    "    'Detailed':,\n",
    "},\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9286337-91d9-49d0-aae8-106b2e33cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath = \"/Users/fenke/Saved Games/Frontier Developments/Elite Dangerous\"\n",
    "logfiles = sorted(\n",
    "    [os.path.join(logpath, f) for f in os.listdir(logpath) if 'Journal' in f.split('.')[0] and '.log' in f],\n",
    "    key=lambda f:f.replace('-', '').replace('Journal.20', 'Journal.').replace('T','')\n",
    ")\n",
    "\n",
    "#logfiles = glob.glob(os.path.join(logpath, \"journal.22*\"))[-51:]\n",
    "\n",
    "last_system = {\n",
    "    \"timestamp\": \"2000-01-01\",\n",
    "    \"StarSystem\": 'No System Found',\n",
    "    \"StarPos\":[]\n",
    "}\n",
    "system_name = last_system[\"StarSystem\"]\n",
    "last_timestamp = make_naive_utc(make_datetime(\"2022-01-01\")).timestamp()\n",
    "session_timestamp = last_timestamp\n",
    "\n",
    "systems = {}\n",
    "system = {}\n",
    "sold_systems = {}\n",
    "bodies = {}\n",
    "system_count = 0\n",
    "jumps = {}\n",
    "data = {}\n",
    "events = {}\n",
    "jump_info = {}\n",
    "sessions = []\n",
    "jumps = {}\n",
    "factions = {}\n",
    "\n",
    "journal_maps_spectral={'-':0}\n",
    "journal_maps_luminosity={'-':0}\n",
    "journal_maps_magnitude={'-':0}\n",
    "\n",
    "map_spectral = create_automapper(journal_maps_spectral)\n",
    "map_luminosity = create_automapper(journal_maps_luminosity)\n",
    "map_magnitude = create_automapper(journal_maps_magnitude)\n",
    "\n",
    "\n",
    "jump_events = set([\n",
    "    \"SupercruiseEntry\",\n",
    "    \"FSDTarget\"\n",
    "    \"StartJump\",\n",
    "    \"FSDJump\",\n",
    "    \"FuelScoop\"\n",
    "])\n",
    "\n",
    "scan_events = set([\n",
    "    \"SAAScanComplete\",\n",
    "    \"SAASignalsFound\",\n",
    "    \"Scan\",\n",
    "    \"FSSDiscoveryScan\",\n",
    "    \"FSSSignalDiscovered\",\n",
    "    \"FSSAllBodiesFound\"\n",
    "])\n",
    "\n",
    "scan_types = set([\n",
    "    \"AutoScan\",\n",
    "\n",
    "])\n",
    "\n",
    "excluded_events = set([\n",
    "    #\"Supercruise\",\n",
    "    \"ReceiveText\",\n",
    "    \"Location\",\n",
    "    \"Commander\",\n",
    "    #\"Materials\",\n",
    "    \"Rank\",\n",
    "    \"Progress\",\n",
    "    #\"Reputation\",\n",
    "    \"LoadGame\",\n",
    "    \"EngineerProgress\",\n",
    "    \"Music\",\n",
    "    #\"Missions\",\n",
    "    \"Loadout\",\n",
    "    \"Music\",\n",
    "    \"Statistics\",\n",
    "    \"Cargo\",\n",
    "    #\"SupercruiseEntry\"\n",
    "])\n",
    "planet_values = {\n",
    "    False: { # was-not-discovered\n",
    "        False: {# was-not-mapped\n",
    "            False: { # is-not-terraformable\n",
    "                'Water world': 1559138,\n",
    "                'Earthlike body': 4224870,\n",
    "                'Ammonia world': 2242455,\n",
    "            },\n",
    "            True : { # is-terraformable\n",
    "                'Water world': 4198704,\n",
    "                \"High metal content body\": 2562654,\n",
    "                \"Rocky body\": 2024270\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        True: {# was-mapped, this specic combo is rubbish\n",
    "        }\n",
    "    },\n",
    "    True: { # was-discovered\n",
    "        False: {# was-not-mapped\n",
    "            False: { # is-not-terraformable\n",
    "                'Water world': 1312209,\n",
    "                'Earthlike body': 3555753,\n",
    "                'Ammonia world': 1887305,\n",
    "            },\n",
    "            True : { # is-terraformable\n",
    "                'Water world': 3533732,\n",
    "                \"High metal content body\": 2156792,\n",
    "                \"Rocky body\": 1703675\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        True: {# was-mapped\n",
    "            False: { # is-not-terraformable\n",
    "                'Water world': 540297,\n",
    "                'Earthlike body': 1464068,\n",
    "                'Ammonia world': 777091,\n",
    "            },\n",
    "            True : { # is-terraformable\n",
    "                'Water world': 1455001,\n",
    "                \"High metal content body\": 888051,\n",
    "                \"Rocky body\": 701482\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "async with pgpool.acquire() as pgconnection: \n",
    "    push_query = await pgconnection.prepare(\n",
    "        \"\"\"INSERT INTO eddb.systems (name, x, y, z) \n",
    "            VALUES ($1, $2, $3, $4) \n",
    "            ON CONFLICT DO NOTHING\n",
    "        \"\"\"\n",
    "    )\n",
    "    get_query = await pgconnection.prepare(\n",
    "        \"\"\"SELECT name, x, y, z\n",
    "            FROM eddb.systems  \n",
    "            WHERE name = $1\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "    \n",
    "        start = time.process_time()\n",
    "\n",
    "        for filename in logfiles[-11:]:\n",
    "\n",
    "            filesize=Path(filename).stat().st_size\n",
    "            chunksize = 1 * 1024 * 1024\n",
    "            est_count = int(filesize/chunksize) + 1\n",
    "            sys.stdout.write(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\\r\")\n",
    "            count = 0\n",
    "\n",
    "            with open(filename, encoding=\"utf-8\") as jsonfile:\n",
    "                firstline = jsonfile.readline()\n",
    "\n",
    "                while True:\n",
    "                    count += 1\n",
    "                    chunk = jsonfile.readlines(chunksize)\n",
    "                    if chunk:\n",
    "                        #data = []\n",
    "                        for line in chunk:\n",
    "                            if len(line) < 5:\n",
    "                                continue\n",
    "                            item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                            if item.get(\"event\") in excluded_events:\n",
    "                                continue\n",
    "\n",
    "                            timestamp = item.pop(\"timestamp\")\n",
    "                            eventname = item.get(\"event\")\n",
    "\n",
    "                            current_timestamp = make_datetime(timestamp).timestamp()\n",
    "                            if round(current_timestamp - last_timestamp) > 900:\n",
    "                                if last_timestamp-session_timestamp > 0:\n",
    "                                    sessions.append([\n",
    "                                        session_timestamp, \n",
    "                                        last_timestamp, \n",
    "                                        last_timestamp-session_timestamp])\n",
    "                                    session_timestamp = current_timestamp\n",
    "\n",
    "                            last_timestamp = current_timestamp\n",
    "                            \n",
    "                            if eventname in [\"StartJump\", \"FSDJump\", \"Location\", \"SupercruiseEntry\", \"FSSDiscoveryScan\", \"Scan\"]:\n",
    "                                \n",
    "                                system_name = item.get(\"StarSystem\", system_name)\n",
    "                                \n",
    "                                if system_name not in jump_info:\n",
    "                                    db_entry = await get_query.fetchrow(system_name)\n",
    "                                    jump_info[system_name] = dict(\n",
    "                                        bodyid=None,\n",
    "                                        bodycount=None,\n",
    "                                        bodies=dict(),\n",
    "                                        #edsm=get_edsm_info(session, system_name) if db_entry else {},\n",
    "                                    )\n",
    "                                \n",
    "                                if system_name not in systems:\n",
    "                                    systems[system_name] = dict(bodies=dict(), factions=dict(), signals=set(), coord=[0,0,0], visits=0)\n",
    "\n",
    "                                system = systems.get(system_name)\n",
    "                                \n",
    "                                for faction in item.get(\"Factions\",[]) + [dict(Name=item.get('Faction','No Faction'))]:\n",
    "                                    factionname = faction.get(\"Name\")\n",
    "                                    system.get(\"factions\").update({factionname:faction.get('Influence',0)})\n",
    "                                    f = factions.get(\n",
    "                                        factionname,\n",
    "                                        dict(systems=dict(), bounty_voucher='1970-01-01', bounty='1970-01-01', reputation=0))\n",
    "                                    f.get('systems').update({item.get(\"StarSystem\"):faction.get('Influence',0)})\n",
    "                                    f.update({\n",
    "                                        'allegiance':faction.get('Allegiance', ''),\n",
    "                                        'reputation':faction.get('MyReputation', 0), \n",
    "                                        'state':faction.get('FactionState', f.get('state', ''))})\n",
    "                                    factions[factionname] = f\n",
    "                                    \n",
    "                                if eventname == \"FSDJump\" and item.get(\"BodyType\") == \"Star\":\n",
    "                                    coordinates = item.get('StarPos', [])\n",
    "                                    jump_info[system_name].update(dict(\n",
    "                                        bodyid=item.get('BodyID'),\n",
    "                                        bodies={\n",
    "                                            item.get('BodyID'):dict(\n",
    "                                                mainstar=True,\n",
    "                                                scans=[]\n",
    "                                            )},\n",
    "                                        coord=coordinates,\n",
    "                                    ))\n",
    "                                    if coordinates:\n",
    "                                        data[system_name] = coordinates\n",
    "                                        systems.get(system_name)[\"coord\"] = coordinates\n",
    "                                        systems.get(system_name)['visits'] += 1\n",
    "\n",
    "                                    # update last system\n",
    "                                    if timestamp > last_system.get(\"timestamp\"):\n",
    "                                        last_system = {k:item.get(k, last_system.get(k)) for k in last_system }\n",
    "                                        last_system['timestamp'] = timestamp\n",
    "                                        \n",
    "                                elif (eventname == \"StartJump\" and item.get(\"Jumptype\") == \"Hyperspace\") or eventname == \"FSDTarget\":\n",
    "                                    jump_info[system_name].update(dict(\n",
    "                                        starclass=item.get('StarClass')\n",
    "                                    ))\n",
    "                                    \n",
    "                                elif eventname == \"FSSDiscoveryScan\":\n",
    "                                    jump_info[system_name].update(dict(\n",
    "                                        bodycount=item.get('BodyCount')\n",
    "                                    ))\n",
    "                                    \n",
    "                                elif eventname == \"Scan\":\n",
    "                                    if item.get('BodyID') not in jump_info.get(system_name).get('bodies'):\n",
    "                                        jump_info.get(system_name)['bodies'].update({\n",
    "                                            item.get('BodyID'):dict(\n",
    "                                                mainstar=False,\n",
    "                                                scans=[])\n",
    "                                        })\n",
    "                                    body = jump_info.get(system_name).get('bodies').get(item.get('BodyID'))\n",
    "                                    body['scans'].append(item.get(\"ScanType\"))\n",
    "                                    \n",
    "                                    if \"StarType\" in item:\n",
    "                                        body.update(dict(\n",
    "                                            starclass=item.get(\"StarType\"),\n",
    "                                            subclass=item.get(\"Subclass\"),\n",
    "                                            stellarmass=item.get(\"StellarMass\"),\n",
    "                                            luminosity=item.get(\"Luminosity\"),\n",
    "                                            absolutemagnitude=item.get(\"AbsoluteMagnitude\"),\n",
    "                                        ))\n",
    "                                        map_spectral(item.get('StarType'))\n",
    "                                        map_luminosity(item.get('Luminosity'))\n",
    "                                        map_magnitude(item.get('AbsoluteMagnitude'))\n",
    "                                        \n",
    "                                    elif \"PlanetClass\" in item:\n",
    "                                        body.update(dict(\n",
    "                                            planetclass=item.get(\"PlanetClass\"),\n",
    "                                            volcanism=item.get(\"Volcanism\"),\n",
    "                                            massem=item.get(\"MassEM\"),\n",
    "                                            radius=item.get(\"Radius\"),\n",
    "                                            terraformstate=item.get(\"TerraformState\"),\n",
    "                                            materials=item.get(\"Materials\"),\n",
    "                                        ))\n",
    "                                    \n",
    "                                    if item.get(\"ScanType\")==\"AutoScan\":\n",
    "                                        body.update(dict(\n",
    "                                            distancefromarrival=item.get(\"DistanceFromArrivalLS\"),\n",
    "                                            wasdiscovered=item.get(\"WasDiscovered\"),\n",
    "                                            wasmapped=item.get(\"WasMapped\"),\n",
    "                                        ))\n",
    "                                    elif item.get(\"ScanType\")==\"Detailed\":\n",
    "                                        body.update(dict(\n",
    "                                            distancefromarrival=item.get(\"DistanceFromArrivalLS\"),\n",
    "                                            wasdiscovered=item.get(\"WasDiscovered\"),\n",
    "                                            wasmapped=item.get(\"WasMapped\"),\n",
    "                                        ))\n",
    "\n",
    "                                elif eventname == 'Bounty':\n",
    "                                    targetship = item.get('Target')\n",
    "                                    if targetship not in bountyships:\n",
    "                                        bountyships[targetship]=[]\n",
    "                                    bountyships[targetship] += [V.get('Reward') for V in item.get('Rewards')]\n",
    "                                    for voucher in item.get('Rewards'):\n",
    "                                        bounty_faction = voucher.get(\"Faction\")\n",
    "                                        bounties[bounty_faction] = bounties.get(bounty_faction,0) + voucher.get('Reward')\n",
    "                                        factions.get(bounty_faction,{}).update(\n",
    "                                            bounty=max(timestamp,factions.get(bounty_faction, {}).get('bounty','')))\n",
    "\n",
    "\n",
    "                                elif eventname == 'RedeemVoucher' and item.get('Type') == 'bounty':\n",
    "                                    for voucher in item.get('Factions'):\n",
    "                                        bounty_faction = voucher.get(\"Faction\")\n",
    "                                        vouchers[bounty_faction] = vouchers.get(bounty_faction, 0) + voucher.get('Amount')\n",
    "                                        factions.get(bounty_faction,{}).update(\n",
    "                                            bounty_voucher=max(timestamp,factions.get(bounty_faction,{}).get('bounty_voucher')))\n",
    "                                \n",
    "\n",
    "                            if eventname in scan_events:\n",
    "                                body_name = item.get(\"BodyName\")\n",
    "                                if body_name not in bodies:\n",
    "                                    bodies[body_name] = dict(IsPlanet=False, PlanetClass=None, Materials=dict())\n",
    "\n",
    "                                body = bodies[body_name]\n",
    "                                if 'bodies' not in system:\n",
    "                                    system['bodies']={}\n",
    "                                if system_name and body_name and body_name not in system.get('bodies',{}):\n",
    "                                    system[\"bodies\"][body_name] = body\n",
    "\n",
    "                                for key in [\n",
    "                                    \"StarSystem\", \"DistanceFromArrivalLS\", \"BodyType\", \"PlanetClass\", \"SurfaceGravity\",\n",
    "                                    \"TerraformState\", \"WasDiscovered\", \"WasMapped\", \"Landable\", \"ProbesUsed\", \"Signals\",\n",
    "                                    \"Volcanism\"\n",
    "                                ]: #items\n",
    "                                    if key in item:\n",
    "                                        body[key] = item.get(key)\n",
    "\n",
    "                                if \"PlanetClass\" in item:\n",
    "                                    body[\"IsPlanet\"] = True\n",
    "\n",
    "                                signal = item.get(\"SignalName_Localised\", None)\n",
    "                                if signal and system:\n",
    "                                    e = system.get(\"signals\", set())\n",
    "                                    e.add(signal)\n",
    "                                    system[\"signals\"] = e\n",
    "\n",
    "                                materials = item.get(\"Materials\")\n",
    "                                if materials and system:\n",
    "                                    body.get('Materials').update({M['Name']:M['Percent'] for M in materials})\n",
    "\n",
    "                            if eventname == 'MultiSellExplorationData':\n",
    "                                bodies_sold = sum([S.get('NumBodies',0) for S in item.get(\"Discovered\",[])])\n",
    "                                \n",
    "                                if bodies_sold > 0:\n",
    "                                    avg_body_val = item.get(\"TotalEarnings\",0) / bodies_sold\n",
    "                                else:\n",
    "                                    avg_body_val = 0\n",
    "                                for datum in item.get(\"Discovered\",[]):\n",
    "                                    sold_systems[datum.get('SystemName')] = datum.get('NumBodies') * avg_body_val\n",
    "                                    \n",
    "                                #for key in [\"Signals\", \"Materials\"]: # lists\n",
    "\n",
    "                            \n",
    "\n",
    "                        continue # -> while\n",
    "\n",
    "\n",
    "                    #print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "                    break\n",
    "\n",
    "            #print(f\"Updating Database ...\")\n",
    "            await push_query.executemany([[S] + data[S] for S in data])\n",
    "            system_count += len(data)\n",
    "            data = {}\n",
    "            \n",
    "        # ------------------------------\n",
    "                        \n",
    "    #print(f\"{count}/{est_count}\\t{system_count}\\tsystems,\\t{int(system_count / (time.process_time() - start))} /s,\\t{round(100*count/est_count,2)}%, {round(((est_count - count) * (time.process_time() - start)/count),1)} remaining\")\n",
    "\n",
    "\n",
    "    if system_count > 0:\n",
    "        tpl = (time.process_time() - start)/system_count\n",
    "        print(f\"\\n{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us.\") \n",
    "        \n",
    "sessions = np.asarray(sessions[1:])\n",
    "#print(f\"Total playing time: {str(datetime.timedelta(seconds=np.sum(sessions[:,2])))}\")\n",
    "\n",
    "print(last_system)\n",
    "system_name = last_system.get(\"StarSystem\",\"Sol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e98cbd-7c5d-4f25-b341-c0f4fd896f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c46cf7-6065-4582-aeea-01e06ed8a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in factions.keys() if 'Blue Maf' in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd996cd-cbf4-4ef2-904f-c0bc5d2bf8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "factions['Eurybia Blue Mafia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ec02e-1af4-4f8e-a901-a847d8d16d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "systems['Enayex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bcd79-6a1d-462f-bc80-b1c07c59e634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4c9a79a-5171-4cf7-b66b-a6adaceee453",
   "metadata": {},
   "source": [
    "## Mapping current system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c892d-d7ec-49c6-838b-63ac57dd0ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c622d-c64d-4c85-8eb4-8e199ae5acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    (\n",
    "        system_name, name.replace(system_name,''),round(body[\"DistanceFromArrivalLS\"]),\n",
    "        body.get(\"BodyType\"),body.get(\"PlanetClass\"), \n",
    "        len(body.get('Materials')), bool(body.get('TerraformState') == 'Terraformable'),\n",
    "        body.get(\"WasDiscovered\"),body.get(\"WasMapped\"), body.get(\"ProbesUsed\"),  \n",
    "        round(planet_values.get(\n",
    "            body.get(\"WasDiscovered\"),{}).get(\n",
    "            body.get(\"WasMapped\"),{}).get(\n",
    "            bool(body.get('TerraformState') == 'Terraformable'),{}).get(\n",
    "            body.get(\"PlanetClass\"),0)/1e6,1),\n",
    "        body.get('Volcanism'),\n",
    "        \",\".join([f\"{S['Count']} {S['Type_Localised']}\" for S in body.get('Signals',[])]), \n",
    "        round(body.get('SurfaceGravity')/9.807,2)\n",
    "    )  \n",
    "    for name, body in systems[system_name][\"bodies\"].items() \n",
    "    #for S in body.get('Signals',[]) \n",
    "    if body[\"IsPlanet\"] and (\n",
    "        #(not body.get(\"WasDiscovered\") and ('metal' in body.get(\"PlanetClass\",\"\").lower()) ) or\n",
    "        planet_values.get(\n",
    "            body.get(\"WasDiscovered\"),{}).get(\n",
    "            body.get(\"WasMapped\"),{}).get(\n",
    "            bool(body.get('TerraformState') == 'Terraformable'),{}).get(\n",
    "            body.get(\"PlanetClass\"),0) > 0\n",
    "        #planet_values.get(body.get(\"PlanetClass\"),False)\n",
    "        or body.get('TerraformState') == 'Terraformable'\n",
    "        #or 'life' in body.get(\"PlanetClass\",\"\").lower()\n",
    "        #or body.get('Volcanism')\n",
    "        or body.get(\"ProbesUsed\") \n",
    "        #or (body.get('Landable', '') and body.get('Materials'))\n",
    "    )\n",
    "], columns=[\n",
    "    \"system\",\"name\",\"dist.(ls)\",\"type\",\"class\", \n",
    "    'raw',\"terra\",'disc.',\"mapped\",\"probes\", \"value\", \"volcanism\",\n",
    "    'signal','gravity']).set_index([\"system\",\"name\"]).sort_values(\n",
    "    [\"mapped\",\"dist.(ls)\"])\n",
    "        \n",
    "    \n",
    "#if   not body.get(\"WasMapped\") and not body.get(\"ProbesUsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ee89c-7f6e-4e4b-929f-10f786342e00",
   "metadata": {},
   "source": [
    "## Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd49fb-a3b6-41a1-a5ad-b22ddd23cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    (\n",
    "        N, \n",
    "        S.get('visits'),\n",
    "        S.get('SystemFaction', '')  ,\n",
    "        round(factions.get(S.get('SystemFaction'),{}).get('reputation',1)), \n",
    "        round(np.sqrt(np.sum(np.square(np.asarray(system.get('coord'))-np.asarray(S.get('coord'))))),1), \n",
    "        E.replace('Resource Extraction Site','RES').replace('Conflict Zone','CZ').replace(' Intensity]',']').replace('Unidentified signal source','USS')\n",
    "    )\n",
    "    for N, S in systems.items()\n",
    "    for E in systems.get(N,{}).get(\"signals\", set())\n",
    "    if round(np.sqrt(np.sum(np.square(np.asarray(system.get('coord'))-np.asarray(S.get('coord'))))),1) < 60\n",
    "    and (\"Resource\" in E )\n",
    "    and not (\"Hazardous\" in E )\n",
    "\n",
    "], columns=[\"system\", \"visits\", 'faction',  'reputation', \"distance\", \"signal\"]).set_index([\"system\",\"faction\"]).sort_values(['distance','signal']).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5865c6-f59c-4976-afa0-55c66d5ede1c",
   "metadata": {},
   "source": [
    "## Discoveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21de390-5ad6-4eec-85ee-a066093adb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    (\n",
    "        sname, bid ,\n",
    "        body.get('starclass'),\n",
    "        body.get(\"subclass\"),\n",
    "        body.get('wasdiscovered',systems.get(sname,{}).get('bodies',{}).get(sname,{}).get('WasDiscovered')),\n",
    "        body.get('scans'),\n",
    "        system.get('bodycount'),\n",
    "        round(sold_systems.get(sname,0)/1e3)\n",
    "        \n",
    "    )\n",
    "    for sname, system in jump_info.items()\n",
    "    for bid, body in system[\"bodies\"].items()\n",
    "    if body.get(\"mainstar\") and  not sold_systems.get(sname)\n",
    "    #and not sold_systems.get(sname,0)\n",
    "    #and systems.get(sname,{}).get('bodies',{}).get(sname,{}).get('WasDiscovered') is None\n",
    "    #for S in body.get('Signals',[]) \n",
    "], columns=[\n",
    "    \"system\",\"id\",\"class\", \"subclass\", \n",
    "    'disc.', 'scans', 'count', 'sold']).set_index([\"system\",\"id\"]).sort_values(\n",
    "    [\"system\",\"id\"]).sort_values('sold')\n",
    "        \n",
    "    \n",
    "#if   not body.get(\"WasMapped\") and not body.get(\"ProbesUsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3a296-dfa5-4fc0-82a4-b085f0e8afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "systems['Thraikoo MR-N d6-154']['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b747ae-d2a3-4f32-b64b-120a46463a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "jump_info['Flyoo Hypooe VZ-M d8-88']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e5ea58-52a8-4432-a6c9-07da59f56988",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f959c87-75c9-4500-b53e-05ef6baa123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    [\n",
    "        N, \n",
    "        # ----------------------- Logfiles\n",
    "        I.get('bodycount',len(I.get('bodies', []))),\n",
    "        len([i for i,b in I.get('bodies').items() if b.get('starclass') and not b.get('mainstar')]),   # star count\n",
    "        round(sum([b.get('stellarmass') for i,b in I.get('bodies').items() if b.get('starclass')])),   # star mass\n",
    "        str(I.get('bodies').get(I.get('bodyid')).get('starclass')) + str(I.get('bodies').get(I.get('bodyid')).get('subclass')), # spectral class\n",
    "        I.get('bodies').get(I.get('bodyid')).get('luminosity'),\n",
    "        I.get('bodies').get(I.get('bodyid')).get('absolutemagnitude'), \n",
    "        # ----------------------- EDSM\n",
    "        round(I.get('edsm',{}).get('bodyCount',0)),\n",
    "        len([1  for eb in I.get('edsm').get('bodies') if  eb.get('spectralClass') and not eb.get('isMainStar')]),\n",
    "        round(sum([eb.get('solarMasses') for eb in I.get('edsm').get('bodies') if  eb.get('spectralClass')])) # solarMasses\n",
    "\n",
    "    ] + [\n",
    "        [eb for eb in I.get('edsm').get('bodies') if eb.get('isMainStar')][0].get(k) for k in ['spectralClass','luminosity','absoluteMagnitude'] \n",
    "    ] \n",
    "    for N, I in jump_info.items()\n",
    "    \n",
    "    if I.get('bodycount', 0) > 0 \n",
    "    and I.get('bodies').get(I.get('bodyid')).get('luminosity') \n",
    "    and I.get('edsm',{}).get('bodies',[])\n",
    "    and I.get('edsm',{}).get('bodyCount')\n",
    "    \n",
    "]).set_index(0).sample(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c959b35-f8bf-446b-b328-6aaa4d39fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "edsm_info_cache.get('Traikeou FN-P c7-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10360d6-4254-41d4-bb54-4430d6776001",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i,b in jump_info['Blau Thua NX-T d3-5'].get('bodies').items() if b.get('starclass') and not b.get('mainstar')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deadf7b-83f6-4310-a74e-a121c1a9ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,b) for i,b in jump_info['Blau Thua QD-S d4-4'].get('bodies').items() if b.get('starclass')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7134ba-78c4-4429-bd93-9261d1b935a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jump_info['Blau Thua NX-T d3-5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8bf8a-e788-40a1-a35c-d305a06ca5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e8788-3c93-4e02-9524-f2d9d88be70a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad91e2d-81bc-4758-8abe-e52cd4c4f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "edsm_info_cache.get('Nyeajeou VK-D d13-48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6949e0-bc4d-4a33-b59e-809741205dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1807ffb0-6abb-40f3-83c3-a20b64180558",
   "metadata": {},
   "source": [
    "# Routing V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e261d14-55e4-4942-be55-451fcbc50a98",
   "metadata": {},
   "source": [
    "## Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcccae1a-97d9-43d3-bcf6-5a91cb5487dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(edcompanion.edsm_api)\n",
    "from edcompanion.edsm_api import get_edsm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7d95e-2912-42d5-b9ef-e6d1f2d4d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(edcompanion.edsm_api)\n",
    "importlib.reload(follow_log)\n",
    "from edcompanion.edsm_api import get_edsm_info\n",
    "from follow_log import follow_journal\n",
    "\n",
    "#systems, system_name = follow_journal(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2193455c",
   "metadata": {},
   "source": [
    "### Guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b45902",
   "metadata": {},
   "outputs": [],
   "source": [
    "await find_system(system_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7456fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for reportname in \"grreports gsreports gbreports\".split(' '):\n",
    "    df = pd.read_csv('data/grreports.csv')[\"systemName coordX coordY coordZ bodyName reportComment\".split(\" \")]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in [row.reportComment\n",
    "            for reportname in \"grreports gsreports gbreports\".split(' ') \n",
    "            for row in pd.read_csv(f'data/{reportname}.csv')[\"systemName coordX coordY coordZ bodyName reportComment\".split(\" \")].itertuples()\n",
    "            if isinstance(row.reportComment, str) and ',' in row.reportComment\n",
    "            \n",
    "        ]:\n",
    "\n",
    "        print(json.loads(j))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c17ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b45861",
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f113861",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(markers=[\n",
    "    dict(\n",
    "        pin='cyan',\n",
    "        text=f\"{n:16} {s}\",\n",
    "\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ecb784",
   "metadata": {},
   "outputs": [],
   "source": [
    "pincolors=dict(\n",
    "    grreports='cyan',\n",
    "    gsreports='green',\n",
    "    gbreports='yellow'\n",
    ")\n",
    "with open('custom/guardian_report.json', 'wt') as of:\n",
    "    json.dump(dict(markers=[\n",
    "        dict(\n",
    "            pin=pincolors[reportname],\n",
    "            text=f\"{row.systemName:24} {row.bodyName:32}\\n{json.loads(row.reportComment).get('Name_Localised') if (isinstance(row.reportComment, str) and ',' in row.reportComment) else ''}\",\n",
    "            **{k:v for k,v in dict(await find_system(row.systemName)).items() if k in ['x','y','z']}\n",
    "        )\n",
    "        for reportname in \"grreports gsreports gbreports\".split(' ') \n",
    "        for row in pd.read_csv(f'data/{reportname}.csv')[\"systemName coordX coordY coordZ bodyName reportComment\".split(\" \")].itertuples()\n",
    "        \n",
    "    ]), of)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c2ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce334fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_guardianruins = pd.read_csv('data/grreports.csv')[\"systemName coordX coordY coordZ bodyName reportComment\".split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c637e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/gbreports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4facc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_guardiansites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0352b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('custom/guardian_report.json', 'wt') as of:\n",
    "    json.dump(dict(markers=[\n",
    "        dict(\n",
    "            pin='purple',\n",
    "            text=f\"{n:16} {s}\",\n",
    "            **{k:v for k,v in dict(await find_system(s)).items() if k in ['x','y','z']}\n",
    "        )\n",
    "        for n,s in targetsystems.items()\n",
    "    ]), of)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28bd020",
   "metadata": {},
   "source": [
    "### POI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d92867-29c1-4db1-bfff-4aa86ff627b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_data = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb1ba5-65b4-413b-b7e9-fad12a9e84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "DVRzMdG_poilist = []\n",
    "\n",
    "map_DVRitem = dict(\n",
    "    coordinates=lambda I: np.asarray([float(I.get(ik, np.nan).replace(',','.')) for ik in ['x','y','z']]),\n",
    "    name=lambda I: ' '.join(I.get('notiz').split('\\n')[0].split(' ')[4:-1]),\n",
    "    region=lambda I: '',\n",
    "    type=lambda I: '',\n",
    "    summary=lambda I: I.get('notiz'),\n",
    "    curation=lambda I: 9,\n",
    "\n",
    ")\n",
    "\n",
    "for csvfile in sorted(\n",
    "        [os.path.join('data', f) for f in os.listdir('data') if 'DVRzMdG' in f.split('_')[0] and '.csv' in f],\n",
    "        key=lambda f:f.lower()\n",
    "    ):\n",
    "    print(csvfile)\n",
    "    with open(csvfile, newline='', encoding='utf-8') as f:\n",
    "        header, *datarows = [i for i in csv.reader(f,delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)]\n",
    "        DVRzMdG_poilist += [\n",
    "            { k.lower():v for k, v in zip(header, r) if k }\n",
    "            for r in datarows\n",
    "        ]\n",
    "        \n",
    "        poi_data.update({\n",
    "            item.get('systemname'):{\n",
    "                c:map_DVRitem.get(c)(item)\n",
    "                for c in ['coordinates','name','region','type','summary','curation']\n",
    "            }\n",
    "            for item in DVRzMdG_poilist\n",
    "            if item.get('systemname')\n",
    "                 and all([True if item.get(ik) else False for ik in ['x','y','z'] ])\n",
    "        })    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d242a90-5919-42a4-af84-b45a60ec60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "map_carrieritem = dict(\n",
    "    coordinates=lambda I: np.asarray([float(I.get(f\"coord_{ik}\", np.nan).replace(',','.')) for ik in ['x','y','z']]),\n",
    "    name=lambda I: f\"{I.get('name')}\",    \n",
    "    callsign=lambda I:f\"{I.get('callsign')}\",\n",
    "    region=lambda I: I.get('estimatedregion'),\n",
    "    owner=lambda I: I.get('owner'),\n",
    "    services=lambda I: set(I.get('services').split(';') if I.get('services') else [])\n",
    ")\n",
    "carrier_list=[]\n",
    "with open('data/fleetcarriers.csv', newline='', encoding='utf-8') as f:\n",
    "    header, *datarows = [i for i in csv.reader(f,delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)]\n",
    "    carrier_list += [\n",
    "        { k.lower():v for k, v in zip(header, r) if k }\n",
    "        for r in datarows\n",
    "    ]\n",
    "\n",
    "carrier_data ={}\n",
    "for item in carrier_list:\n",
    "    if all([True if item.get(f\"coord_{ik}\") else False for ik in ['x','y','z'] ]) and item.get('services') and 'DSSA' in item.get('name') :\n",
    "        if item.get('lastsystem') not in carrier_data:\n",
    "            carrier_data[item.get('lastsystem')]=dict(\n",
    "                coordinates=map_carrieritem.get('coordinates')(item),\n",
    "                services=set(['']),\n",
    "                carriers=[]\n",
    "            )\n",
    "        carrier_data.get(item.get('lastsystem')).get('carriers').append({\n",
    "                map_carrieritem.get('callsign')(item):map_carrieritem.get('name')(item)\n",
    "            }            \n",
    "        )\n",
    "        carrier_data.get(item.get('lastsystem')).get('services').update( map_carrieritem['services'](item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b388dc-a75e-424b-8861-4fedc1e25c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with aiohttp.ClientSession() as session:\n",
    "    async with session.get('https://edastro.com/gec/json/all') as req:\n",
    "        _edastro_poi = await req.json()\n",
    "        poi_data.update({\n",
    "            item.get('galMapSearch'):{\n",
    "                c:item.get(k)\n",
    "                for c,k in zip(['coordinates','name','region','type','summary','curation'], ['coordinates','name','region','type','summary','curation'])\n",
    "            }\n",
    "            for item in _edastro_poi\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d1f9b-bd01-4e02-837e-7340957859d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(poi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b1a89e-4748-4d2d-9d0f-c7ec67e919fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f531328-5e37-4973-bd67-a9024a628423",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_systems = np.asarray([ \n",
    "    i.get('coordinates')\n",
    "    for k, i in poi_data.items() ])\n",
    "poi_systemnames = np.asarray([ \n",
    "    (k, i.get('name'),  i.get('summary'), i.get('curation'))\n",
    "    for k, i in poi_data.items() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda7f09-3281-4023-b53e-a6428a730bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_poi(coord1, coord2=None):\n",
    "    p1 = np.asarray(coord1)\n",
    "    p2 = np.asarray(coord2) if coord2 is not None else coord1\n",
    "\n",
    "    poi_distances = np.linalg.norm(poi_systems[:, [0,1,2]] - p1, axis=1) + np.linalg.norm(poi_systems[:, [0,1,2]] - p2, axis=1)\n",
    "    ordering = poi_distances.argsort()\n",
    "    return poi_systems[ordering][0], poi_systemnames[ordering][0]\n",
    "\n",
    "def find_poi_between(coord1, coord2):\n",
    "    p1 = np.asarray(coord1)\n",
    "    p2 = np.asarray(coord2)\n",
    "    \n",
    "    distance = np.sqrt(np.sum(np.square(p1-p2)))\n",
    "    \n",
    "    poi_distances1 = np.linalg.norm(poi_systems[:, [0,1,2]] - p1, axis=1) \n",
    "    poi_distances2 = np.linalg.norm(poi_systems[:, [0,1,2]] - p2, axis=1)\n",
    "    \n",
    "    poi_n = (\n",
    "                np.less(poi_distances1, distance) &\n",
    "                np.less(poi_distances2, distance)\n",
    "            )\n",
    "    return poi_systems[poi_n], poi_systemnames[poi_n], np.argmin(poi_distances1[poi_n])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca268726-691d-429b-bc9c-9e1fe308be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetsystems = {L.split('\\t')[0].replace('#','Waypoint '):L.split('\\t')[1].split('(')[0].strip() for L in '''\n",
    "#1\tHIP 117029\t\t\n",
    "0.00%\n",
    "0.00%\n",
    "#2\tDrojia YW-B d13-4\t4,377.94 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#3\tLysooh WT-R b7-0 (Halley's World)\t7,658.41 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#4\tPlaa Ain FF-Z d76 (Rekohu Project)\t10,162.39 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#5\tBlaa Hypai LA-J c11-3\t13,299.43 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#6\tFloawns XE-R d4-45 (The Three Kings)\t15,978.96 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#7\tMynoaw LC-L d8-1429\t21,214.37 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#8\tEgnairs AA-A h72 (Mairon)\t26,508.32 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#9\tHypiae Aurb AA-A g588 (Planet Pancake)\t29,889.18 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#10\tJuenae XZ-G d10-651 (Red River Run)\t34,771.56 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#11\tSagittarius A*\t36,288.63 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#12\tHypio Proo VE-Q e5-1485\t38,824.33 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#13\tPho Aoscs OS-U f2-26 (Black Fields)\t43,891.03 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#14\tAthaip WR-H d11-7577\t48,599.27 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#15\tDryau Scraa AA-A h747 (The Shiner)\t54,319.41 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#16\tChroabs TI-S d4-58 (Goliath)\t58,392.41 ly\t\n",
    "0.00%\n",
    "0.00%blu\n",
    "#17\tEllairb SJ-B b42-10 (Lair of Unicorns)\t63,416.44 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#18\tPlaa Aescs BK-Q d5-60\t66,359.12 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#19\tBleia Dryiae XJ-R e4-1\t69,286.00 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "#20\tOcshodhis\t74,359.19 ly\t\n",
    "0.00%\n",
    "0.00%\n",
    "'''.splitlines() if '#' in L}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473208c0-10f7-4da0-b8ad-b934c8852a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('custom/reise_markers.json', 'wt') as of:\n",
    "    json.dump(dict(markers=[\n",
    "        dict(\n",
    "            pin='purple',\n",
    "            text=f\"{n:16} {s}\",\n",
    "            **{k:v for k,v in dict(await find_system(s)).items() if k in ['x','y','z']}\n",
    "        )\n",
    "        for n,s in targetsystems.items()\n",
    "    ]), of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ceaa2-731c-4e64-8314-73ae196f3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## interesting systems\n",
    "targetsystems.update(dict(\n",
    "    sol='Sol',\n",
    "    ix='Ix',\n",
    "    proto_1='Dehoae HH-U e3-14',\n",
    "    guardian_fsd1='HD 63154',\n",
    "    guardian_fsd2='Synuefe PX-J c25-8',\n",
    "    mel_brandon='Luchtaine',\n",
    "    marsha_hicks='Tir',\n",
    "    # Verruckte POI\n",
    "    three_dwarfs='Drojia YW-B d13-4',\n",
    "    eye_of_fatima='Drojaea DG-E d12-4',\n",
    "    gravi_nightmare='Drojia XK-D c26-0',\n",
    "    # Events\n",
    "    hip22460='HIP 22460',\n",
    "    running_man='HIP 23759',\n",
    "    fc_hip22460='Pleiades Sector CW-U b3-2',\n",
    "    uia_3='Synuefai XI-F c2', #[-243,-170,-1033],\n",
    "    # Asteroid & deep space bases\n",
    "    witch_science='HIP 23759',\n",
    "    orion_tourist='PMD2009 48',\n",
    "    medusa='Crescent Sector GW-W c1-8',\n",
    "    anchorage='Rohini',\n",
    "    new_growth='Pencil Sector EL-Y d5',\n",
    "    # Nebula / POI\n",
    "    scorch_red_moon='Blue Hypooe VV-A c2-12',\n",
    "    cygnus_x1='V1357 Cygni',\n",
    "    elephant='IC 1396 Sector QI-S d4-9',\n",
    "    ngc7026='Csi+47-21046',\n",
    "    ngc7354='Csi+61-22385',\n",
    "    heartsoul='Hypoae Ain MO-I d9-37',\n",
    "    siteseeing_01='Phua Bre FB-O e6-257',\n",
    "    thors_eye=\"Thor's Eye\",\n",
    "    sagitarius_a='Sagittarius A*',\n",
    "    annihilator='Great Annihilator',\n",
    "    four_of_a_kind='Dryao Phylio AA-A h410',\n",
    "    witch_head='Witch Head Sector BQ-Y d14',\n",
    "    amundsen='Lyed YJ-I d9-0',\n",
    "    five_eyes='Phrio Phoea AA-A h12',\n",
    "    statue_liberty='Statue of Liberty Sector DL-Y d27',\n",
    "    # Carriers\n",
    "    sublime='Gleeque HW-N e6-149',\n",
    "    artemis='Synuefuae CM-J d10-42',\n",
    "    gorgon='NGC 7822 Sector BQ-Y d12',\n",
    "    rocksteady='Prooe Hypue FH-U e3-2',\n",
    "    paradox='Prai Hypoo TX-B d4', # carrier\n",
    "    inverness='Thraikoo PS-U e2-4',       # carrier\n",
    "    maerzenbecher='Hedgo GL-P c5-4',      # carrier\n",
    "    fuelmoon='Hypo Aeb WK-H b51-0',\n",
    "    brazillian='Graesms CW-E d11-2',\n",
    "    sanctuary='Syreadiae JX-F c0',        # carrier\n",
    "    ngc281='BD+55 191',\n",
    "    farsight='Heart Sector IR-V b2-0',\n",
    "    minbari=\"Eor Aescs UP-N c20-0\",\n",
    "    traikeou='Traikeou WL-I c11-1',\n",
    "    nearby_station='Prielo TZ-G d10-4',\n",
    "    # Voyager journey\n",
    "    caretaker='Byoo Briae TJ-G c24-0',\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c02e3c-6dff-4554-970d-c9016623cf42",
   "metadata": {},
   "source": [
    "### Verruckte Reise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5194b0-a98b-4e85-8cd2-e20d3d8bcb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vri = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289e8dc-41d9-425c-bdce-3ea304dc87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vri+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79404c-189e-44cf-bacf-c853dbb28891",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_system = await find_system(targetsystems.get(f'Waypoint {vri}'))\n",
    "target_system = await find_system(targetsystems.get(f'Waypoint {vri+1}'))\n",
    "end = np.round(np.asarray([target_system.get(k) for k in [\"x\",\"y\",\"z\"]]))\n",
    "print(f\"waypoint {vri:2}: {dict(start_system)}\")\n",
    "print(f\"waypoint {vri+1:2}: {dict(target_system)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c381f34-333a-42b9-9694-f8c317eed2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9c823-86f9-4010-96d6-05f28681dfc6",
   "metadata": {},
   "source": [
    "### Various"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3be4e9-3a2d-4d25-8446-f1dc74343665",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await find_system(targetsystems.get('eye_of_fatima')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ad8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( get_edsm_info(targetsystems.get('eye_of_fatima'), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ef487-88c0-4cd3-9c0c-95324d524e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup target system vars\n",
    "target_system = await find_system(targetsystems.get('eye_of_fatima'))\n",
    "if target_system:\n",
    "    end = np.round(np.asarray([target_system.get(k) for k in [\"x\",\"y\",\"z\"]]))\n",
    "else:\n",
    "    target_system = get_edsm_info(targetsystems.get('eye_of_fatima'), False)\n",
    "    end = np.round(np.asarray([target_system.get('coords').get(k) for k in [\"x\",\"y\",\"z\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90583b01-0892-4df4-9f91-1ff7dd00c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup target system vars\n",
    "target_system = await find_system(targetsystems.get(f'DRMG{vri}'))\n",
    "end = np.round(np.asarray([target_system.get(k) for k in [\"x\",\"y\",\"z\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21d985-3433-49a1-af73-e850d95ba795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup target system vars\n",
    "target_system = await find_system(targetsystems.get('paradox'))\n",
    "end = np.round(np.asarray([target_system.get(k) for k in [\"x\",\"y\",\"z\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1917822",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(target_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111685c-e1e4-44e6-bc2d-a88d45c0a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup target system vars\n",
    "target_system = await find_system(targetsystems.get('running_man'))\n",
    "end = np.round(np.asarray([target_system.get(k) for k in [\"x\",\"y\",\"z\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f56c4-8a23-48d1-8c3b-a657b689f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup target system vars\n",
    "target_system = await find_system(targetsystems.get('statue_liberty'))\n",
    "end = np.round(np.asarray([target_system.get(k) for k in [\"x\",\"y\",\"z\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422570a3-7be0-4db5-808d-0cf939f8745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup target system vars\n",
    "target_system = await find_system('Eurybia')\n",
    "end = np.round(np.asarray([target_system.get(k) for k in [\"x\",\"y\",\"z\"]]))\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2cb85b-e146-43c3-b978-586403eeba6a",
   "metadata": {},
   "source": [
    "## Startpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0ee8a-b90b-4b68-8ade-1c09daf855db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set starting point vars and initialize route & path vars\n",
    "start_system = await find_system('Ix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebdf40e-fae6-4492-b3eb-e3e51cdd1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set starting point vars and initialize route & path vars\n",
    "start_system = await find_system(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4033a793-5d04-4099-abaf-96850e5ddd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set starting point vars and initialize route & path vars\n",
    "start_system = await find_system(route[-1][2].rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e4e6c-90d7-49d2-90f5-a44d3b635eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from follow_log import follow_journal\n",
    "systems, system_name = follow_journal(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cfef15-88fa-4bdb-9728-32d7a6ae4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_name = get_commander_position('immerlicht', os.getenv('EDSM_TOKEN')).get('system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f54c5-5f79-44f0-87ff-18629fcc2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set starting point vars and initialize route & path vars\n",
    "start_system = await find_system(system_name,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb7f02-1232-4fb1-9ec3-33f3285e102a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73339628-4f5d-4951-8367-224783e0cea4",
   "metadata": {},
   "source": [
    "## Calculate route waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66695bff-4731-4cd2-badb-611f25b7f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(start_system)\n",
    "print(target_system)\n",
    "#start_system = await find_system('Sol')\n",
    "start = np.round(np.asarray([start_system.get(k) for k in [\"x\",\"y\",\"z\"]]))\n",
    "\n",
    "path = [start] # contains coordiantes of waypoints\n",
    "system_names = [(await find_system(start.tolist())).get('name')] # systemnames of waypoints\n",
    "path_info = {} # information about waypoints and algorithm execution\n",
    "\n",
    "visit_poi = True\n",
    "cube_side = 80\n",
    "search_side = 1200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e96478-ed35-4888-ade6-66160d5899e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add no mare then x waypoints\n",
    "pause_at = len(path)+11\n",
    "\n",
    "# distance from current path end to final target\n",
    "travel_distance = np.sqrt(np.sum(np.square(end-path[-1])))\n",
    "print(system_names[-1], path[-1])\n",
    "print(f\"\\t{round(travel_distance)} ly -> {(await find_system(end.tolist())).get('name')} {np.round(end).tolist()}\")\n",
    "\n",
    "# open DB connection\n",
    "async with pgpool.acquire() as pgconnection:\n",
    "    \n",
    "    while cube_side > 20 and len(path) < pause_at:\n",
    "        # Fix cube_side to be divisible by two (easier on our center points, though not neccesary)\n",
    "        cube_side = round(2 * math.floor(cube_side/2))\n",
    "        wxz = 16 * cube_side\n",
    "        search_side = max(search_side, cube_side*9)\n",
    "        cube_radius = np.sqrt(np.sum(np.square([cube_side/2, cube_side/2, cube_side/2])))\n",
    "        def cube_center(point):\n",
    "            return [round(cube_side*math.floor(v/cube_side) + cube_side/2) for v in point]\n",
    "        def cube_location(point):\n",
    "            return [round(cube_side*math.floor(v/cube_side) + cube_side/2) for v in point]\n",
    "\n",
    "        # End of search condition \n",
    "        while travel_distance > 500 and len(path) < pause_at:\n",
    "            currentwp = cube_center(np.round(path[-1],2))\n",
    "\n",
    "            travel_distance = np.sqrt(np.sum(np.square(end - currentwp)))\n",
    "            direction = (end - currentwp)/travel_distance\n",
    "\n",
    "            # first estimated waypoint \n",
    "            wpdistance = math.floor(0.9*search_side/cube_side) * (\n",
    "                cube_side if len(path) > 2 and travel_distance > search_side else 2*cube_side)\n",
    "            nextwp = cube_center(np.round(currentwp + min(wpdistance, travel_distance) * direction,2))\n",
    "            sys.stdout.write(f\"\\nCurrent {str(currentwp)} to {str(nextwp)}\")\n",
    "\n",
    "            poiinfo = None\n",
    "                \n",
    "            poi_distances1 = np.linalg.norm(poi_systems[:, [0,1,2]] - currentwp, axis=1) \n",
    "            poi_distances2 = np.linalg.norm(poi_systems[:, [0,1,2]] - nextwp, axis=1)\n",
    "            poi_n = np.less(poi_distances1, wpdistance) & np.less(poi_distances2, wpdistance)\n",
    "            poi_targets = poi_systems[poi_n]\n",
    "            sys.stdout.write(f\" {poi_targets.shape[0]} POI's\")\n",
    "\n",
    "            if visit_poi and poi_targets.shape[0]>0 and target_system.get('name') not in poi_data:\n",
    "                nextpoiidx = np.argmin(poi_distances1[poi_n])\n",
    "                nextwp = cube_center(poi_systems[poi_n][nextpoiidx])\n",
    "                sys.stdout.write(f\" set nextwp: {nextwp}\")\n",
    "                poiinfo = [dict(system=r[0],name=r[1], description=r[2], coord=c) for r,c in zip(poi_systemnames[poi_n],poi_systems[poi_n])][nextpoiidx]\n",
    "                sys.stdout.write(f\" POI: {poiinfo['system']} {poiinfo['name']}\")\n",
    "                        \n",
    "            \n",
    "            location_cube = [round(cube_side*math.floor(v/cube_side)) for v in currentwp]\n",
    "            destination_cube = [round(cube_side*math.floor(v/cube_side)) for v in nextwp]\n",
    "            wxz = max(2*cube_side,min(wxz, cube_side*round(travel_distance/(6*cube_side)))) \n",
    "            if np.square(direction[0]) > np.square(direction[2]): # travel more along x then z axis\n",
    "                enclosure = [\n",
    "                    min(location_cube[0], destination_cube[0]) - cube_side, max(location_cube[0], destination_cube[0]) + cube_side + 1,\n",
    "                    min(-1500, location_cube[0], destination_cube[1]), max(1500, location_cube[0], destination_cube[1]),\n",
    "                    min(location_cube[2], destination_cube[2]) - wxz, max(location_cube[2], destination_cube[2]) + wxz+1,\n",
    "                ]    \n",
    "            else:\n",
    "                enclosure = [\n",
    "                    min(location_cube[0], destination_cube[0]) - wxz, max(location_cube[0], destination_cube[0]) + wxz+1,\n",
    "                    min(-1500, location_cube[0], destination_cube[1]), max(1500, location_cube[0], destination_cube[1]),\n",
    "                    min(location_cube[2], destination_cube[2]) - 1*cube_side, max(location_cube[2], destination_cube[2]) + 1*cube_side + 1,\n",
    "                ]    \n",
    "\n",
    "            t = [int(x) for x in enclosure]\n",
    "            extend = [x for x in zip(t[::2], t[1::2])]  \n",
    "            \n",
    "            # Note: from now work with the center-points of the cubes\n",
    "            full = np.unique(np.asarray([(round(cx + cube_side/2),round(cy + cube_side/2),round(cz + cube_side/2))  \n",
    "                                             for cx in range(*extend[0], cube_side) \n",
    "                                             for cy in range(*extend[1], cube_side) \n",
    "                                             for cz in range(*extend[2], cube_side)], \n",
    "                                        dtype=[(\"cx\",\"int64\"),(\"cy\",\"int64\"),(\"cz\",\"int64\")]))\n",
    "\n",
    "            sys.stdout.write(f\"\\rCurrent {str(currentwp)} to {str(nextwp)}, {str(full.shape[0])} cubes ...  {poi_targets.shape[0]} POI's\")\n",
    "            \n",
    "            candidate_cubes = await pgpool.fetch('''\n",
    "                SELECT ROUND($7*FLOOR(x/$7) + $7/2) AS cx, ROUND($7*FLOOR(y/$7) + $7/2) AS cy, ROUND($7*FLOOR(z/$7) + $7/2) AS cz, \n",
    "                        count(1)  filter(where not n) as starcount, \n",
    "                        ROUND(|/((AVG(x)-$8)^2 + (AVG(y)-$9)^2 + (AVG(z)-$10)^2)) distance,\n",
    "                        0 weight,\n",
    "                        count(*) filter(where n) as ncount\n",
    "                FROM systems \n",
    "                WHERE x >= $1 AND x <= $2 AND  y >= $3 AND y <= $4  AND z  >= $5 AND z <= $6 \n",
    "                GROUP BY cx, cy, cz\n",
    "\n",
    "            ''', *enclosure, cube_side, *end.tolist())\n",
    "\n",
    "            regions = np.unique(np.asarray(\n",
    "                [tuple(R) for R in candidate_cubes], \n",
    "                dtype=[(\"cx\",\"int64\"), (\"cy\",\"int64\"),(\"cz\",\"int64\"),(\"starcount\",\"float64\"), (\"distance\",\"float64\"), (\"weight\",\"float64\"),(\"ncount\",\"float64\")]))\n",
    "\n",
    "            sys.stdout.write(f\"\\rCurrent {str(currentwp)} to {str(nextwp)}, {str(regions.shape[0])} regions found ...\")\n",
    "            assert not regions.shape[0] > full.shape[0]\n",
    "            joined = join_by(\n",
    "                ('cx', 'cy','cz'), \n",
    "                full,regions, \n",
    "                jointype=\"outer\", usemask=False,\n",
    "                defaults = {\"starcount\":0, \"total\":np.nan, \"distance\":np.nan, 'weight':np.nan, 'ncount':0}\n",
    "            )\n",
    "\n",
    "            joined[\"distance\"] = np.round(np.sqrt(\n",
    "                np.square(joined[\"cx\"]-end[0]) + \n",
    "                np.square(joined[\"cy\"]-end[1]) + \n",
    "                np.square(joined[\"cz\"]-end[2]))) + np.round(np.sqrt(\n",
    "                np.square(joined[\"cx\"]-currentwp[0]) + \n",
    "                np.square(joined[\"cy\"]-currentwp[1]) + \n",
    "                np.square(joined[\"cz\"]-currentwp[2])))\n",
    "\n",
    "            joined['weight'] = np.log(1 + (1+joined['starcount'])/(1+joined['ncount']))\n",
    "            #joined['weight'] = np.round(np.log((3+joined['starcount'])/(1+joined['ncount'])) * joined[\"distance\"]/travel_distance)\n",
    "            #joined['weight'] = np.round(np.log((3+joined['starcount'])/(1+joined['ncount'])) * joined[\"distance\"])\n",
    "            #max_count = np.amax(joined['starcount'])\n",
    "\n",
    "            # Travel direction, z-x order and normal\n",
    "            if np.square(direction[0]) > np.square(direction[2]): # travel more along x then z axis\n",
    "                main_direction = direction[0]\n",
    "                main_order = ['cx', 'cz']\n",
    "                plane_normal = np.array([1,0,0])\n",
    "\n",
    "            else: # travel more along z then x axis\n",
    "                main_direction = direction[2]\n",
    "                main_order = ['cz','cx']\n",
    "                plane_normal = np.array([0,0,1])\n",
    "\n",
    "            if main_direction < 0:\n",
    "                joined.sort(order=main_order)\n",
    "                plane_normal *= -1\n",
    "            else:\n",
    "                joined[::-1].sort(order=main_order) # reverse sort\n",
    "\n",
    "            #('cx', 'cy', 'cz', 'starcount', 'distance', 'weight', 'ncount')\n",
    "            cubespace = np.zeros(shape=(joined.shape[0],len(joined.dtype)+1+2))\n",
    "            for ci, cn in zip(range(len(joined.dtype)), joined.dtype.names) :\n",
    "                cubespace[:,ci] = joined[cn]\n",
    "\n",
    "            candidate_waypoints = joined[np.less(joined['distance'], np.percentile(joined['distance'],8))]\n",
    "            waypoints = np.zeros(shape=(candidate_waypoints.shape[0],7))\n",
    "            waypoints[:,3] = candidate_waypoints['starcount'] * candidate_waypoints['weight'] \n",
    "            for ci, cn in zip([0,1,2], ['cx', 'cy','cz']) :\n",
    "                waypoints[:,ci] = candidate_waypoints[cn]\n",
    "\n",
    "            # calculate a weight for each line from currentwp to waypoints\n",
    "            # based on the distance of each cube in our extend and the starcount\n",
    "            #sys.stdout.write(f\", {waypoints.shape[0]} waypoints\")\n",
    "            for waypoint in waypoints:\n",
    "                # First calculate distances between cubes and travel 'lines'\n",
    "                # d = norm(np.cross(lp2-lp1, lp1-p3))/norm(lp2-lp1)\n",
    "                lp1 = waypoint[0:3]\n",
    "                lp2 = currentwp\n",
    "                p3 = cubespace[:,0:3]\n",
    "                \n",
    "                d = np.linalg.norm(np.cross(lp2-lp1, lp1-p3,axisb=1),axis=1)/max(np.linalg.norm(lp2-lp1),0.1)\n",
    "\n",
    "                w = np.less(d,4*cube_radius) # wider selection to get a relative density\n",
    "                n = np.less(d,2*cube_radius) # select cubes near the travel line\n",
    "                # sum and divide by traveldistance to get a measure of density\n",
    "                waypoint[4] = np.sum(cubespace[n][:,3]/(1+d[n])) / (1+np.sqrt(np.sum(np.square(lp1-lp2))))\n",
    "                waypoint[5] = np.mean(cubespace[n][:,3])\n",
    "                waypoint[6] = np.mean(cubespace[n][:,6])\n",
    "                waypoint[4] *= (1+np.mean(cubespace[n][:,3])) / (1+np.mean(cubespace[w][:,3]))\n",
    "\n",
    "            sys.stdout.write(f\"\\rCurrent {str(currentwp)} to {str(nextwp)}, weighed {str(waypoints.shape[0])} waypoint cubes\\r\")\n",
    "            path_info[system_names[-1]] = {'candidates':[], 'stations':[]}\n",
    "            waypoints = waypoints[np.less(waypoints[:,4], np.percentile(waypoints[:,4],15))]\n",
    "            \n",
    "            if candidate_cubes:\n",
    "                candidates = []\n",
    "\n",
    "                for weighed_target in waypoints[waypoints[:,4].argsort()]:\n",
    "                    sys.stdout.write(f\"\\rTarget, w={weighed_target[4]}: {np.round([weighed_target[k] for k in [0,1,2]])}\\r\")\n",
    "                    s = cube_side\n",
    "                    while not candidates and s < 200:\n",
    "                        s += cube_side\n",
    "                        target_coordinates = np.asarray([weighed_target[k] for k in [0,1,2]])\n",
    "                        if poiinfo:\n",
    "                            if 4*s > np.sqrt(np.sum(np.square(target_coordinates - poiinfo['coord']))):\n",
    "                                candidates = await find_nearby_systems(poiinfo['coord'],2*s, include_neutron=False)\n",
    "                                sys.stdout.write(f\"... Found {len(candidates)} POI for search\\r\")\n",
    "                            else:\n",
    "                                sys.stdout.write(f\" ... POI distance {np.sqrt(np.sum(np.square(target_coordinates - poiinfo['coord'])))} \\r\")\n",
    "                            \n",
    "                        if not candidates:\n",
    "                            candidates = await find_nearby_systems(target_coordinates, s, include_neutron=False)\n",
    "                            \n",
    "                        sys.stdout.write(f\"\\rFound {len(candidates)} candidates for {np.round([weighed_target[k] for k in [0,1,2]])} {s} ly search\\r\")\n",
    "\n",
    "                    if candidates:   \n",
    "                        for candidate in candidates: \n",
    "                            #candidate = candidates[0]\n",
    "                            #print(candidate)\n",
    "                            path_info[system_names[-1]]['candidates'].append(record_to_dict(candidate))\n",
    "                            sys.stdout.write(f\"\\rFound candidate {record_to_dict(candidate).get('name')} for {np.round([weighed_target[k] for k in [0,1,2]])} {s} ly search\\r\")\n",
    "                            #path_info[system_names[-1]]['stations'] += [(R.get('system'),R.get('station'), round(R.get('distance'))) for R in await find_nearby_stations([joined[0][k] for k in [\"cx\",\"cy\",\"cz\"]],300)]\n",
    "                            if candidate.get(\"name\") in system_names:\n",
    "                                sys.stdout.write(f\" ... is already a waypoint\")\n",
    "                                continue\n",
    "\n",
    "                            if np.round(np.sqrt(np.sum(np.square(path[-1]-end)))) < 2*cube_side+np.round(np.sqrt(np.sum(np.square(np.asarray([candidate.get(k) for k in [\"x\",\"y\",\"z\"]])-end)))):\n",
    "                                sys.stdout.write(f\" {path[-1]} is already closer to {end} than {[round(candidate.get(k)) for k in ['x','y','z']]} {candidate.get('name')}\")\n",
    "                                continue\n",
    "\n",
    "                            cube_density = weighed_target[3]\n",
    "                            path.append(np.asarray([candidate.get(k) for k in [\"x\",\"y\",\"z\"]]))\n",
    "                            sys.stdout.write(f\"\"\"\\r{system_names[-1]:26} {np.sqrt(np.sum(np.square(path[-1]-currentwp))):7.1f} ly ->\\t{candidate.get('name'):26} {weighed_target[4]:6.2f}\\t{np.sqrt(np.sum(np.square(path[-1]-end))):7.1f} ly remaining {' ':12}\"\"\")\n",
    "\n",
    "                            path_info[system_names[-1]]['nextwp']=dict(\n",
    "                                system=candidate.get('name'),\n",
    "                                distance=np.round(np.sqrt(np.sum(np.square(path[-1]-currentwp))),1),\n",
    "                                extend=extend,\n",
    "                                wxz=wxz,\n",
    "                                remaining=np.round(np.sqrt(np.sum(np.square(path[-1]-end))),1),\n",
    "                                weight=weighed_target[4],\n",
    "                                density=1000*weighed_target[5]/math.pow(cube_side,3),#1e6*weighed_target[3]/(np.power(cube_side/3.26,3)*weighed_target[6]),\n",
    "                                ndensity=1000*weighed_target[6]/math.pow(cube_side,3),#1e6*weighed_target[3]/(np.power(cube_side/3.26,3)*weighed_target[6]),\n",
    "                                cube=[weighed_target[k] for k in [0,1,2]],\n",
    "                                error=np.round(np.sqrt(np.sum(np.square(path[-1]-np.asarray(nextwp))))),\n",
    "                                poi=[dict(system=r[0],name=r[1], description=r[2], coord=c) for r,c in zip(poi_systemnames[poi_n],poi_systems[poi_n])]\n",
    "                            )\n",
    "                            system_names.append(candidate.get(\"name\"))\n",
    "                            break\n",
    "                        break\n",
    "\n",
    "                if np.sqrt(np.sum(np.square(np.asarray(cube_center(np.round(path[-1],2))) - currentwp))) < cube_side:\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                sys.stdout.write(f\"\\rCurrent {str(currentwp)} to {str(nextwp)}, no candidate-cubes\\r\")\n",
    "                break\n",
    "\n",
    "        cube_side = int(0.8*cube_side)\n",
    "        \n",
    "    print()    \n",
    "    with open('custom/route.json', 'wt') as of:\n",
    "        json.dump(dict(markers=[\n",
    "            dict(\n",
    "                pin='red',\n",
    "                text=f\"{I.get('system')}\\n{str(I.get('cube'))}\\n{str(I.get('extend'))}\",\n",
    "                x=I.get('cube',[])[0],\n",
    "                y=I.get('cube',[])[1],\n",
    "                z=I.get('cube',[])[2],\n",
    "                #**{k:v for k,v in dict(await find_system()).items() if k in ['x','y','z']}\n",
    "            )\n",
    "            for W,I in {WP:N.get('nextwp',{}) for WP, N in path_info.items()}.items() \n",
    "            if I.get('cube')\n",
    "            #for n,s in targetsystems.items()\n",
    "        ]), of,indent=4, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f540b0-f1f4-41de-9d7b-30d9fa2969d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    (\n",
    "        W,I.get('distance'), I.get('system'), I.get('remaining'), \n",
    "        I.get('error'),1000*I.get('weight',0),I.get('density'),I.get('ndensity',1)/I.get('density',1),\n",
    "        I.get('cube',[np.nan,np.nan,np.nan])[0],\n",
    "        I.get('cube',[np.nan,np.nan,np.nan])[1],\n",
    "        I.get('cube',[np.nan,np.nan,np.nan])[2],\n",
    "        I.get('extend'),\n",
    "        ' | '.join([f\"{p.get('system')} - {p.get('name')}\" for p in I.get('poi',[])])\n",
    "    ) \n",
    "    for W,I in {WP:N.get('nextwp',{}) for WP, N in path_info.items()}.items()],\n",
    "    columns=['waypoint', 'distance', 'nextwp', 'remaining','stray','weight', 'density','neutron','x','y','z','extend', 'poi']\n",
    ").set_index('waypoint').dropna()#.to_markdown(f'waypoints-{system_names[0]}-{system_names[-1]}.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc04cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(joined).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb06948-5ca8-41e9-ba52-efd2b2f13707",
   "metadata": {},
   "source": [
    "## Nav Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209ea8a-f17b-4a94-8246-53e2117fefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(edcompanion.edsm_api)\n",
    "importlib.reload(follow_log)\n",
    "from edcompanion.edsm_api import get_edsm_info\n",
    "from follow_log import follow_journal\n",
    "system_name = get_commander_position('immerlicht', os.getenv('EDSM_TOKEN')).get('system')\n",
    "#systems, system_name = follow_journal(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4956c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutron_db = {}\n",
    "with open('collected_neutron_systems.json', \"rt\") as jsonfile:\n",
    "    neutron_db=json.load(jsonfile)\n",
    "\n",
    "await pgpool.executemany(\n",
    "    \"\"\"INSERT INTO eddb.systems (name, x, y, z, n) \n",
    "        VALUES ($1, $2, $3, $4,TRUE) \n",
    "        ON CONFLICT (name) DO UPDATE SET n=TRUE\n",
    "    \"\"\", [(\n",
    "    s.get('StarSystem'),        \n",
    "    s.get('StarPos')[0],\n",
    "    s.get('StarPos')[1],\n",
    "    s.get('StarPos')[2]\n",
    "\n",
    ") for n, s in neutron_db.items() if 'N' == s.get('StarClass')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ec255-1420-4cbe-9694-26985c116bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logpath = \"/Users/fenke/Saved Games/Frontier Developments/Elite Dangerous\"\n",
    "navfile = os.path.join(logpath, \"NavRoute.json\")\n",
    "print(navfile)\n",
    "\n",
    "route = []\n",
    "edsm_info={}\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    with open(navfile, \"rt\") as jsonfile:\n",
    "        navroute = json.load(jsonfile)\n",
    "        #prettyprint(navroute)\n",
    "\n",
    "        for item in navroute.get('Route'):\n",
    "            N = item.get('StarSystem')\n",
    "            #print(N, *[c for c in item.get('StarPos')])\n",
    "            #assert False\n",
    "            await pgpool.execute(\n",
    "                \"\"\"INSERT INTO eddb.systems (name, x, y, z) \n",
    "                    VALUES ($1, $2, $3, $4) \n",
    "                    ON CONFLICT DO NOTHING\n",
    "                \"\"\",\n",
    "                N, *[c for c in item.get('StarPos')]\n",
    "            )\n",
    "\n",
    "            edsm_info = get_edsm_info(N)\n",
    "            route.append([\n",
    "                '*' if system_name==N else \">\" if not edsm_info else \" \" ,\n",
    "                \"#\" if item.get('StarClass') in ['A','F','G'] else \" \",\n",
    "                f\"{edsm_info.get('name'):26}\" if edsm_info else f\"{item.get('StarSystem'):26}\",\n",
    "                f\"{systems.get(N,item).get('StarClass',([B.get('subType').replace('Star', '') for B in edsm_info.get('bodies',[{}]) if B.get('isMainStar')]+[''])[0])}\", \n",
    "                f\"{len([B.get('discovery') for B in edsm_info.get('bodies',[])])} / {edsm_info.get('bodyCount',len(systems.get(N,{}).get('bodies',{})))}\",\n",
    "                np.round(np.sqrt(np.sum(np.square(np.asarray([c for c in item.get('StarPos')])-end))),1),\n",
    "                \n",
    "                \", \".join([f\"{round(c):6}\" for c in item.get('StarPos')]),\n",
    "                \"\".join([B.get('discovery',{}).get('commander','') for B in edsm_info.get('bodies',[{}]) if B.get('isMainStar') and B.get('discovery')]), \n",
    "\n",
    "            ])\n",
    "\n",
    "\n",
    "with pd.option_context('display.max_rows', len(route)+1, 'display.max_columns', len(route[0])+1):\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            route, \n",
    "            columns=[\"d\", \"s\", \"system\",\"class\", \"bodies\", \"remaining\", \"starpos\", \"commander\"])   \n",
    "    \n",
    "    ) #need display to show the dataframe when using with in jupyter\n",
    "    #some pandas stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe031d78-2d80-47f0-ae74-697832737721",
   "metadata": {},
   "source": [
    "## Thin-routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe50c88-67ef-4f0d-bebd-581d18d9ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(edcompanion.edsm_api)\n",
    "from edcompanion.edsm_api import get_edsm_info, get_systems_in_cube, get_systems_in_sphere, distance_between_systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c64aba-73b3-484b-a7c1-5f27baa6af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name='Pho Aoscs OS-U f2-26'\n",
    "\n",
    "target = dict(name=get_edsm_info(target_name,False).get('name'))\n",
    "target.update(dict(coords=np.asarray([get_edsm_info(target_name,False).get('coords').get(k) for k in ['x','y','z']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_name='Pho Aoscs OY-Z d13-10'\n",
    "\n",
    "current = dict(name=get_edsm_info(current_name,False).get('name'))\n",
    "current.update(dict(coords=np.asarray([get_edsm_info(current_name,False).get('coords').get(k) for k in ['x','y','z']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c7faf3-831d-4f78-9558-a98d09464025",
   "metadata": {},
   "outputs": [],
   "source": [
    "current=dict(\n",
    "    name=last_system.get('StarSystem'),\n",
    "    coords=np.asarray(last_system.get('StarPos'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885474d0-f48b-4f95-85ff-d7dceaa0aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance=np.ceil(np.sqrt(np.sum(np.square(current.get('coords')-target.get('coords')))))\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33e6a1-ac69-42a6-8a06-d8b517a254bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby = get_systems_in_cube(target['name'],100)\n",
    "nearby.sort(key = lambda S:S['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e06672",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby = get_systems_in_cube('Pho Aoscs OS-U f2-44',100)\n",
    "nearby.sort(key = lambda S:S['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2289a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby = get_systems_in_cube('Pho Aoscs BQ-P e5-4',100)\n",
    "nearby.sort(key = lambda S:S['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3256137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby = get_systems_in_cube('Pho Aoscs FW-N e6-7',100)\n",
    "nearby.sort(key = lambda S:S['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b349ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby = get_systems_in_cube('Pho Aoscs FW-N e6-5',100)\n",
    "nearby.sort(key = lambda S:S['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby = get_systems_in_cube('Pho Aoscs LS-B d13-16',100)\n",
    "nearby.sort(key = lambda S:S['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby = get_systems_in_cube('Pho Aoscs OY-Z d13-10',100)\n",
    "nearby.sort(key = lambda S:S['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ba9ce-c25e-4685-be29-625c2cb7e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[dict(**S, stars=[dict(name=B['name'], subtype=B['subType']) for B in get_edsm_info(S['name'])['bodies'] if B['type'] == 'Star']) for S in nearby if S['distance']<60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d70a962-a68a-47cf-b4d5-2db15dc83bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_with_info = [dict(**S, stars=[dict(name=B['name'], subtype=B['subType']) for B in get_edsm_info(S['name'])['bodies'] if B['type'] == 'Star']) for S in nearby]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977839f-df79-4eb2-9611-608c2152e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([S.get('name') for S in nearby_with_info for B in S.get('stars') if B.get('subtype') == 'Neutron Star' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a6552-30e8-468a-96db-aeaacba883cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v for k,v in zip(['x','y','z'], current.get('coords'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33efa6b-a778-4beb-b5dc-1b3740f7d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby.sort(key = lambda S:S['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62f8ca-d499-4754-b6b4-c6ea9e29b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_with_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c74116-21c3-4a1b-9abc-9fc19a7211ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "? nearby.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbfa62-6a5e-4d81-87bb-3b04de78b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.get('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33fedc-91cd-4bd3-a5f0-92abe248220f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87189581-79a0-463e-90e3-81e0c6d0856e",
   "metadata": {},
   "source": [
    "## edastro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11335526-aaee-42cf-b184-220be8c6e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_edsm_info('Pho Aoscs OY-Z d13-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae30be8-3d17-4acc-8ddd-0077aef099b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b751ab-d612-4d7a-8fd6-f672aaacd1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda194fc-0b9e-4050-85ad-d174fc31b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyprint(get_edsm_info('Drojia TB-S c18-0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7424323-6133-4fb0-89a8-59093280a816",
   "metadata": {},
   "source": [
    "# Routing V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b253ea3-58ee-4ff2-884d-2e86228218b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## interesting systems\n",
    "targetsystems = dict(\n",
    "    sol='Sol',\n",
    "    guardian_fsd1='HD 63154',\n",
    "    guardian_fsd2='Synuefe PX-J c25-8',\n",
    "    mel_brandon='Luchtaine',\n",
    "    marsha_hicks='Tir',\n",
    "    cygnus_x1='V1357 Cygni',\n",
    "    siteseeing_01='Phua Bre FB-O e6-257',\n",
    "    paradox_destiny='Prai Hypoo TX-B d4', # carrier\n",
    "    scorch_red_moon='Blue Hypooe VV-A c2-12',\n",
    "    inverness='Thraikoo PS-U e2-4',       # carrier\n",
    "    maerzenbecher='Hedgo GL-P c5-4',      # carrier\n",
    "    sanctuary='Syreadiae JX-F c0',        # carrier\n",
    "    sagitarius_a='Sagittarius A*',\n",
    "    traikeou='Traikeou WL-I c11-1',\n",
    "    thors_eye=\"Thor's Eye\",\n",
    "    annihilator='Great Annihilator',\n",
    "    nearby_station='Prielo TZ-G d10-4',\n",
    "    four_of_a_kind='Dryao Phylio AA-A h410'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76f9b6-2e7d-4c23-bc61-100177c2c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_system = await find_system(targetsystems.get('nearby_station'))\n",
    "end = np.round(np.asarray([target_system.get(k) for k in [\"x\",\"y\",\"z\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91418f-ea86-47b3-8839-c11868e966d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_system = await find_system(targetsystems.get('scorch_red_moon'))\n",
    "end = np.round(np.asarray([target_system.get(k) for k in [\"x\",\"y\",\"z\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294f969-d8fc-4239-9c61-64a8b79e2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_system = await find_system(targetsystems.get('guardian_fsd1'))\n",
    "end = np.round(np.asarray([target_system.get(k) for k in [\"x\",\"y\",\"z\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611a76b-4eb8-4b2d-9fa3-862dfd25dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_side = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2f1af-9f3c-48bc-872d-c737cff1b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_system = await find_system(system_name)\n",
    "start = np.round(np.asarray([start_system.get(k) for k in [\"x\",\"y\",\"z\"]]))\n",
    "path = [start]\n",
    "system_names = [(await find_system(start.tolist())).get('name')]\n",
    "path_info = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3d894-bf90-4b6b-874a-d704c62a2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pause_at = len(path)+5\n",
    "travel_distance = np.sqrt(np.sum(np.square(end-path[-1])))\n",
    "print(system_names[-1], path[-1])\n",
    "print(f\"\\t{round(travel_distance)} ly -> {(await find_system(end.tolist())).get('name')} {np.round(end).tolist()}\")\n",
    "\n",
    "cube_side = round(2 * math.floor(cube_side/2))\n",
    "\n",
    "\n",
    "async with pgpool.acquire() as pgconnection:\n",
    "    \n",
    "    while travel_distance > 2*cube_side and len(path) < pause_at:\n",
    "        currentwp = np.round(path[-1],2)\n",
    "\n",
    "        travel_distance = np.sqrt(np.sum(np.square(end - currentwp)))\n",
    "        direction = (end - currentwp)/travel_distance\n",
    "        nextwp = np.round(currentwp + 6*cube_side * direction,2)\n",
    "\n",
    "        #print(f\"Travelling from {system_names[-1]} {currentwp} in direction {np.round(direction,2)} towards {np.round(nextwp,2)}\")\n",
    "\n",
    "        cube_location = [round(cube_side*math.floor(v/cube_side)) for v in currentwp]\n",
    "        cube_destination = [round(cube_side*math.floor(v/cube_side)) for v in nextwp]\n",
    "\n",
    "        enclosure = [\n",
    "            min(cube_location[0], cube_destination[0]) - 2*cube_side, max(cube_location[0], cube_destination[0]) + 2*cube_side + 1,\n",
    "            min(cube_location[1], cube_destination[1]) - 4*cube_side, max(cube_location[1], cube_destination[1]) + 4*cube_side + 1,\n",
    "            min(cube_location[2], cube_destination[2]) - 2*cube_side, max(cube_location[2], cube_destination[2]) + 2*cube_side + 1,\n",
    "        ]    \n",
    "        \n",
    "        t = [int(x) for x in enclosure]\n",
    "        extend = [x for x in zip(t[::2], t[1::2])]  \n",
    "\n",
    "        #print(enclosure)\n",
    "        #print(extend)\n",
    "        \n",
    "        full = np.unique(np.asarray([(round(cx + cube_side/2),round(cy + cube_side/2),round(cz + cube_side/2))  \n",
    "                                         for cx in range(*extend[0], cube_side) \n",
    "                                         for cy in range(*extend[1], cube_side) \n",
    "                                         for cz in range(*extend[2], cube_side)], \n",
    "                                    dtype=[(\"cx\",\"int64\"),(\"cy\",\"int64\"),(\"cz\",\"int64\")]))\n",
    "        \n",
    "        #print('full', full.shape)\n",
    "        #print(pd.DataFrame(full).head())\n",
    "\n",
    "        candidate_cubes = await pgpool.fetch('''\n",
    "            SELECT ROUND($7*FLOOR(x/$7) + $7/2) AS cx, ROUND($7*FLOOR(y/$7) + $7/2) AS cy, ROUND($7*FLOOR(z/$7) + $7/2) AS cz, \n",
    "                    count(1) starcount, \n",
    "                    ROUND(|/((AVG(x)-$8)^2 + (AVG(y)-$9)^2 + (AVG(z)-$10)^2)) distance,\n",
    "                    0 weight\n",
    "            FROM systems \n",
    "            WHERE x >= $1 AND x <= $2 AND  y >= $3 AND y <= $4  AND z  >= $5 AND z <= $6 \n",
    "            GROUP BY cx, cy, cz\n",
    "            ORDER BY starcount\n",
    "\n",
    "        ''', *enclosure, cube_side, *end.tolist())\n",
    "        #print(pd.DataFrame([R for R in candidate_cubes]).head())\n",
    "        \n",
    "        regions = np.unique(np.asarray(\n",
    "            [tuple(R) for R in candidate_cubes], \n",
    "            dtype=[(\"cx\",\"int64\"), (\"cy\",\"int64\"),(\"cz\",\"int64\"),(\"starcount\",\"float64\"), (\"distance\",\"float64\"), (\"weight\",\"float64\")]))\n",
    "        #print('regions', regions.shape)\n",
    "        #print(pd.DataFrame(regions).head())\n",
    "\n",
    "        assert not regions.shape[0] > full.shape[0]\n",
    "        joined = join_by(\n",
    "            ('cx', 'cy','cz'), \n",
    "            full,regions, \n",
    "            jointype=\"outer\", usemask=False,\n",
    "            defaults = {\"starcount\":0, \"distance\":np.nan, 'weight':np.nan}\n",
    "        )\n",
    "\n",
    "        #print('joined', joined.shape)\n",
    "\n",
    "        n = ~np.isfinite(joined[\"distance\"])\n",
    "\n",
    "        #joined[\"starcount\"][n] = 1\n",
    "        joined[\"distance\"][n] = np.round(np.sqrt(\n",
    "            np.square(joined[n][\"cx\"]-end[0]) + \n",
    "            np.square(joined[n][\"cy\"]-end[1]) + \n",
    "            np.square(joined[n][\"cz\"]-end[2])))\n",
    "        \n",
    "        #joined['weight'][n] = np.round(np.sqrt(1+joined['starcount'][n]) * joined[\"distance\"][n])\n",
    "        #joined['weight'][~n] = np.round(np.sqrt(joined['starcount'][~n]) * joined[\"distance\"][~n])\n",
    "        #joined['weight'] = np.round(np.sqrt(1+joined['starcount']) * joined[\"distance\"])\n",
    "        #joined['weight'] = np.round(np.sqrt(3+joined['starcount']) * joined[\"distance\"])\n",
    "        joined['weight'] = np.round(np.log(3+joined['starcount']) * joined[\"distance\"])\n",
    "        #joined.sort(order=[\"distance\"])\n",
    "        #print(\"full\\n\",pd.DataFrame(joined[n]).head(7))\n",
    "        #print(\"regions\\n\", pd.DataFrame(joined[~n]).head(7))\n",
    "\n",
    "        joined.sort(order=['weight', \"distance\",\"starcount\"])        \n",
    "        #print(pd.DataFrame(joined).head(30))\n",
    "        path_info[system_names[-1]] = {'cubes':np.asarray(joined[0:3]), 'candidates':[], 'stations':[]}\n",
    "        \n",
    "        if candidate_cubes:\n",
    "            candidates = []\n",
    "            s = 30\n",
    "            while not candidates and s < 300:\n",
    "                s += 20\n",
    "                candidates = await find_nearby_systems([joined[0][k] for k in [\"cx\",\"cy\",\"cz\"]], s)\n",
    "                \n",
    "            candidate = candidates[0]\n",
    "            #print(candidate)\n",
    "            path_info[system_names[-1]]['candidates'].append(record_to_dict(candidate))\n",
    "            #path_info[system_names[-1]]['stations'] += [(R.get('system'),R.get('station'), round(R.get('distance'))) for R in await find_nearby_stations([joined[0][k] for k in [\"cx\",\"cy\",\"cz\"]],300)]\n",
    "            if candidate.get(\"name\") in system_names:\n",
    "                break\n",
    "                \n",
    "            if np.round(np.sqrt(np.sum(np.square(path[-1]-end)))) < 2*cube_side+np.round(np.sqrt(np.sum(np.square(np.asarray([candidate.get(k) for k in [\"x\",\"y\",\"z\"]])-end)))):\n",
    "                break\n",
    "\n",
    "            path.append(np.asarray([candidate.get(k) for k in [\"x\",\"y\",\"z\"]]))\n",
    "            print(f\"\"\"{system_names[-1]:26} {np.sqrt(np.sum(np.square(path[-1]-currentwp))):.1f} ly towards\\t{candidate.get('name'):26} ({round(joined[0]['starcount'])})\\t{np.sqrt(np.sum(np.square(path[-1]-end))):.1f} ly remaining\"\"\")\n",
    "             \n",
    "            path_info[system_names[-1]]['nextwp']=dict(\n",
    "                system=candidate.get('name'),\n",
    "                distance=np.round(np.sqrt(np.sum(np.square(path[-1]-currentwp))),1),\n",
    "                remaining=np.round(np.sqrt(np.sum(np.square(path[-1]-end))),1),\n",
    "                weight=round(np.sqrt(np.sum(np.square(path[-1]-end)))/joined[0]['weight'],2),\n",
    "                density=joined[0]['starcount']/np.power(cube_side/3.26,3),\n",
    "                cube=[joined[0][k] for k in [\"cx\",\"cy\",\"cz\"]],\n",
    "                error=np.round(np.sqrt(np.sum(np.square(path[-1]-[joined[0][k] for k in [\"cx\",\"cy\",\"cz\"]]))))\n",
    "            )\n",
    "            system_names.append(candidate.get(\"name\"))\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c94c90-2af5-417d-9390-519be8ad8eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb6085-a437-4898-8554-c8dd4d614aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    (\n",
    "        W,I.get('distance'), I.get('system'), I.get('remaining'), \n",
    "        I.get('error'),I.get('weight'),I.get('density'), \n",
    "        I.get('cube',[np.nan,np.nan,np.nan])[1]) \n",
    "    for W,I in {WP:N.get('nextwp',{}) for WP, N in path_info.items()}.items()],\n",
    "    columns=['waypoint', 'distance', 'nextwp', 'remaining','error','weight','density','y']\n",
    ").set_index('waypoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b2826-77c6-4513-a97d-71a332d852e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with aiohttp.ClientSession() as session:\n",
    "\n",
    "    info = await get_edsm_info(session, system_name)\n",
    "    prettyprint(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe1087d-5e37-4e26-937a-f6941b36202a",
   "metadata": {},
   "source": [
    "## Navigation Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5e269-c227-4b66-9e3f-f9e5f9e472db",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath = \"/Users/fenke/Saved Games/Frontier Developments/Elite Dangerous\"\n",
    "logfile = os.path.join(logpath, \"NavRoute.json\")\n",
    "print(logfile)\n",
    "route = []\n",
    "edsm_info={}\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    with open(logfile, \"rt\") as jsonfile:\n",
    "        navroute = json.load(jsonfile)\n",
    "        #prettyprint(navroute)\n",
    "\n",
    "        for item in navroute.get('Route'):\n",
    "            N = item.get('StarSystem')\n",
    "            #print(N, *[c for c in item.get('StarPos')])\n",
    "            #assert False\n",
    "            await pgpool.execute(\n",
    "                \"\"\"INSERT INTO eddb.systems (name, x, y, z) \n",
    "                    VALUES ($1, $2, $3, $4) \n",
    "                    ON CONFLICT DO NOTHING\n",
    "                \"\"\",\n",
    "                N, *[c for c in item.get('StarPos')]\n",
    "            )\n",
    "\n",
    "            if N not in edsm_info_cache:\n",
    "                async with session.get('https://www.edsm.net/api-system-v1/bodies', params=dict(systemName=N)) as req:\n",
    "                    edsm_info_cache[N] = await req.json()\n",
    "            edsm_info = await get_edsm_info(session, N)\n",
    "            route.append([\n",
    "                '*' if system_name==N else \">\" if not edsm_info else \" \" ,\n",
    "                \"#\" if item.get('StarClass') in ['A','F','G'] else \" \",\n",
    "                f\"{edsm_info.get('name'):26}\" if edsm_info else f\"{item.get('StarSystem'):26}\",\n",
    "                f\"{systems.get(N,item).get('StarClass',([B.get('subType').replace('Star', '') for B in edsm_info.get('bodies',[{}]) if B.get('isMainStar')]+[''])[0])}\", \n",
    "                f\"{len([B.get('discovery') for B in edsm_info.get('bodies',[])])} / {edsm_info.get('bodyCount',len(systems.get(N,{}).get('bodies',{})))}\",\n",
    "                \"|\".join([\n",
    "                    B.get('name','').replace(edsm_info.get('name'),'') \n",
    "                    for B in (edsm_info.get('bodies',[]) if edsm_info else systems.get(N,{}).get('bodies',{}).values())\n",
    "                    for k in ['earth', 'water', 'ammonia', 'metal'] \n",
    "                    if k in B.get('subType','').lower()]),\n",
    "\n",
    "            ])\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(\n",
    "    route, \n",
    "    columns=[\"d\", \"s\", \"system\",\"class\", \"bodies\", \"valuable\"])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaaa2fd-9adc-4460-b667-11e0c12eb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "systems.get('Zejai KY-Z d13-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d946a9-fbbc-4b72-a8a7-8938a60db961",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies.get(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d55c296-4851-4a4b-b33a-4bf10b976e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876da9e-bc73-4a55-8cc3-65c39b42df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5cbddd-0234-424e-9964-a2479a3dbcbf",
   "metadata": {},
   "source": [
    "# Scratchpad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4abee0e-870e-4ea6-af8b-e4282244a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutron_planes = await pgpool.fetch('''\n",
    "    SELECT ROUND($5*FLOOR(y/$5) + $5/2) AS cy, \n",
    "            count(*) starcount, \n",
    "            count(*) filter (WHERE ) as neutron_count\n",
    "    FROM systems \n",
    "    WHERE x >= $1 AND x <= $2 AND z  >= $3 AND z <= $4\n",
    "    GROUP BY cy\n",
    "\n",
    "''', *enclosure, cube_side, *end.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4867f-a403-4033-a0b3-681ceaac7b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(x, y=None, fname=None, **kwargs):\n",
    "\n",
    "    '''\n",
    "\n",
    "    Create a 1 dimensional x-plot or 2-dimensional (y versus x) plot.\n",
    "\n",
    "    Parameters\n",
    "    x: The x coordinates of the points or line nodes.\n",
    "    y: The y coordinates of the points or line nodes.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Only plot x-coördinates.\n",
    "    if y is None:\n",
    "        plt.plot(x, **kwargs)\n",
    "\n",
    "    # Plot both x and y coördinates.\n",
    "    else:\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.plot(x, y, **kwargs)\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname)\n",
    "\n",
    "\n",
    "def duoplot(x1, y1, x2, y2, xlabel, y1label, y2label, **kwargs):\n",
    "\n",
    "    fig, ax1 = plt.subplots(**kwargs)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    color = 'tab:orange'\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_ylabel(y1label, color=color)\n",
    "    l1, *a1 = ax1.plot(x1, y1, color=color, label=y1label)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(y2label, color=color)  # we already handled the x-label with ax1\n",
    "    l2, *a2 = ax2.plot(x2, y2, color=color, label=y2label)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.legend(handles=[l1,l2])\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax1, ax2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e6495",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_commander_position('immerlicht', os.getenv('EDSM_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def star_distribution(side, x_between, z_between):\n",
    "cube_side = 120\n",
    "x_between = [-4000, 4000]\n",
    "z_between = [-0, 4000]\n",
    "\n",
    "y_extend_record = dict(await pgpool.fetchrow('''\n",
    "    SELECT min(y), max(y)\n",
    "    FROM eddb.systems\n",
    "    where y < 3000 and y > -3000\n",
    "'''))\n",
    "\n",
    "cube_side = int(2 * np.round(cube_side/2))\n",
    "\n",
    "y_between = [int(cube_side * np.ceil(y_extend_record['min']/cube_side)), int(cube_side * np.floor(y_extend_record['max']/cube_side))]\n",
    "x_between = [int(cube_side * np.ceil(min(*x_between)/cube_side)), int(cube_side * np.floor(max(*x_between)/cube_side))]\n",
    "z_between = [int(cube_side * np.ceil(min(*z_between)/cube_side)), int(cube_side * np.floor(max(*z_between)/cube_side))]\n",
    "\n",
    "extend = [x_between, y_between, z_between]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0170cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution along Axis\n",
    "# \n",
    "galactic_axis = 'y'                     \n",
    "candidate_cubes = await pgpool.fetch(f'''\n",
    "    SELECT ROUND($7*FLOOR({galactic_axis}/$7) + $7/2) AS c{galactic_axis}, \n",
    "            count(1) filter(where not n) as starcount, \n",
    "            count(*) filter(where n) as ncount\n",
    "    FROM systems \n",
    "    WHERE x >= $1 AND x <= $2 AND  y >= $3 AND y <= $4  AND z  >= $5 AND z <= $6 \n",
    "    GROUP BY c{galactic_axis}\n",
    "\n",
    "''', *extend[0], *extend[1], *extend[2], cube_side)\n",
    "\n",
    "regions = np.unique(np.asarray(\n",
    "    [tuple(R) for R in candidate_cubes], \n",
    "    dtype=[(f\"c{galactic_axis}\",\"int64\"),(\"starcount\",\"float64\"), (\"ncount\",\"float64\")]))\n",
    "\n",
    "stardistdata = np.zeros(shape=(regions.shape[0],len(regions.dtype)+1))\n",
    "stardistdata[:,len(regions.dtype)] = np.log(1+ (1+regions['ncount'])/(1+regions['starcount']))\n",
    "for ci, cn in zip(range(len(regions.dtype)), regions.dtype.names) :\n",
    "    stardistdata[:,ci] = regions[cn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(stardistdata[:,0], np.log((1+stardistdata[:,1])/(1+stardistdata[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "duoplot(stardistdata[:,0], stardistdata[:,1],stardistdata[:,0], stardistdata[:,2], f\"{galactic_axis}\", 'stars', 'neutron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "duoplot(stardistdata[:,0], np.log(1+stardistdata[:,1]),stardistdata[:,0], np.log(1+stardistdata[:,2]), f\"{galactic_axis}\", 'stars', 'neutron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f46ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bbaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb29c5-b7bf-4561-99e8-be4185b55d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f854da-24cb-454d-b002-509a51cb96e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ff01a-2666-4f88-b038-1092851c1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L(object):\n",
    "    ldata = \"L\"\n",
    "    def get_rdata(self):\n",
    "        return self.get_data()\n",
    "    \n",
    "class R(object):\n",
    "    rdata = \"R\"\n",
    "    def get_ldata(self):\n",
    "        return self.get_data()\n",
    "    \n",
    "class C(L,R):\n",
    "    cdata = \"C\"\n",
    "    def get_data(self):\n",
    "        return self.ldata + self.rdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e03237a-03cb-405b-9587-e4e16c3e5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5087ee5-bed3-4e99-944e-142a6ba0d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.get_ldata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74763a7-eab4-4385-b4b8-acf6f6f3c456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62855ea8-ab4f-462b-ba4d-339848298cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f993a-dccd-4f9e-9706-2ccf8e60024e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655095a-e2b3-4ce9-bf00-5782db4c81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query(select_obj):\n",
    "    buildquery_context = {}\n",
    "    the_query = f\"\"\"\n",
    "        SELECT {build_select(select_obj.get('select',[]))}\n",
    "        FROM   {build_from_item(select_obj.get('from',[]))}\n",
    "        WHERE  {build_condition(select_obj.get('where',[]))}\n",
    "    \"\"\"\n",
    "    \n",
    "    return the_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e0677-9c8e-4a62-862e-09be43e7a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_select(select_items):\n",
    "    if isinstance(select_items, str):\n",
    "        return select_items\n",
    "    #elif isinstance(select_items, list):\n",
    "    \n",
    "    return select_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3ba23-47ed-4d5d-a3e6-d52020aa0c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def build_from_item(from_item):\n",
    "    if not from_item:\n",
    "        return ' '\n",
    "    if isinstance(from_item, str):\n",
    "        return f\" {str(from_item)} \"\n",
    "    elif isinstance(from_item,list):\n",
    "        return f\" {str(from_item[0])} AS {str(from_item[1])} \"\n",
    "    elif isinstance(from_item, dict):\n",
    "        return f\"\"\" \n",
    "            {build_from_item(from_item.get('items')[0])} \n",
    "            {str(from_item.get('type')).upper()} {build_from_item(from_item.get('items')[1])}\n",
    "            ON {build_condition(from_item.get('on'))}\n",
    "        \"\"\" + f\"\"\"\n",
    "            AS {str(from_item.get('as'))}\n",
    "        \"\"\" if str(from_item.get('as', '')) else \"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2287f-b75f-46f8-8840-6e9753e52caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_condition(condition):\n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0e819-1c1c-4f9d-af55-8a77565e8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_from_item(\n",
    "        \"systems \"+\n",
    "        \"LEFT JOIN populated \" +\n",
    "        \"ON systems.name = populated.systemname \"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9041e5-d7ff-42cf-8254-a020d0a58a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec544566-4775-4052-a024-7979e59e130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.pop(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799f7f2-20e9-4f15-b7bd-e0432f8b0ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f686ecc-26c6-4ac3-9d95-6fcccd25e830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d3dcd-9cc8-4dd4-b944-3a1b2e7a03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(build_query({\n",
    "    \"select\":\"*\",\n",
    "    \"from\": {\n",
    "        \"items\":['systems', 'populated'],\n",
    "        \"type\":'inner join',\n",
    "        \"on\":'systems.name = populated.systemname'\n",
    "    }\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f44e4c-0f53-407b-8208-974050a0ed9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72943088-c143-4040-9b56-fe2d7e803bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494bfdb-bf1e-40c1-9256-e1341a14f65d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625c8cd-0912-4d44-b050-e58a194f2821",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with aiohttp.ClientSession() as session:\n",
    "    \n",
    "    edsm_info = await get_edsm_info(session, 'HIP 75281')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea788f1-5125-40ac-a60f-31d7f1cfb654",
   "metadata": {},
   "outputs": [],
   "source": [
    "edsm_info['bodies'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330b8ed-4fc8-4dbc-9aeb-bc8d54076f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(N) for N,S in edsm_info_cache.items() for B in S.get('bodies',[] if B.get('isMainStar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3069d-3837-486f-bbd2-ea63438b69db",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath = \"/Users/fenke/Saved Games/Frontier Developments/Elite Dangerous\"\n",
    "\n",
    "\n",
    "def edc_track_journal(journalpath=logpath, journalglob=\"journal.22*\"):\n",
    "    # Generator\n",
    "\n",
    "    try:\n",
    "        logfiles = sorted(glob.glob(os.path.join(logpath, \"journal.22*.01.log\")))\n",
    "        current_journal = logfiles[-1]\n",
    "\n",
    "        with open(current_journal, \"rt\") as journalfile:\n",
    "            line = journalfile.readline()\n",
    "            item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "            print(\n",
    "                f\"{'Odyssey' if item.get('Odyssey') else 'Horizons'}\",\n",
    "                f\"version {item.get('gameversion')}\"\n",
    "                f\"\\nCurrent Journal: {current_journal}\"\n",
    "            ) \n",
    "\n",
    "            while True:# not shutdown_seen:\n",
    "                line = journalfile.readline()\n",
    "                if not line:\n",
    "                    time.sleep(0.3)\n",
    "                    continue\n",
    "\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "\n",
    "                yield item\n",
    "                if item.get('event', '') == 'Shutdown':\n",
    "                    print(f\"Shutdown\")\n",
    "                    break\n",
    "\n",
    "    except KeyboardInterrupt as kbi:\n",
    "        print(f\"Keyboard Interrupt\")\n",
    "        pass\n",
    "\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644abc6c-c43f-4b58-bac6-6e48cf05870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = ed_follow_journal()\n",
    "for I in logger:\n",
    "    pass\n",
    "    #print(I.get('event'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c6bc2-d452-4ea9-a72d-f9f30364c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb154d7-9ee5-4d11-bb09-06402a7957dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d18d81-ba37-4a18-89fb-f25c9453705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.signature(find_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638dd29-2166-4e21-9940-3ccfd08d12db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f3147c-4c56-46b4-a5da-d9aef5e86188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7cf62d2-cdb1-483d-919d-93cbbe09739f",
   "metadata": {},
   "source": [
    "# Analyzing bodies\n",
    "\n",
    "We want to get data that may tell us if there's a pattern relating\n",
    "Starclass with materials found on planets - preferably with \n",
    "'signals'/'volcanism'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e913138-924a-444a-9b5d-8104c72d47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_values = {\n",
    "    'Water world': 415613,\n",
    "    'Earth-like world': 1126206,\n",
    "    'Ammonia world': 597762,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ae30a-6241-40a9-8776-0cfe201fe4e6",
   "metadata": {},
   "source": [
    "## First Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24389c33-02bb-45a6-a15e-5244bb228640",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"E:/data/eddb/galaxy_1day.json.gz\"\n",
    "#filename = \"E:/data/eddb/galaxy_7days.json.gz\"\n",
    "#filename = \"E:/data/eddb/galaxy_1month.json.gz\"\n",
    "\n",
    "\n",
    "spectral_class_list = set(sorted([\n",
    "    'A0', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', \n",
    "    'AeBe0', 'AeBe1', 'AeBe2', 'AeBe3', 'AeBe4', 'AeBe5', 'AeBe6', 'AeBe7', 'AeBe8', 'AeBe9', \n",
    "    'B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', \n",
    "    'C1', 'C5', 'CJ5', 'CN5', \n",
    "    'F', 'F0', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', \n",
    "    'G0', 'G1', 'G2', 'G3', 'G4', 'G5', 'G6', 'G7', 'G8', 'G9', \n",
    "    'K0', 'K1', 'K2', 'K3', 'K4', 'K5', 'K6', 'K7', 'K8', 'K9', \n",
    "    'L0', 'L1', 'L2', 'L3', 'L4', 'L5', 'L6', 'L7', 'L8', 'L9', \n",
    "    'M0', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', \n",
    "    'MS0', 'MS2', 'MS3', 'MS4', 'MS5', \n",
    "    'O0', 'O5', 'O6', 'O7', 'O8', 'O9', \n",
    "    'S0', 'S1', 'S2', 'S3', 'S4', 'S5', \n",
    "    'T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', \n",
    "    'TTS0', 'TTS1', 'TTS2', 'TTS3', 'TTS4', 'TTS5', 'TTS6', 'TTS7', 'TTS8', 'TTS9', \n",
    "    'W5', 'WC0', 'WC5', 'WN0', 'WNC0', 'WO0', \n",
    "    'Y0', 'Y1', 'Y2', 'Y3', 'Y4', 'Y5', 'Y6',\n",
    "    'CJ4', 'CN4', 'K', 'MS1',\n",
    "    'C0', 'C3', 'C4', 'C6', 'O4', 'W6', 'W9', 'WN5'\n",
    "]))\n",
    "spectral_class_map = {s:i for i,s in zip(range(len(spectral_class_list)), spectral_class_list)}\n",
    "\n",
    "luminosity_list = set(sorted([\n",
    "    'I', 'II', 'III', 'IIIa', 'IIIab', 'IIIb', 'IIa', 'IIab', \n",
    "    'IV', 'IVa', 'IVab', 'IVb', 'Ia0', 'Iab', 'Ib', \n",
    "    'O', 'V', 'VI', 'VII', 'Va', 'Vab', 'Vb', 'Vz',\n",
    "    'IIb', 'Ia'\n",
    "]))\n",
    "luminosity_map = {s:i for i,s in zip(range(len(luminosity_list)), luminosity_list)}\n",
    "\n",
    "magnitude_list = set([str(m) for m in range(-22,27)])\n",
    "magnitude_map = {s:i for i,s in zip(range(len(magnitude_list)), magnitude_list)}\n",
    "\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(7*filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "#itemdb =[]\n",
    "data = []\n",
    "\n",
    "spectral_class_map = recreate_map(spectral_class_map)\n",
    "luminosity_map = recreate_map(luminosity_map)\n",
    "magnitude_map = recreate_map(magnitude_map)\n",
    "terraform_map = {}\n",
    "\n",
    "mapping = dict(\n",
    "    spectralClass=lambda X: spectral_class_map.get(X,0),\n",
    "    luminosity=lambda X: luminosity_map.get(X,0),\n",
    "    absoluteMagnitude=lambda X: round(X)\n",
    ")\n",
    "\n",
    "dumped = dict(systems=0, count=0, total=0)\n",
    "missed = dict(\n",
    "    spectral=set(),\n",
    "    luminosity=set(),\n",
    "    magnitude=set(),\n",
    "    terraform=set()\n",
    ")\n",
    "mapcat = dict(\n",
    "    spectral=spectral_class_map,\n",
    "    luminosity=luminosity_map,\n",
    "    magnitude=magnitude_map,\n",
    "    terraform=terraform_map\n",
    ")\n",
    "def catmap(cat, value):\n",
    "    if value not in mapcat[cat]:\n",
    "        missed[cat].add(value)\n",
    "        return np.nan\n",
    "    else:\n",
    "        return mapcat[cat][value]\n",
    "    \n",
    "#totals = {C:dict(systems=0, count=0, total=0) for C in spectral_class_map }\n",
    "totals = {C:dict(systems=0, count=0, total=0) for C in set([s[:-1] for s in spectral_class_map]) }\n",
    "\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with gzip.open(filename, 'rt') as jsonfile:\n",
    "\n",
    "    firstline = jsonfile.readline()\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                # This is where we decode the item and get data from it ----------------\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                system_count += 1\n",
    "                #itemdb.append(item)\n",
    "                #continue\n",
    "                # get the data we need\n",
    "                bodies = item.get('bodies',[])\n",
    "                if not bodies:\n",
    "                    continue\n",
    "                if item.get('bodyCount',0) != len(bodies):\n",
    "                    continue\n",
    "                    \n",
    "                mainstars = [B for B in bodies if B.get('mainStar') and B.get('spectralClass')]\n",
    "                if not mainstars:\n",
    "                    continue\n",
    "                mainstar = mainstars[0]\n",
    "                data.append(\n",
    "                    [ # Xn\n",
    "                        item.get('name',''),\n",
    "                        item.get('bodyCount',len(bodies)),\n",
    "                        len([B for B in bodies if B.get('spectralClass') and not B.get('mainStar') ]),\n",
    "                        round(sum([B.get('solarMasses') for B in bodies if B.get('spectralClass')])),\n",
    "                        #spectral_class_map.get(mainstar.get('spectralClass'),np.nan),\n",
    "                        #luminosity_map.get(mainstar.get('luminosity'),np.nan),\n",
    "                        #magnitude_map.get(str(round(mainstar.get('absoluteMagnitude'))),np.nan),\n",
    "                        catmap('spectral', mainstar.get('spectralClass')),\n",
    "                        catmap('luminosity', mainstar.get('luminosity')),\n",
    "                        catmap('magnitude', str(round(mainstar.get('absoluteMagnitude')))),\n",
    "                        int(bool([True for b in bodies if b.get('terraformingState') == 'Terraformable'])), # y1\n",
    "                        int(bool([True for b in bodies if b.get('subType') in planet_values])) # y2\n",
    "                    ])\n",
    "                \n",
    "                    \n",
    "            # keep our programmer up to date of progress\n",
    "            print(f\"{count}/{est_count}\\t{100*count/est_count:3.2f}%, {int(system_count / (time.process_time() - start)):6} /s,\\t{system_count:9} systems, {((est_count - count) * (time.process_time() - start)/count):5.1f} seconds remaining\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Empy chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    "            \n",
    "adata = np.asarray(np.asarray(data)[:,1:], dtype=float)\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7a13c-1142-42cc-93ba-4c78d8de53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "missed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f715f-28d3-435d-9131-de9e80e521c2",
   "metadata": {},
   "source": [
    "## Next Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb855250-619f-48bc-853c-215f3b78a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_class_list = set(sorted([\n",
    "    'A0', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', \n",
    "    'AeBe0', 'AeBe1', 'AeBe2', 'AeBe3', 'AeBe4', 'AeBe5', 'AeBe6', 'AeBe7', 'AeBe8', 'AeBe9', \n",
    "    'B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', \n",
    "    'C1', 'C5', 'CJ5', 'CN5', \n",
    "    'F', 'F0', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', \n",
    "    'G0', 'G1', 'G2', 'G3', 'G4', 'G5', 'G6', 'G7', 'G8', 'G9', \n",
    "    'K0', 'K1', 'K2', 'K3', 'K4', 'K5', 'K6', 'K7', 'K8', 'K9', \n",
    "    'L0', 'L1', 'L2', 'L3', 'L4', 'L5', 'L6', 'L7', 'L8', 'L9', \n",
    "    'M0', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', \n",
    "    'MS0', 'MS2', 'MS3', 'MS4', 'MS5', \n",
    "    'O0', 'O5', 'O6', 'O7', 'O8', 'O9', \n",
    "    'S0', 'S1', 'S2', 'S3', 'S4', 'S5', \n",
    "    'T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', \n",
    "    'TTS0', 'TTS1', 'TTS2', 'TTS3', 'TTS4', 'TTS5', 'TTS6', 'TTS7', 'TTS8', 'TTS9', \n",
    "    'W5', 'WC0', 'WC5', 'WN0', 'WNC0', 'WO0', \n",
    "    'Y0', 'Y1', 'Y2', 'Y3', 'Y4', 'Y5', 'Y6',\n",
    "    'CJ4', 'CN4', 'K', 'MS1',\n",
    "    'C0', 'C3', 'C4', 'C6', 'O4', 'W6', 'W9', 'WN5'\n",
    "]))\n",
    "spectral_class_map = {s:i for i,s in zip(range(len(spectral_class_list)), spectral_class_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8295c-a010-4575-bf9b-edc031905f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "luminosity_list = set(sorted([\n",
    "    'I', 'II', 'III', 'IIIa', 'IIIab', 'IIIb', 'IIa', 'IIab', \n",
    "    'IV', 'IVa', 'IVab', 'IVb', 'Ia0', 'Iab', 'Ib', \n",
    "    'O', 'V', 'VI', 'VII', 'Va', 'Vab', 'Vb', 'Vz',\n",
    "    'IIb', 'Ia'\n",
    "]))\n",
    "luminosity_map = {s:i for i,s in zip(range(len(luminosity_list)), luminosity_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468ec6c-c4db-4d6e-bb35-103280d37a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_list = sorted([m for m in range(-15,27)])\n",
    "magnitude_map = {s:i for i,s in zip(range(len(magnitude_list)), magnitude_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041448ca-e4aa-4899-bc8b-360008301f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_sequence=[\"O\", \"B\", \"A\", \"F\", \"G\", \"K\", \"M\"]\n",
    "main_subclass=[str(c) for c in range(10)]\n",
    "main_sequence_map = {s:i for i,s in zip(range(len(main_sequence)), main_sequence)}\n",
    "main_subclass_map = {s:i for i,s in zip(range(len(main_subclass)), main_subclass)}\n",
    "\n",
    "main_spectral_map={\n",
    "    q+c:dict(seq=main_sequence_map[q], sub=main_subclass_map[c]) \n",
    "    for q in main_sequence for c in main_subclass\n",
    "    if q+c in spectral_class_map\n",
    "}\n",
    "#  {s:i for i,s in zip(range(len(templist)), templist)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05745e9f-42aa-4782-aa09-ed6e590b8fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_luminosity=[l for l in [\n",
    "    l for l in [\n",
    "        m+s for m in ['I','II','III','IV','V','VI','VII','VIII'] for s in ['z','a', 'ab','','b']\n",
    "    ] if l in luminosity_map] ]\n",
    "main_luminosity_map = {s:i for i,s in zip(range(len(main_luminosity)), main_luminosity) }\n",
    "\n",
    "#main_luminosity_map.update({k+'b':v for k,v in main_luminosity_map.items() if 'a' in k})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ff1be-2815-484b-9cba-2e3a9d704e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_luminosity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b2556-a0e2-4080-9a2b-9fc1cab45dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_values = {\n",
    "    'Water world': 415613,\n",
    "    'Earth-like world': 1126206,\n",
    "    'Ammonia world': 597762,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c4bea-3e47-4d79-a8bd-07e8bd58a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"E:/data/eddb/galaxy_1day.json.gz\"\n",
    "#filename = \"E:/data/eddb/galaxy_7days.json.gz\"\n",
    "#filename = \"E:/data/eddb/galaxy_1month.json.gz\"\n",
    "filename = \"E:/data/eddb/galaxy.json.gz\"\n",
    "\n",
    "filesize=Path(filename).stat().st_size\n",
    "chunksize = 64 * 1024 * 1024\n",
    "est_count = int(6.5*filesize/chunksize) + 1\n",
    "print(f\"Reading {filename}, {round(filesize/(1024*1024),1)} Mb in approx {est_count} chunks\")\n",
    "\n",
    "#itemdb =[]\n",
    "data = []\n",
    "\n",
    "missed = dict(\n",
    "    spectral=set(),\n",
    "    subclass=set(),\n",
    "    luminosity=set(),\n",
    "    magnitude=set(),\n",
    "    terraform=set()\n",
    ")\n",
    "mapcat = dict(\n",
    "    spectral=main_spectral_map,\n",
    "    luminosity=main_luminosity_map,\n",
    "    magnitude=magnitude_map)\n",
    "\n",
    "def catmap(cat, value):\n",
    "    if value not in mapcat[cat]:\n",
    "        missed[cat].add(value)\n",
    "        return None\n",
    "    else:\n",
    "        return mapcat[cat][value]\n",
    "    \n",
    "#totals = {C:dict(systems=0, count=0, total=0) for C in spectral_class_map }\n",
    "totals = {C:dict(systems=0, count=0, total=0) for C in set([s[:-1] for s in spectral_class_map]) }\n",
    "\n",
    "count = 0\n",
    "system_count = 0\n",
    "columns = slice(2,6)\n",
    "start = time.process_time()\n",
    "with gzip.open(filename, 'rt') as jsonfile:\n",
    "\n",
    "    firstline = jsonfile.readline()\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        count += 1\n",
    "        chunk = jsonfile.readlines(chunksize)\n",
    "        if chunk:\n",
    "            for line in chunk:\n",
    "                if len(line) < 5:\n",
    "                    continue\n",
    "                # This is where we decode the item and get data from it ----------------\n",
    "                item = json.loads(line[0:-2]) if line[-2] == \",\" else json.loads(line)\n",
    "                system_count += 1\n",
    "                #itemdb.append(item)\n",
    "                #continue\n",
    "                # get the data we need\n",
    "                bodies = item.get('bodies',[])\n",
    "                if not bodies:\n",
    "                    continue\n",
    "                if item.get('bodyCount',0) != len(bodies):\n",
    "                    continue\n",
    "\n",
    "                mainstars = [B for B in bodies if B.get('mainStar') and B.get('spectralClass')]\n",
    "                if not mainstars:\n",
    "                    continue\n",
    "                mainstar = mainstars[0]\n",
    "                \n",
    "                if mainstar.get('spectralClass') not in  main_spectral_map:\n",
    "                    continue\n",
    "                if mainstar.get('luminosity') not in main_luminosity_map:\n",
    "                    continue\n",
    "                if mainstar.get('absoluteMagnitude') < -15:\n",
    "                    continue\n",
    "                    \n",
    "                data.append(\n",
    "                    [ # Xn\n",
    "                        #item.get('name',''), \n",
    "                        item.get('bodyCount'),\n",
    "                        len([B for B in bodies if B.get('spectralClass')]),\n",
    "                        #int(round(2*sum([B.get('solarMasses') for B in bodies if B.get('spectralClass')]))),\n",
    "                        int(round(np.sqrt(sum([B.get('solarMasses') for B in bodies if B.get('spectralClass')])))),\n",
    "                        main_spectral_map.get(mainstar.get('spectralClass'))['seq'],\n",
    "                        main_spectral_map.get(mainstar.get('spectralClass'))['sub'],\n",
    "                        main_luminosity_map.get(mainstar.get('luminosity')),\n",
    "                        magnitude_map.get(int(round(mainstar.get('absoluteMagnitude')))),\n",
    "                        #int(round(mainstar.get('absoluteMagnitude'))),\n",
    "                        #int(bool([True for b in bodies if b.get('terraformingState') == 'Terraformable'])), # y1\n",
    "                        int(bool(sum([1 for b in bodies if b.get('terraformingState') == 'Terraformable'])>.5)), # y1\n",
    "                        int(bool(sum([planet_values.get(b.get('subType'),0) for b in bodies])>400000)) # y2\n",
    "                    ])\n",
    "                \n",
    "                    \n",
    "            # keep our programmer up to date of progress\n",
    "\n",
    "            sys.stdout.write(f\"{count}/{est_count}\\t{100*count/est_count:3.2f}%, {int(system_count / (time.process_time() - start)):6} /s, {system_count:9} systems, {((est_count - count) * (time.process_time() - start)/count):5.1f} seconds remaining\\r\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nEmpty chunk -> Done! Imported {system_count} systems in {round(time.process_time() - start,1)} seconds\")\n",
    "        break\n",
    "\n",
    " \n",
    "\n",
    "           \n",
    "adata = np.asarray(data, dtype=int)\n",
    "adata[:,0] = adata[:,0]-1\n",
    "adata[:,1] = adata[:,1]-1\n",
    "#n = np.isfinite(adata[:,6])\n",
    "#np.savetxt('data/mssystems.csv', adata[n], delimiter=',', fmt='%.2f')\n",
    "tpl = (time.process_time() - start)/system_count\n",
    "print(f\"{ (time.process_time() - start)} seconds {system_count} systems, per system {round(1000000*tpl,2)} us, est: {tpl*51854708}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2cc6f-b570-45be-9678-db44d83305f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(adata, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf06cb7-92a2-4c44-8897-3a048135b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.isfinite(adata[:,6])\n",
    "np.savetxt('data/systems_full.csv', adata, delimiter=',', fmt='%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cfa22-54ea-4acd-b2b2-3bd6fd1e1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(adata)\n",
    "# name, bodycount, starcount, starmass, spectralclass, luminosity, magnitude, terra, high_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb2556-efd5-4358-9d13-07261dce3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4733855-7244-499c-a2aa-d72185a1e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.greater(adata[:,7],0) | np.greater(adata[:,8],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd1dfb-d624-43cb-a3aa-24457e8ac969",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.isfinite(adata[:,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74faf5-bb1e-4b4f-b62c-ac8ae500d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=np.less(adata[:,2],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d321820-89b6-48b2-b8ce-933fe9586262",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amin(adata[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf5f7d7-6297-4bec-b74b-1b95db175684",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(adata[n]).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03b5ba-38aa-47c3-98b7-f263a86391e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ece1d-823b-4ea2-9c39-82af8f0aeec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with aiohttp.ClientSession() as session:\n",
    "    prettyprint(await get_edsm_info(session, 'Lyncis Sector KR-W b1-3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ea1ee-f679-4b06-aa9e-bfb26f28cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[5]>0].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e65d4-8bf5-487a-a958-5d0861c83bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[8]>0].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e2bf8-37e6-4066-a768-2b325fa02b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[7]>0].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dab273-bc77-42ce-84cf-11916bd709a8",
   "metadata": {},
   "source": [
    "## Write as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16986212-119b-461f-9e60-06a375c2f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e721b-ef01-4c35-beb3-16f966305d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b2ef5-b771-4f99-b86f-2d5275c6ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = adata[:, 1:].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe3f09-8300-420d-8837-3104aa6dc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75987750-e697-451a-ac04-711c28065ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87def31c-6e2e-496e-b9c9-fed9e2add185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84491c26-9b74-49b3-8ab6-7081813d4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd[np.isfinite(dfd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f3b02-09e8-4f1e-ab1d-938c00f8fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dfd[np.isfinite(dfd)])\n",
    "# name, bodycount, starcount, starmass, spectralclass, luminosity, magnitude, terra, high_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696842ca-4f89-4aec-a9fb-3ad7d1b1a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338b75b-508a-45da-9820-868a3a4b189e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf24008-9a8e-4a21-94c2-442b46a4ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[1,2,3,4,5,6,7,8]].to_csv('data/system_scan.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af483d14-a4bb-467f-8dcc-1644557f1cd4",
   "metadata": {},
   "source": [
    "## Read from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1086f8-8eb2-474a-86ba-75d1b6799f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = np.loadtxt('data/systems_full.csv', delimiter=',',dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0409108-7301-4542-8bfe-ae66baee48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718bad37-efc8-475d-850c-a1c389e56068",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(adata).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f669a0f-58b9-4def-a599-032bb0fbd713",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(adata).sample(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb3db9a-7c21-4380-b899-32da158fe86e",
   "metadata": {},
   "source": [
    "## Prep & cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d078d31-f14a-444b-a20d-f233915591af",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(adata, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed4542-d90d-4321-b902-3280b187ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata[:,0] = adata[:,0]-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a14570-6ffd-46ae-a400-3d0aea156eb6",
   "metadata": {},
   "source": [
    "## TPOT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fed681-0fa3-46fe-ae91-87d2727fc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier, TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acaee27-9d42-4515-b820-694e16b25b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(tpot)\n",
    "from tpot import TPOTClassifier, TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f9457-f2c2-41e9-b606-102e46d0d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Checking CUDA: {torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else 'No-Cuda'} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9a7f7-bd87-40bf-b32d-bf173d9f12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier_config_cuml = {\n",
    "    # cuML + DMLC/XGBoost Classifiers\n",
    "\n",
    "    \"cuml.neighbors.KNeighborsClassifier\": {\n",
    "        \"n_neighbors\": range(1, 101),\n",
    "        \"weights\": [\"uniform\",],\n",
    "    },\n",
    "\n",
    "    \"cuml.linear_model.LogisticRegression\": {\n",
    "        \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "        \"C\": [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.,],\n",
    "    },\n",
    "\n",
    "    # Sklearn Preprocesssors\n",
    "\n",
    "    \"sklearn.preprocessing.Binarizer\": {\n",
    "        \"threshold\": np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    \"sklearn.decomposition.FastICA\": {\n",
    "        \"tol\": np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    \"sklearn.cluster.FeatureAgglomeration\": {\n",
    "        \"linkage\": [\"ward\", \"complete\", \"average\"],\n",
    "        \"affinity\": [\"euclidean\", \"l1\", \"l2\", \"manhattan\", \"cosine\"]\n",
    "    },\n",
    "\n",
    "    \"sklearn.preprocessing.MaxAbsScaler\": {\n",
    "    },\n",
    "\n",
    "    \"sklearn.preprocessing.MinMaxScaler\": {\n",
    "    },\n",
    "\n",
    "    \"sklearn.preprocessing.Normalizer\": {\n",
    "        \"norm\": [\"l1\", \"l2\", \"max\"]\n",
    "    },\n",
    "\n",
    "    \"sklearn.kernel_approximation.Nystroem\": {\n",
    "        \"kernel\": [\"rbf\", \"cosine\", \"chi2\", \"laplacian\", \"polynomial\", \"poly\", \"linear\", \"additive_chi2\", \"sigmoid\"],\n",
    "        \"gamma\": np.arange(0.0, 1.01, 0.05),\n",
    "        \"n_components\": range(1, 11)\n",
    "    },\n",
    "\n",
    "    \"sklearn.decomposition.PCA\": {\n",
    "        \"svd_solver\": [\"randomized\"],\n",
    "        \"iterated_power\": range(1, 11)\n",
    "    },\n",
    "\n",
    "    \"sklearn.kernel_approximation.RBFSampler\": {\n",
    "        \"gamma\": np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    \"sklearn.preprocessing.RobustScaler\": {\n",
    "    },\n",
    "\n",
    "    \"sklearn.preprocessing.StandardScaler\": {\n",
    "    },\n",
    "\n",
    "    \"tpot.builtins.ZeroCount\": {\n",
    "    },\n",
    "\n",
    "    \"tpot.builtins.OneHotEncoder\": {\n",
    "        \"minimum_fraction\": [0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "        \"sparse\": [False],\n",
    "        \"threshold\": [10]\n",
    "    },\n",
    "\n",
    "    # Selectors\n",
    "\n",
    "    \"sklearn.feature_selection.SelectFwe\": {\n",
    "        \"alpha\": np.arange(0, 0.05, 0.001),\n",
    "        \"score_func\": {\n",
    "            \"sklearn.feature_selection.f_classif\": None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"sklearn.feature_selection.SelectPercentile\": {\n",
    "        \"percentile\": range(1, 100),\n",
    "        \"score_func\": {\n",
    "            \"sklearn.feature_selection.f_classif\": None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"sklearn.feature_selection.VarianceThreshold\": {\n",
    "        \"threshold\": [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "classifier_config_nn = {\n",
    "\n",
    "    'tpot.builtins.PytorchLRClassifier': {\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'batch_size': [4, 8, 16, 32],\n",
    "        'num_epochs': [5, 10, 15],\n",
    "        'weight_decay': [0, 1e-4, 1e-3, 1e-2]\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.PytorchMLPClassifier': {\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'batch_size': [4, 8, 16, 32],\n",
    "        'num_epochs': [5, 10, 15],\n",
    "        'weight_decay': [0, 1e-4, 1e-3, 1e-2]\n",
    "    },\n",
    "\n",
    "    # Classifiers\n",
    "    'sklearn.naive_bayes.GaussianNB': {\n",
    "    },\n",
    "\n",
    "    'sklearn.naive_bayes.BernoulliNB': {\n",
    "        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n",
    "        'fit_prior': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.naive_bayes.MultinomialNB': {\n",
    "        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n",
    "        'fit_prior': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.tree.DecisionTreeClassifier': {\n",
    "        'criterion': [\"gini\", \"entropy\"],\n",
    "        'max_depth': range(1, 11),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21)\n",
    "    },\n",
    "\n",
    "    'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'criterion': [\"gini\", \"entropy\"],\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.ensemble.RandomForestClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'criterion': [\"gini\", \"entropy\"],\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf':  range(1, 21),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.ensemble.GradientBoostingClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'max_depth': range(1, 11),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.neighbors.KNeighborsClassifier': {\n",
    "        'n_neighbors': range(1, 101),\n",
    "        'weights': [\"uniform\", \"distance\"],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "\n",
    "    'sklearn.svm.LinearSVC': {\n",
    "        'penalty': [\"l1\", \"l2\"],\n",
    "        'loss': [\"hinge\", \"squared_hinge\"],\n",
    "        'dual': [True, False],\n",
    "        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]\n",
    "    },\n",
    "\n",
    "    'sklearn.linear_model.LogisticRegression': {\n",
    "        'penalty': [\"l1\", \"l2\"],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],\n",
    "        'dual': [True, False]\n",
    "    },\n",
    "\n",
    "    'xgboost.XGBClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': range(1, 11),\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'n_jobs': [1],\n",
    "        'verbosity': [0]\n",
    "    },\n",
    "\n",
    "    'sklearn.linear_model.SGDClassifier': {\n",
    "        'loss': ['log', 'hinge', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'penalty': ['elasticnet'],\n",
    "        'alpha': [0.0, 0.01, 0.001],\n",
    "        'learning_rate': ['invscaling', 'constant'],\n",
    "        'fit_intercept': [True, False],\n",
    "        'l1_ratio': [0.25, 0.0, 1.0, 0.75, 0.5],\n",
    "        'eta0': [0.1, 1.0, 0.01],\n",
    "        'power_t': [0.5, 0.0, 1.0, 0.1, 100.0, 10.0, 50.0]\n",
    "    },\n",
    "\n",
    "    'sklearn.neural_network.MLPClassifier': {\n",
    "        'alpha': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'learning_rate_init': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
    "    },\n",
    "\n",
    "    # Preprocesssors\n",
    "    'sklearn.preprocessing.Binarizer': {\n",
    "        'threshold': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.decomposition.FastICA': {\n",
    "        'tol': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.cluster.FeatureAgglomeration': {\n",
    "        'linkage': ['ward', 'complete', 'average'],\n",
    "        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.MaxAbsScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.MinMaxScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.Normalizer': {\n",
    "        'norm': ['l1', 'l2', 'max']\n",
    "    },\n",
    "\n",
    "    'sklearn.kernel_approximation.Nystroem': {\n",
    "        'kernel': ['rbf', 'cosine', 'chi2', 'laplacian', 'polynomial', 'poly', 'linear', 'additive_chi2', 'sigmoid'],\n",
    "        'gamma': np.arange(0.0, 1.01, 0.05),\n",
    "        'n_components': range(1, 11)\n",
    "    },\n",
    "\n",
    "    'sklearn.decomposition.PCA': {\n",
    "        'svd_solver': ['randomized'],\n",
    "        'iterated_power': range(1, 11)\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.PolynomialFeatures': {\n",
    "        'degree': [2],\n",
    "        'include_bias': [False],\n",
    "        'interaction_only': [False]\n",
    "    },\n",
    "\n",
    "    'sklearn.kernel_approximation.RBFSampler': {\n",
    "        'gamma': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.RobustScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.StandardScaler': {\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.ZeroCount': {\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.OneHotEncoder': {\n",
    "        'minimum_fraction': [0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "        'sparse': [False],\n",
    "        'threshold': [10]\n",
    "    },\n",
    "\n",
    "    # Selectors\n",
    "    'sklearn.feature_selection.SelectFwe': {\n",
    "        'alpha': np.arange(0, 0.05, 0.001),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.SelectPercentile': {\n",
    "        'percentile': range(1, 100),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.VarianceThreshold': {\n",
    "        'threshold': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.RFE': {\n",
    "        'step': np.arange(0.05, 1.01, 0.05),\n",
    "        'estimator': {\n",
    "            'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "                'n_estimators': [100],\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.SelectFromModel': {\n",
    "        'threshold': np.arange(0, 1.01, 0.05),\n",
    "        'estimator': {\n",
    "            'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "                'n_estimators': [100],\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "classifier_config_pytorch = {\n",
    "\n",
    "    'tpot.builtins.PytorchLRClassifier': {\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'batch_size': [4, 8, 16, 32],\n",
    "        'num_epochs': [5, 10, 15],\n",
    "        'weight_decay': [0, 1e-4, 1e-3, 1e-2]\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.PytorchMLPClassifier': {\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'batch_size': [4, 8, 16, 32],\n",
    "        'num_epochs': [5, 10, 15],\n",
    "        'weight_decay': [0, 1e-4, 1e-3, 1e-2]\n",
    "    },\n",
    "\n",
    "\n",
    "    # Preprocesssors\n",
    "    'sklearn.preprocessing.Binarizer': {\n",
    "        'threshold': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.decomposition.FastICA': {\n",
    "        'tol': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.cluster.FeatureAgglomeration': {\n",
    "        'linkage': ['ward', 'complete', 'average'],\n",
    "        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.MaxAbsScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.MinMaxScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.Normalizer': {\n",
    "        'norm': ['l1', 'l2', 'max']\n",
    "    },\n",
    "\n",
    "    'sklearn.kernel_approximation.Nystroem': {\n",
    "        'kernel': ['rbf', 'cosine', 'chi2', 'laplacian', 'polynomial', 'poly', 'linear', 'additive_chi2', 'sigmoid'],\n",
    "        'gamma': np.arange(0.0, 1.01, 0.05),\n",
    "        'n_components': range(1, 11)\n",
    "    },\n",
    "\n",
    "    'sklearn.decomposition.PCA': {\n",
    "        'svd_solver': ['randomized'],\n",
    "        'iterated_power': range(1, 11)\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.PolynomialFeatures': {\n",
    "        'degree': [2],\n",
    "        'include_bias': [False],\n",
    "        'interaction_only': [False]\n",
    "    },\n",
    "\n",
    "    'sklearn.kernel_approximation.RBFSampler': {\n",
    "        'gamma': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.RobustScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.StandardScaler': {\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.ZeroCount': {\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.OneHotEncoder': {\n",
    "        'minimum_fraction': [0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "        'sparse': [False],\n",
    "        'threshold': [10]\n",
    "    },\n",
    "\n",
    "    # Selectors\n",
    "    'sklearn.feature_selection.SelectFwe': {\n",
    "        'alpha': np.arange(0, 0.05, 0.001),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.SelectPercentile': {\n",
    "        'percentile': range(1, 100),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.VarianceThreshold': {\n",
    "        'threshold': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.RFE': {\n",
    "        'step': np.arange(0.05, 1.01, 0.05),\n",
    "        'estimator': {\n",
    "            'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "                'n_estimators': [100],\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.SelectFromModel': {\n",
    "        'threshold': np.arange(0, 1.01, 0.05),\n",
    "        'estimator': {\n",
    "            'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "                'n_estimators': [100],\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'max_features': np.arange(0.05, 1.01, 0.05)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0d1d2-38f8-42ca-baa8-c0ee5db2b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_config_nn.pop('xgboost.XGBClassifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07328e04-1d0d-489f-9fef-d7103e0635ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b26d91-2af4-452c-bef8-885cae5f6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8dbf4d-158b-434b-91bc-7c40bdfa168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_X, final_X, tpot_y, final_y = train_test_split(\n",
    "    adata[:, [0,1,2,3,4,5,6]], adata[:,[7,8]],\n",
    "    train_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919498ba-e787-4c8d-8039-bf43e5966cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer_terra = TPOTClassifier(\n",
    "    #config_dict=classifier_config_nn,\n",
    "    config_dict='TPOT NN',\n",
    "    generations=3,\n",
    "    population_size=59,\n",
    "    warm_start=True,\n",
    "    n_jobs=-3,\n",
    "    max_eval_time_mins=5,\n",
    "    max_time_mins=4*60,\n",
    "    verbosity=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba61b9f-b540-46cb-8d0e-dc77ec787653",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer_terra.fit(tpot_X, tpot_y[:,0]) # Terraformable\n",
    "\n",
    "pipeline_optimizer_terra.export('data/tpot_classifier_terra_1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be53aa-a122-46d7-9a9f-63f5189aa235",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer_terra.fit(tpot_X, tpot_y[:,0]) # Terraformable\n",
    "\n",
    "pipeline_optimizer_terra.export('data/tpot_classifier_terra_2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319bd7bc-3bcf-46b6-9acb-43f259ea793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline_terra = pipeline_optimizer_terra.fitted_pipeline_.steps[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f66a2-1dff-4908-9a1e-91a5b78bace7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901fa3a-8fd1-4d9f-b4b5-6a683b5fd379",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer_value = TPOTClassifier(\n",
    "    #config_dict=classifier_config_nn,\n",
    "    config_dict='TPOT NN',\n",
    "    generations=3,\n",
    "    population_size=59,\n",
    "    warm_start=True,\n",
    "    n_jobs=-3,\n",
    "    max_eval_time_mins=5,\n",
    "    max_time_mins=4*60,\n",
    "    verbosity=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b24feb-8d61-4b5b-baef-38cca67f0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer_value.fit(tpot_X, tpot_y[:,1]) # Value planets\n",
    "\n",
    "pipeline_optimizer_value.export('data/tpot_classifier_value_1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b014d90-f673-4fec-9ad9-d25ceac8cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer_value.fit(tpot_X, tpot_y[:,1]) # Value planets\n",
    "\n",
    "pipeline_optimizer_value.export('data/tpot_classifier_value_2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c4e46-c381-4a55-b70a-c93172ea1cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline_value = pipeline_optimizer_value.fitted_pipeline_.steps[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed07e7-fca7-4315-b25d-ecb03ae5ae48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85045f-7492-4397-b978-d14e4f0d1af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1b9bd-98eb-4fff-8fb1-ee742093754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    adata[:, [0,1,2,3,4,5,6]], adata[:,7],\n",
    "    train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894968a5-05d1-4d85-a5ee-ec8f1b86ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02bd53d-2ec0-42a0-bd7b-48836d80be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer.export('data/tpot_classifier_terra_1m.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509db67a-d1b7-4efc-966d-58c864d60291",
   "metadata": {},
   "outputs": [],
   "source": [
    "exctracted_best_model = pipeline_optimizer.fitted_pipeline_.steps[-1][1]\n",
    "exctracted_best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7166789-f795-44c9-b778-86c15b99a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred  = exctracted_best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3302cd-665e-487a-9787-3b94bf0f6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,y_test_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90dca50-9405-441e-ba95-50f4611e4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d6111-883c-44a3-858d-aba5e5d0ea79",
   "metadata": {},
   "source": [
    "## Testing the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2155841-50a4-4034-9412-103608ba8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tpot.builtins import StackingEstimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba3df97-7d12-422b-89d2-2e6d73e8f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features, testing_features, training_target, testing_target = train_test_split(\n",
    "    final_X, final_y,\n",
    "    train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920880c-f9c9-4f26-9d8d-c2e948f96af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features, testing_features, training_target, testing_target = train_test_split(\n",
    "    adata[:,[0,1,2,3,4,5,6]], adata[:,[7,8]],\n",
    "    train_size=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc4da1-d982-4c46-af76-b434e9825ec9",
   "metadata": {},
   "source": [
    "### Value planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da09b4d-659a-4f7a-a7b4-88ac560e297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average CV score on the training set was: 0.9119772778209425\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "exported_pipeline_values = ExtraTreesClassifier(\n",
    "    bootstrap=True, criterion=\"gini\", max_features=0.8500000000000001, \n",
    "    min_samples_leaf=18, min_samples_split=8, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf18e2-4346-4a51-b5cf-667ab215d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline_values.fit(training_features, training_target[:,1])\n",
    "predicted_target_values = exported_pipeline_values.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abc127-f030-4ef3-8eb8-8e0c6494083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(testing_target[:,1], predicted_target_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c983e93-5fbe-4620-aaa4-2e78f771dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(testing_target[:,1], predicted_target_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00905184-7878-4cde-90f0-6069bdda06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(testing_target[:,1], exported_pipeline_values.predict(testing_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c04e5-2da9-45a8-9efc-da2dd87d5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(testing_target[:,1], exported_pipeline_values.predict(testing_features)).ravel()\n",
    "\n",
    "print(f\"Actually False: \\t True Negatives: {tn:6}, False Positives: {fp:6}\")\n",
    "print(f\"Actually True:  \\t False Negatives:{fn:6}, True Positives:  {tp:6}\")\n",
    "print(f\"Predicted       \\t     False        \\t   True    \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579c297-6a48-42ae-b11d-15ab2e74c8cf",
   "metadata": {},
   "source": [
    "### Terraforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39f170-e2a2-4217-9f33-f43f08081d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average CV score on the training set was: 0.885480051857461\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "exported_pipeline_terra = GradientBoostingClassifier(\n",
    "    learning_rate=0.1, max_depth=8, max_features=1.0, \n",
    "    min_samples_leaf=15, min_samples_split=7, \n",
    "    n_estimators=100, subsample=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa11ad-3f4c-42a7-809f-9737a06ee62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline_terra.fit(training_features, training_target[:,0])\n",
    "predicted_target_terra = exported_pipeline_terra.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99949749-4c41-4bbd-bd1d-41325cd438f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(testing_target[:,0], predicted_target_terra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685dfd23-74d9-4b19-94ee-f886761ea7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(testing_target[:,0], predicted_target_terra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ff245-5b9d-4f38-a453-c26276f7f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(testing_target[:,0], exported_pipeline_terra.predict(testing_features)).ravel()\n",
    "\n",
    "print(f\"Actually False: \\t True Negatives: {tn:6}, False Positives: {fp:6}\")\n",
    "print(f\"Actually True:  \\t False Negatives:{fn:6}, True Positives:  {tp:6}\")\n",
    "print(f\"Predicted       \\t     False        \\t   True    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9848645-488a-4d66-bbfd-08b40b035b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa6748-6120-4c35-b9e6-1e76a62f012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features.shape, testing_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777ba66-47ba-4c48-ad0e-13e8995103e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features[np.greater(testing_target[:,0],0.5)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a48b4-1ae7-4463-b48d-957280151e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ebaa6-312d-4f27-8bc8-fc2b4330dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[np.greater(results,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e8a67-34c4-4cf3-843a-8d8d3f31eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[y_test[7]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e664b9f-ad1a-4165-adae-2ee9a645beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[4]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0325a-47d4-4a3f-9ed7-20133cf2c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[4]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb718381-2e73-44e9-ab02-143afdcef4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caea973-733c-4d52-a28f-b832f20e247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347af0c7-249c-4c41-880d-691234462869",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 if l.update(dict(a=3)) else l.get('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849fe33-590a-49a9-a48c-cb2e0ab6301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd651e8-3125-4397-bba5-df107f0c6649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c40b8-eeb8-48a6-ab1d-88df8cac7737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a866d-722b-4475-8e17-523f6778c0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3352f849-9f43-4617-b4c0-bdb633897f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = system_count / 1000\n",
    "pd.DataFrame([\n",
    "    (\n",
    "        k,\n",
    "        round(100*v['count']/v['systems']),\n",
    "        v['count'],\n",
    "        v['systems'],\n",
    "        v['total'],\n",
    "        round(v['total'] / v['count'])\n",
    "        \n",
    "    )\n",
    "    for k,v in totals.items()\n",
    "    if v.get('count')>0 and v['count']/v['systems'] > .1 and v['systems'] > min_count\n",
    "], columns=['spectralclass', 'chance', 'count', 'systems', 'total', 'average']).set_index(\n",
    "    ['spectralclass']).sort_values(['chance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6a6e9-0ed7-49e3-9bd8-8e83149451aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    chance\tcount\ttotal\taverage\n",
    "class\t\t\t\t\n",
    "MS\t11\t33\t20731734\t628234\n",
    "CN\t12\t27\t16021355\t593384\n",
    "G\t13\t10232\t6778211922\t662452\n",
    "AeBe\t14\t191\t134299067\t703136\n",
    "F\t17\t19025\t12397439067\t651639\n",
    "A\t18\t10030\t6386976681\t636787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6d689-faf2-470a-9344-dfe45bbbc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc505e-3d54-44c4-8168-1d1ee9cfd388",
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyprint({k:v for k,v in totals.items() if v.get('count')>0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ccb61-7357-426f-889c-d99412c15d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48081d-13cd-410c-b88b-4dda6097d0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a136d86e-661c-4e16-a477-4835e467b1a0",
   "metadata": {},
   "source": [
    "## Create mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b91728-9857-4482-a78c-b10e34f11c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "['spectralClass','luminosity','absoluteMagnitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c606ec2-4d3d-4649-b7db-cd931f6dd8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_map(set_to_map):\n",
    "    templist = sorted(set_to_map)\n",
    "    return {s:i for i,s in zip(range(len(templist)), templist)}\n",
    "\n",
    "def recreate_map(mapping):\n",
    "    subval = min(mapping.values())\n",
    "    if subval > 0:\n",
    "        mapping = {k:v-subval for k,v in mapping.items()}\n",
    "\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23c6bde-7864-4952-8e72-f281f5c9f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_types = auto_map([T for T in set([B.get('type') for S in itemdb for B in S.get('bodies',[]) ])])\n",
    "prettyprint(body_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f90ab-3fcd-401e-8f70-aea9ea588c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "terraform_types = auto_map([T for T in set([B.get('terraformingState') for S in itemdb for B in S.get('bodies',[]) if B.get('terraformingState') ]) ])\n",
    "prettyprint(terraform_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a280d-e3ea-4373-863e-1296a1f56ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_map(terraform_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066c82a-1c86-4d6d-8dc9-3230d6cd96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "terraform_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9906ba89-6596-458f-a574-cf918b318e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_class = auto_map(set([B.get('subType') for S in itemdb for B in S.get('bodies',[]) if B.get('type') == 'Planet']))\n",
    "prettyprint(planet_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969dbddc-f708-4ad3-bfd4-8d6302af3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_values = {\n",
    "    'Water world': 415613,\n",
    "    'Earth-like world': 1126206,\n",
    "    'Ammonia world': 597762,\n",
    "}\n",
    "classes_valuable = set([planet_class.get(C) for C in planet_values])\n",
    "prettyprint(classes_valuable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f288d7-0bd8-454d-b48d-fc88833bca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "[planet_values.get(C,0) for C in planet_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edece95d-286a-4610-9b41-3437eed816a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_class_valuable = auto_map(set([\n",
    "    B.get('subType') \n",
    "    for S in itemdb for B in S.get('bodies',[]) \n",
    "    if B.get('type') == 'Planet' and B.get('subType') in planet_values]))\n",
    "prettyprint(planet_class_valuable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d6361-0346-421e-a10b-16abe61c3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_classes = auto_map([\n",
    "    T for T in set(\n",
    "        [B.get('spectralClass') for S in itemdb for B in S.get('bodies',[])  if B.get('spectralClass')] \n",
    "    )])\n",
    "prettyprint(spectral_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b28ca0-70c9-4ea1-ace4-9e17bce2d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "luminosity_map = auto_map([\n",
    "    T for T in set(\n",
    "        [B.get('luminosity') for S in itemdb for B in S.get('bodies',[])  if B.get('luminosity')] \n",
    "    )])\n",
    "prettyprint(luminosity_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b805ca-95fd-4a97-ac2a-fde6f27d6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_map = auto_map([\n",
    "    T for T in set(\n",
    "        [int(round(B.get('absoluteMagnitude'))) for S in itemdb for B in S.get('bodies',[])  if B.get('absoluteMagnitude')] \n",
    "    )])\n",
    "prettyprint(magnitude_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388bcad-40cd-43c1-bf89-4c7c774f735b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb889f77-36a1-4fcc-9ee3-a5f14051aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "        set([B.get('subType')[0] for S in itemdb for B in S.get('bodies',[])  if B.get('type') == 'Star' and B.get('subType')] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ba991-1901-4b07-a771-6911bb47e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(itemdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06ae9f-fbba-4773-86a3-6baf61f69266",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([\n",
    "    B.get('subType') \n",
    "    for S in itemdb for B in S.get('bodies',[]) \n",
    "    if B.get('mainStar') and B.get('spectralClass') and B.get('spectralClass')[:-1] == 'CJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214b5f3-9016-4651-ad23-69f8a71ccedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([\n",
    "    signal \n",
    "    for S in itemdb \n",
    "    for B in S.get('bodies',[]) \n",
    "    for signal in B.get('signals', {}).get('signals',{})  \n",
    "    if B.get('type') == 'Planet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe71f5-3c59-4401-820f-7209f5c9efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([B.get('luminosity') for S in itemdb for B in S.get('bodies',[]) if B.get('luminosity') ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232e249-75d3-4fac-8d79-f6ac642fa8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in itemdb][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9e579-0d3a-44a4-b6a3-763624f92c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b120cd5-a443-42e3-805a-2450c92a4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91389689-21d6-4e44-8bd5-19533d106c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[\n",
    "    (\n",
    "        \n",
    "        round(B.get('solarMasses'),2),\n",
    "        spectral_class_prefix.get(B.get('spectralClass')[:-1]),\n",
    "        spectral_class_suffix.get(B.get('spectralClass')[-1])\n",
    "\n",
    "    )  \n",
    "    for B in itemdb[i].get('bodies',[])\n",
    "    if B.get('mainStar')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6b124-9207-449f-8145-77a185f6d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[\n",
    "    (\n",
    "        B.get('subType')[0],\n",
    "        B.get('spectralClass'),\n",
    "        B.get('solarMasses')\n",
    "\n",
    "    )  \n",
    "    for B in itemdb[i].get('bodies',[])\n",
    "    if B.get('type') == 'Star'# and not B.get('mainStar')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd4e48-1356-4638-8bd5-6a608982a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "round(sum([\n",
    "    B.get('solarMasses')\n",
    "\n",
    "    for B in itemdb[i].get('bodies',[])\n",
    "    if B.get('type') == 'Star'\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee78ce-18dc-4cc8-8cec-ea2ffefdb5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a984c8-cd38-4328-b471-6a596ceb6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i+=1\n",
    "sum([\n",
    "    planet_values.get(B.get('subType'),0)\n",
    "\n",
    "    for B in itemdb[i].get('bodies',[])\n",
    "    if B.get('type') == 'Planet' \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27153f-f34a-484c-8769-0e5644e6f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    (\n",
    "        I.get('name'),\n",
    "        I.get()\n",
    "    for I in itemdb\n",
    "    if sum([\n",
    "        planet_values.get(B.get('subType'),0)\n",
    "\n",
    "        for B in I.get('bodies',[])\n",
    "        #if B.get('type') == 'Planet' \n",
    "    ]) > 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad056c6-2ddb-4f3a-b3d9-e3edec6defde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[\n",
    "    B.get('subType')\n",
    "\n",
    "    for B in itemdb[i].get('bodies',[])\n",
    "    if B.get('type') == 'Planet' \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639030ad-c495-478f-98b6-b6ab4d0e1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyprint(itemdb[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6204d-e211-42b4-a439-b8109680f795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d04121-ef61-449a-9d01-975f89a6271c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "warp-cuda",
   "language": "python",
   "name": "warp-cuda"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
